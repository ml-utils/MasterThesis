@inproceedings{bostrom2020byte,
	title={Byte Pair Encoding is Suboptimal for Language Model Pretraining},
	author={Bostrom, Kaj and Durrett, Greg},
	booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
	pages={4617--4624},
	year={2020},
	key = {Bostrom and Durret (2020}
}

@inproceedings{chaves2014subject,
	title={Which subject islands will the acceptability of improve with repeated exposure},
	author={Chaves, Rui P and Dery, Jeruen E},
	booktitle={Proceedings of the 31st West Coast Conference on Formal Linguistics},
	pages={96--106},
	year={2014},
	organization={Citeseer},
	key = {Chaves et al. 2014}
}

@inproceedings{de2020geppetto,
	title={GePpeTto Carves Italian into a Language Model},
	author={De Mattei, Lorenzo and Cafagna, Michele and Dell'Orletta, Felice and Malvina, Nissim and Guerini, Marco},
	booktitle={Seventh Italian Conference on Computational Linguistics (CLIC-it 2020)},
	volume={2769},
	pages={136},
	year={2020},
	key = {De Mattei et al. 2020}
}

@article{futrell2018rnns,
	title={RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency},
	author={Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Levy, Roger},
	journal={arXiv preprint arXiv:1809.01329},
	year={2018},
	key = {Futrell et al. 2018}
}

@inproceedings{futrell-etal-2019-neural,
	title = "Neural language models as psycholinguistic subjects: Representations of syntactic state",
	author = "Futrell, Richard  and
	Wilcox, Ethan  and
	Morita, Takashi  and
	Qian, Peng  and
	Ballesteros, Miguel  and
	Levy, Roger",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1004",
	doi = "10.18653/v1/N19-1004",
	pages = "32--42",
	abstract = "We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.",
	key = {Futrell et al. 2019}
}

@inproceedings{holtzman2019curious,
	title={The Curious Case of Neural Text Degeneration},
	author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	booktitle={International Conference on Learning Representations},
	year={2019},
	key = {Holtzman et al. 2019}
}

@inproceedings{hu2020systematic,
	title={A Systematic Assessment of Syntactic Generalization in Neural Language Models},
	author={Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P},
	booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	pages={1725--1744},
	year={2020},
	key = {Hu et al. 2020}
}

@article{lau2020furiously,
	title={How furiously can colorless green ideas sleep? sentence acceptability in context},
	author={Lau, Jey Han and Armendariz, Carlos and Lappin, Shalom and Purver, Matthew and Shu, Chang},
	journal={Transactions of the Association for Computational Linguistics},
	volume={8},
	pages={296--310},
	year={2020},
	publisher={MIT Press},
	key = {Lau et al. 2020}
}

@article{linzen2016assessing,
	title={Assessing the ability of LSTMs to learn syntax-sensitive dependencies},
	author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	journal={Transactions of the Association for Computational Linguistics},
	volume={4},
	pages={521--535},
	year={2016},
	publisher={MIT Press},
	key = {Linzen et al. 2016}
}

@techreport{radford2018improving,
	author      = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
	title       = {Improving language understanding by generative pre-training},
	institution = "OpenAI",
	year        = {2018},
	key = {Radford et al. 2018}
}

@article{sprouse2016experimental,
	title={Experimental syntax and the variation of island effects in English and Italian},
	author={Sprouse, Jon and Caponigro, Ivano and Greco, Ciro and Cecchetto, Carlo},
	journal={Natural Language \& Linguistic Theory},
	volume={34},
	number={1},
	pages={307--344},
	year={2016},
	publisher={Springer},
	key = {Sprouse et al. 2016}
}

@article{warstadt2020blimp,
  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R},
  title={BLiMP: The benchmark of linguistic minimal pairs for English},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={377--392},
  year={2020},
  publisher={MIT Press},
  key = {Warstadt et al. 2020}
}

@inproceedings{wei2021frequency,
	title={Frequency Effects on Syntactic Rule Learning in Transformers},
	author={Wei, Jason and Garrette, Dan and Linzen, Tal and Pavlick, Ellie},
	booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	pages={932--948},
	year={2021},
	key = {Wei et al. 2021}
}