\thispagestyle{plain}
\begin{center}
	\Large
	\vspace{1.8cm}
    \textbf{Abstract}
    	\vspace{0.9cm}
\end{center}

\normalsize
Modern language models based on deep artificial neural networks have achieved impressive progress in Natural Language Processing benchmarks and applications in the last few years. This has made increasingly important to clarify which linguistic phenomena and generalizations they actually learn. This has spawned a line of research on the fine-grained targeted linguistic evaluations of neural language models, in which the targeted syntactic evaluation approach in one of the main ones.

The assessment is done by administering to these models minimal pairs of sentences that vary minimally and isolate a particular linguistic phenomenon, and expect the model to give a higher score to the grammatical sentence over the ungrammatical one. A factorial experimental setup, common in psycholinguistic studies, can be considered a generalization of the minimal pairs approach, and allows to test more complex linguistic phenomena while still controlling for confounds.

% A generalization of the minimal pairs approach is the factorial experimental setup, imported from psycholinguistic research, in which test items are composed of more than two sentences, covering a combination of conditions, to better control for confounds, and allowing the assessment of more complex linguistic phenomena.

% or factorial items, minimally varying 
% administering to these models 
% testing these models' sentence acceptability estimates with fine-grained targeted linguistic evaluations, based on minimal pairs that isolate a particular linguistic phenomenon.\\

This kind of assessment is relevant to address the open problems of the limitations that these models still have, like being significantly data-inefficient in their training, compared to humansâ€™ language acquisition and learning skills; or their still insufficient linguistic performance or generalization for some linguistic phenomena. This kind of assessment has also a broad interdisciplinary relevance since language models could be used to test theoretical linguistics hypotheses, and theoretical linguistics and psycholinguistics could in turn provide insights on how to improve these models' linguistic skills to more human-like levels.\\ 

In this work, we focus on the syntactic phenomena of island effects, and extend the Italian test suite from the psycholinguistic and experimental syntax work by \citet{sprouse2016experimental}. Then,  we evaluate on these test suites two transformer-based language models (Gpt-2 and Bert), pretrained in Italian, and compare their performance with those on humans. (..)% We propose that our adaptation of the factorial test design by Sprouse et al. could be complementary to benchmarks like BLiMP \citep{warstadt2020blimp} which are based on minimal pairs tests, and allows to test more aspects of linguistic phenomena like extraction islands.
