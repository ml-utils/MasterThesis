\thispagestyle{plain}
\begin{center}
	\Large
	\vspace{1.8cm}
    \textbf{Abstract}
    	\vspace{0.9cm}
\end{center}

\normalsize
Modern language models based on deep artificial neural networks have achieved impressive progress in Natural Language Processing benchmarks and applications in the last few years. This has made increasingly important to clarify which linguistic phenomena and generalizations they actually learn. This has spawned a line of research on the fine-grained targeted linguistic evaluations of neural language models, in which the targeted syntactic evaluation approach in one of the main ones.

The assessment is done by administering to these models minimal pairs of sentences that vary minimally and isolate a particular linguistic phenomenon, and expect the model to give a higher score to the grammatical sentence over the ungrammatical one. A factorial experimental setup, common in psycholinguistic studies, can be considered a generalization of the minimal pairs approach, and allows to test more complex linguistic phenomena while still controlling for confounds.

% A generalization of the minimal pairs approach is the factorial experimental setup, imported from psycholinguistic research, in which test items are composed of more than two sentences, covering a combination of conditions, to better control for confounds, and allowing the assessment of more complex linguistic phenomena.

% or factorial items, minimally varying 
% administering to these models 
% testing these models' sentence acceptability estimates with fine-grained targeted linguistic evaluations, based on minimal pairs that isolate a particular linguistic phenomenon.\\
% 
% In assessing transformer-based  language models, pretrained in the Italian language, on syntactic phenomena known as island effects, we found that the models responses for some types of island effects can mirror or resemble that of humans, as observed in previous psycholinguistic studies.


This kind of assessment is 
%might  for the development of these models, to accelerate their developemnt
relevant to address the open problems and the limitations that these models still have, like being significantly data-inefficient in their training, compared to humansâ€™ language acquisition and learning skills; or their still insufficient linguistic performance or generalization for some linguistic phenomena. Therefore it might accelerate progress toward more general models for natural language understanding. This line of research has also a broad interdisciplinary relevance since neural language models (NLMs) could be used to test theoretical linguistics hypotheses, and theoretical linguistics and psycholinguistics could in turn provide insights on how to better evaluate these models and how to improve their linguistic skills to more human-like levels.\\ 

In this work, we focus on the assessment of island effects, which is one of the most challenging syntactic phenomena, to learn which NLMs have been shown to need more training data than most other syntactic phenomena. We extend and adapt an Italian test suite on wh-dependencies from a psycholinguistic study \citep{sprouse2016experimental}, and use it to evaluate four transformer-based models (GPT-2 and variants of BERT) pretrained in Italian, of fixed parameters size but varying in the amount of training data (from 2B to 13B tokens). % these test suites two transformer-based language models (Gpt-2 and Bert), pretrained in Italian, and compare their performance with those on humans. (..)% We propose that our adaptation of the factorial test design by Sprouse et al. could be complementary to benchmarks like BLiMP \citep{warstadt2020blimp} which are based on minimal pairs tests, and allows to test more aspects of linguistic phenomena like extraction islands.

We find that subject islands is the phenomenon most correlated with training set size, while whether islands seems the easiest to learn.%, and this could correlate with subject islands being a weaker
We find that the models responses resemble in part those of humans on average, as compared from the trends in plots of normalized acceptability judgments. We find that although the factorial design is able to implicitly factor out some confounds when evenly distributed across conditions, the items should still be strictly controlled to have the same lexical content, as semantic and collocations effects seem to be predominant in affecting the model scores.
% scoring the models out of the box using without finetuning
% We found that the factorial experimental design we adopted from psycholinguistic studies was able to implicitly control for some unexpected confounds, when they were evenly distributed within the test items. However, as it is the case with assessment approaches based on minimal pairs, also in an experimental setup based on a factorial design, sentences within an item should have ideally exactly the same lexical content, in order to avoid confounds, since semantics effects and effects due to collocations tend to predominate over syntactic ones. Designing experimental items with such strict requirements is a challenging tasks in the development of tests covering more complex syntactic phenomena like island effects.
%
%Our results give indications that, in accordance with what found in previous work, to master more complex syntactic phenomena, like some types of island effects, modern transformer-based language models still require an amount of training data above the threshold of 100M tokens, which is the amount of data to which humans are exposed when they learn language, and also the threshold at which transformer-based models have been shown (although possibly by test suites not challenging enough) to acquire knowledge of most (but not all) syntactic and semantic phenomena.
%
%In our contribution to the relatively novel line of research on targeted syntactic evaluation of deep neural language models (an area for which adequate methodologies are sill in development), we found that it is an effective and informative approach to complement the analysis of the performance scores achieved by the models, with the qualitative analysis of the per-token surprisals, which reveal to which phenomena a model shows sensitivity. This combination of approaches seems also a promising avenue to reach better explainability of the internal representations learned by these models.
%
%We agree with previous work \citep{wilcox2018rnn, ettinger2020bert} that observed that developing more comprehensive and more challenging targeted linguistic tests for modern language model can play a significant role in accelerating their development. 
%
%"more fine-grained evaluation tools may accelerate work on general-purpose neural network modules for sentence understanding. "  \citep{wilcox2018rnn}