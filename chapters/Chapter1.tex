% shadow references (in comments) for thesis structure, approach, ..

\chapter{Introduction}


\section{Motivation}

With the large progress made since 2018 in NLP benchmarks by the last generation of artificial neural network language models, which are based on a deep architecture of transformers layers and adopt a self-supervised learning approach, it has become increasingly important to understand what these model actually learn about human language \citep{hewitt2019structural, manning2020emergent, trotta2021monolingual}. \\
Investigating this serves multiple purposes, like providing insights for building better models, but it is also of interest for addressing research questions in linguistics \citep{hewitt2019structural}.  \\

(TODO: virgolettati da sintetizzare di seguito) \\

"Linguistic competence of neural language models (LMs) has emerged as one of the core sub-fields in NLP. "  (Cherniavskii et al. 2022 Acceptability Judgements via Examining the Topology of Attention Maps)
% "research paradigms" (Cherniavskii et al. 2022)

"more fine-grained evaluation tools may accelerate work on general-purpose neural network modules for sentence understanding. " \\
"studying the linguistic competence of ANNs bears on foundational questions in linguistics about the learnability of grammar."

(black boxes and explainability) : "the nature of the representations learned by these models is not properly understood." \citep{wilcox2018rnn}

"In addition to the insight these results provide about neural NLP systems, they also bear on questions central to cognitive science and linguistics, putting lower bounds on what syntactic knowledge can be acquired from string input alone"  \citep{hu2020systematic} 

% "We see two chief motivations that guide work on acceptability classification with ANNs by us and by others: First," 
% "Second, 
% better models: more performant, more efficient. Detect which linguistic phenomena they learn and which they don't.


% "Targeted syntactic evaluations have shown that these models [neural language models ] also implicitly capture many syntactic generalizations, ranging from subject–verb agreement to long-distance filler–gap dependencies (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2019b)" \citep{hu2020systematic}

%"similar developments in semantic evaluation (McCoy et al., 2019)" \citep{hu2020systematic}

% A particular interest of language in AI, is that it ..measures AI models with ..different concepts ..phenomena for which some of the standard ..alghoritmic/statistical or teory of information ..metrics, approaches, .. , don't work (eg. perplexity, ..probability estimation of next word, ..)

% transformer SotA :
% "transformer-based models, as they have become the benchmark for many NLP tasks in recent years (Vaswani et al., 2017; Devlin et al., 2019; Yang et al., 2019)"  \citep{lau2020furiously}. 
% transformers: much larger models, much more training data
% "Recent technological advances have led to an explosion of neural network-based LM architectures. The most popular ones are based on recurrent neural networks (RNNs) .. , LSTMs are still highly competitive (Melis et al., 2018)" \citep{marvin2018targeted}

% the above section should be renamed also "introduction"
% then have a more specific motitvation section on the motivation of this thesis within its line of research (targeted linguistic tests for language models)

% a  self-supervisedlearning approach. In self-supervised learning, a system is given no  explicit  labeling  of  raw  data,  but  it  is  able  to  construct  its own supervised learning problems by choosing to interpret someof  the  data  as  a  “label”  to  be  predicted \citep{manning2020emergent}

% The  canonical  casefor  human  language  is  the  language-modeling  task  of  trying to  predict  the  next  word  in  an  utterance  based  on  the  tempo-rally preceding words (Fig. 2). Variant tasks include the maskedlanguage-modeling  task  of  predicting  a  masked  word  in  a  text[a.k.a.  the  cloze  task  (11)]  and  predicting  the  words  likely  tooccur  around  a  given  word  (12,  13).  Autoencoders  (14)  canalso be thought of as self-supervised learning systems. Since noexplicit labeling of the data is required, self-supervised learningis  a  type  of  unsupervised  learning,  but  the  approach  of  self-generating supervised learning objectives differentiates it fromother unsupervised learning techniques such as clustering \citep{manning2020emergent}
% (that unsupervised models could not develop interesting knowledge of language) this  has  been  the  dominant  perspective  in  linguistics, where language models have long been seen as inadequateand having no scientific interest, even when their usefulness inpractical engineering applications is grudgingly accepted (15, 16). \citep{manning2020emergent}



% "A language model (LM) defines a probability distribution over sequences of words"  \citep{marvin2018targeted}



% "building better models": more performant and more data/computataion efficient
% The purpose of assessing which linguistic knowledge is acquired by language models based on neural networks: 
% - finding how/where/on what to improve the model, ..guiding research on further developing them for better performance, more data efficiency and computational efficiency
% - ..explainability, 
% - having a complementary measure of model performance, explaining performance (good or bad) in downstream application benchmarks, 
 
 
% BERT " implicitly learns to recover the rich latent structure of human language. "   (manning2020emergent)
 

% Addressing the question of how the linguistic knowledge of state-of-the-art language models (LMs) varies across linguistic phenomena \citep{warstadt2020blimp}

% "One promising line of research aims to crack open these ‘black boxes’ by investigating how" "language models perform on specially controlled sentences designed to draw out behavior that indicates representation of" "a syntactic dependency." \citep{wilcox2018rnn}



% "Many recent studies have searched for evidence that neural networks (NNs) learn representations ("\textit{linguistic knowledge}") that implicitly encode grammatical concepts. "  \citep{warstadt2020blimp}

% Purposes: to uncover linguistic phenomena where current state-of-the-art transformer-based language models lack human-like knowledge, to focus on investigating and improving those  \citep{warstadt2020blimp}.
% offer broad coverage of linguistic phenomena \citep{warstadt2020blimp} 

% indirect evidence about each model’s linguistic knowledge \citep{warstadt2020blimp}
% compare models in a fine-grained way \citep{warstadt2020blimp}.



\section{Research on linguistic tests on language models}
(TODO)
%(general introductory overview of previous studies, state of the art, chronicle history, ..)


\section{Island Effects and their assessment}

\subsection{Overview on island effects}

(TODO: sintetizzare i virgolettati di seguito) \\

"filler–gap dependencies are subject to numerous complex island constraints:  \citet{ross1967constraints} identified five syntactic positions in which gaps are illicit, dubbing them syntactic islands." \citep{wilcox2018rnn} \\
% "Even though the filler–gap dependency is flexible and potentially unbounded, it is not entirely unconstrained.

Examples: (..) \\

"open question whether these “island constraints” are true grammatical constraints, or whether they are effects of processing difficulty or discourse-structural factors (Ambridge and Goldberg, 2008; Hofmeister and Sag, 2010; Sprouse and Hornstein, 2014)" \citep{wilcox2018rnn} 

Argument from the poverty of the stimulus (APS):
"Because of their complexity and ubiquity, these dependencies have figured prominently in arguments that natural language would be unlearnable by children without a great deal of innate knowledge (Phillips, 2013) (cf. Pearl and Sprouse, 2013; Ellefson and Christiansen, 2000)" \citep{wilcox2018rnn} 

"The influential argument from the poverty of the stimulus (APS) ..  has been subject to much criticism (Pullum and Scholz, 2002)"  Geoffrey K. Pullum and Barbara C. Scholz. 2002. Empirical assessment of stimulus poverty arguments. The Linguistic Review, 18(1-2):9–50.

% psycholinguistic studies (Sprouse)

Wh-Island Constraint definition: 
"A gap cannot appear inside doubly nested clauses headed by wh-complementizers. This phenomenon is called the Wh-Island Constraint (WHC). "  \citep{wilcox2018rnn}

%Adjunct Island Constraint
%"We used three different prepositions to construct temporal adjuncts: ‘while’, ‘after’ and ‘before’." \citep{wilcox2018rnn}

\subsection{Island effects assessment in language models}

(TODO)

% Wilcox, Blimp, ..

\pagebreak

\section{Transformer-based language models}

% section on overview of NN language models, DL language models, transformers based language models

\subsection{Transformers and the Attention Mechanism}

Transformers  are "a neural network architecture,  without  any  recurrent  connections  (22),  which  takes  a sequence  of  words  (or  other  symbols)  as  input  and  produces a contextualized vector representation of each word as its out-put (Fig. 3). It contains many millions of trainable parameters in  a  number  of  layers,  typically  requiring  massive  amounts  of data  and  computation  to  train.  This  makes  Transformers  difficult  to  train,  but  also  highly  expressive  models  that  can  out-perform  other  contemporary  neural  networks  when  properly optimized" \citep{manning2020emergent}

"The key mechanism by which Transformers contextualize representations  is  multi-headed  attention  (see  Fig.  5)."  \citep{manning2020emergent} 
"attention is a kind of \textbf{generalized dot product}" % (Manning Lecture) https://www.youtube.com/watch?v=aKO1Yok6e4Q&list=PLGJm1x3XQeK0gmqfRkP-VmrEf4UYx5IDW&index=4
which models all pairwise interactions between words.

"Attention(23) dynamically assigns a weight to every pair of words in the sequence, indicating how much the model should “pay attention to” the first word when computing the representation of the second one."  \citep{manning2020emergent}
"Transformers use multiple attention heads in parallel, where  each  head  can  potentially  capture  a  completely  different word–word relation. Transformers aggregate the information from each head to produce a single output vector representation for each word in the sequence." \citep{manning2020emergent}
% "We provide more mathematical detail below" \citep{manning2020emergent}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/Chapter1/attention-manning-pnas.png} 
	\caption{Overview of the attention mechanism, taken from \citet{manning2020emergent} (TODO: request permission, pnas licence is CC ND)} 
	\label{fig:manning_attention} 
	\medskip
	\small
	..
\end{figure}

% a non-recurrent architecture, called the Transformer \citep{vaswani2017attention} 
The lack of recurrence in the transformers architecture "enables greater within-training-example parallelization, at the cost of quadratic complexity in the input sequence length" \citep{liu2018generating} 

% Jurasfy book (draft new 3rd edition), ch.10 .. \citep[ch.9-10]{jurafsky2021speech} 

the terminology used to describe the Transformer: "the attention is a function of a query (Q) and set of key (K) and value (V ) pairs" \citep{liu2018generating} 
% multi-head self-attention of the Transformer

% to reduce memory usage, limiting the dot products between Q and K

% "projecting the tokens into the query, key, and value embeddings"

% decoder-only vs encoder-decoder
% what is an attention head? See "explorable transformers" 

% explorable Bert (online javascript demo): \href{https://huggingface.co/exbert/?model=gpt2&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true}{exbert}

% possible TODOs: 
% -give detailed description of each layer dimensions (eg vocab x ..) 
% -compare bert and gpt architecture
% -explain output of gpt and bert

% save the checkpoints of gpt and bert with pythorch inspect the layers

% is dot product in attention a form of linear basis expansion?
% (from machine learning course lectures: name of "augmenting" the input ..)(..appropriate name for semantic info addition?)(LBE, linear basis expansion)(..equivalent to semantic augmentation of the input?)

% schema of the self-attention mechanism (multipling on prev inputs)

% (horizontal vs vertical attention?)

% criticism of attention mechanism (refs)
% alternative attention mechanism
% attention mechanism more efficient for NLP
% auto pruning of attention heads?


Training objective function, cross-entropy loss:
 "During training, the probability assigned to the correct word is used to calculate the cross-entropy loss for each item in the sequence. As with RNNs, the loss for a training sequence is the average cross-entropy loss over the entire sequence." (Jurasfky 3rd ed. ch.9)(figure 9.21)

% query, key, value

transformer blocks "Each provides a multi-headed self-attention unit over all input words, allowing it to capture multiple dependencies between words, while avoiding the need for recurrence. With no need to process a sentence in sequence, the model parallelizes more efficiently, and scales in a way that RNNs cannot" \citep{lau2020furiously}


\subsection{Traditional unidirectional language models (Gpt2)}

% Traditional LM can "produce sentence probabilities (density estimation) " \citep{salazar2020masked}

% todo: definition of token, distinction from words
% todo: definition/mention of tokenizations, and subword units

Conventional language models, like LSTM, TDLM, and GPT2, are unidirectional, they predict the probability of a token using only past tokens
% "conventional language models (LMs) predict wt using only past tokens W<t := (w1; : : : ; wt-1)." 
"the input [to GPT2] is a sequence of previously seen words, which are then mapped to embeddings (along with their positions) and fed to multiple layers of ‘‘transformer blocks’’ before the target word is predicted" ""Much of its power resides in these transformer blocks" \citep{lau2020furiously}


The unidirectionality of conventional LMs allows to estimate log probabilities for a sentence W via the chain rule \citep{salazar2020masked} \\
$log P_{LM}(W) = \sum_{t=1}^{|W|} log P_{LM}(w_t | W_{<t}$" 
% "which can be used out of the box to rescore hypotheses in end-to-end speech recognition and machine translation (Chan et al., 2016; Gulcehre et al., 2015), and to evaluate sentences for linguistic acceptability (Lau et al., 2017)."  \citep{salazar2020masked}

"Given a unidirectional language model, we can infer the probability of a sentence by multiplying the estimated probabilities of each token using
previously seen (left) words as context (Bengio et al., 2003):" \citep{lau2020furiously}
% →P (s) = |s| Y i =0 P(wi|w<i)
% "where s is the sentence, and wi a token in s. LSTM, TDLM, and GPT2 are unidirectional models, so they all compute sentence probability as described" \citep{lau2020furiously}

% The GPT architecture \citep{radford2018improving} ..
% TODO: intended meaning of the loss measure, sentence likelihood, contrast with sentence acceptability, .., ungrammatical sentences that are more likely than rarer grammatical ones, ..)
% (math formulas of Gpt output, activation functions..)


% C’è da aggiungere che la loss del modello gpt (score PenLP), usato come misura di accettabilità, è in realtà una stima globale della ..likelihood della frase (reference for this .. from gpt paper or other paper on ..transformers based language models). Ciò non corrisponde esattamente ad un giudizio di accettabilità/grammaticalità. Infatti, potrebbe darsi il caso, che una frase piuttosto comune (..) ma con un errore (sintattico, semantico, o altro) riceva una ..loss/likelihood maggiore di una frase. grammaticalmente corretta ma “rara”, o meglio, che usa un costrutto ricercato.. (es. ..).

% recap: average cross entropy loss should be how the gpt2 output loss is calculated (add img with Jurafsky figures)
% ..check with transformers code how it's actually calculated
% (calls torch CrossEntropyLoss, which "computes the cross entropy loss between input and target")

% gpt forward method:

% https://www.overleaf.com/learn/latex/Code_listing
%\begin{lstlisting}[language=Python]
%
%# modeling_gpt2.py
%
%class GPT2Model(GPT2PreTrainedModel):
%    def __init__(self, config):
%    	..
%    def forward(
%	    self,
%	    input_ids=None,	
%	    ..
%	 ):
%	 	..
%	 	return BaseModelOutputWithPastAndCrossAttentions(
%		 	last_hidden_state=hidden_states,
%		 	past_key_values=presents,
%		 	hidden_states=all_hidden_states,
%		 	attentions=all_self_attentions,
%		 	cross_attentions=all_cross_attentions,
%	 	)
%
%class GPT2LMHeadModel(GPT2PreTrainedModel):	
%
%	def __init__(self, config):
%		self.transformer = GPT2Model(config)
%		self.lm_head = nn.Linear(
%			config.n_embd, 
%			config.vocab_size, bias=False
%		)
%	
%	def forward(input_ids=None, ..	labels=None, ..):
%		r"""labels (`torch.LongTensor` of shape 
%		`(batch_size, sequence_length)`, *optional*):
%		Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
%		`labels = input_ids` Indices are selected in 
%		`[-100, 0, ..., config.vocab_size]` 
%		All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
%		"""
%		
%		transformer_outputs = self.transformer(input_ids, ..)
%		hidden_states = transformer_outputs[0]  # last_hidden_state or hidden_states?
%		lm_logits = self.lm_head(hidden_states)
%		shift_logits = lm_logits[..., :-1, :].contiguous()
%		shift_labels = labels[..., 1:].contiguous()
%		
%		loss_fct = CrossEntropyLoss()
%		loss = loss_fct(
%			shift_logits.view(-1, shift_logits.size(-1)), 
%			shift_labels.view(-1)
%		)
%	
%		return CausalLMOutputWithCrossAttentions(
%			loss=loss,
%			..
%		)
%\end{lstlisting}

% todo, clarify: cross entropy fun takes (input, target), but gpt2 calls it with (logits, labels), which are both the sentence_ids (the ids of the tokens in the sentence)

% pytorch CrossEntropyLoss formula:

%\begin{lstlisting}
%	
%class CrossEntropyLoss(_WeightedLoss):
%	def forward(self, input: Tensor, target: Tensor) -> Tensor:
%		return F.cross_entropy(input, target, ..)
%		
%		lm_logits = self.lm_head(hidden_states)
%\end{lstlisting}
%
%usage:
%\begin{lstlisting}
%model_output = model(
%sentence_ids_in_batch_as_tensor,
%labels=sentence_ids_in_batch_as_tensor,
%)
%loss = model_output.loss
%\end{lstlisting}

%From the pytorch cross entropy documentation, about the two parameters input and labels:
%
%"The input is expected to contain raw, unnormalized scores for each class. input has to be a Tensor of size (C)(C) for unbatched input, (minibatch, C)(minibatch,C) "

..

\subsection{Bidirectional language models (BERT)}


"BERT (Devlin et al., 2019) and its improvements to natural language understanding have spurred a rapid succession of contextual language representations (Yang et al., 2019; Liu et al., 2019; inter alia) which use larger datasets and more involved training schemes. Their success is attributed to their use of bidirectional context, often via their masked language model (MLM) objectives." \citet{salazar2020masked}

"The self-supervision task used to train BERT is the masked language-modeling  or  cloze  task,  where  one  is  given  a  text  in which  some  of  the  original  words  have  been  replaced  with  a special  mask  symbol.  The  goal  is  to  predict,  for  each  masked position,  the  original  word  that  appeared  in  the  text  (Fig.  3).To perform well on this task, the model needs to leverage the surrounding context to infer what that word could be." \citep{manning2020emergent}


% "MLMs model all pairwise interactions $w_s$ and $w_{s'}$ via dot product attention" "conditioned on past and future context" \citep{salazar2020masked}
% PLLs of models like EMLO would have different properties from [BERT] (e.g., their cross-entropies in Figure 4 may be convex instead of flat)  \citep{salazar2020masked}

% On the use of the Softmax, the logistic function (or possibly a probability measure based on the logistic function, but dividing all values for the total sum)


"BERT’s MLM objective can be viewed as stochastic maximum pseudolikelihood estimation (MPLE) \citet{wang2019bert}(Besag, 1975) on a training set  .. " \citep{salazar2020masked}
"In this way, MLMs learn an underlying joint distribution whose conditional distributions 
$w_t | W_{\setminus t}$ 
are modeled by masking at position $t$."  \citep{salazar2020masked}
% We include a further discussion in Appendix B" \citep{salazar2020masked}



% \pagebreak

\chapter{Island effects}


An \textit{island effect} is the phenomenon of a sentence becoming ungrammatical or unacceptable when there is a long-distance “filler-gap” dependency which crosses the boundaries of certain constructs, which have been metaphorically called syntactic \textit{islands} \citep{ross1967constraints, sprouse2013experimental}.

In psycholinguistic terminology (since \citep{fodor1978parsing}) a filler–gap dependency is the relationship between the antecedent or “filler” (e.g. a complementizer like “what” or ”who”) and the canonical position (often called “gap”, as a postulated empty position or element with no surface manifestation) of the predicate argument % check if only predicates, or any other constituent, like nouns, that the filler/"argument"/modifier modifies
that it replaces \citep{pearl2013syntactic, wilcox2018rnn, hawkins1999processing}.

Consider the following examples, in which we mark the gaps with underscores, and the fillers in bold:

%\begin{exe}
%	\ex\label{here}
%		\begin{xlist}
%			\ex{Susan thinks that John bought the book.} \label{sub1}
%			\ex{What does Susan think that John bought \_?} \label{sub2}
%			\ex[*]{What does Susan wonder [whether John bought \_]?} \label{sub3}
%		\end{xlist}
%\end{exe}

\begin{exe}
	\ex{Susan thinks that John bought the book.} \label{no_filler_gap_1}
	\ex{\textbf{What} does Susan think that John bought \textbf{\_}?} \label{filler_gap_1}
\end{exe}



% ? todo? separate section for the examples in italian?
% mention here: "(we will discuss equivalent examples for Italian in section ..)"

% names common to English and Italian: Andrea, Lara, ..

% % todo: add an example in italian of a filler-gap dependency
% In the (\subref{sub2}) examples. In the (\subref{sub}) examples.
%\begin{exe}
%	\ex
%	\begin{xlist}
	%		\ex Chi pensa che io abbia riscosso il pagamento?
	%		\ex ..
	%	\end{xlist}
%\end{exe}

%\begin{exe}
%	\ex[*]{Example this ungrammatical is.}
%	\ex[??]{This example questionable is.}		
%\end{exe}
%


In (\ref{no_filler_gap_1}) the argument \textit{the book } is adjacent to the predicate that selects it (\textit{bought}), while in (\ref{filler_gap_1}) the wh-word \textit{What} is not. Because of this, syntacticians sometimes call wh-dependencies as in (\ref{filler_gap_1}) \textit{long-distance} dependencies, distinguishing them from \textit{short-distance} dependencies as in (\ref{no_filler_gap_1}) where the two constituents in the dependency relationship are adjacent or nearly adjacent to each other \citep{pearl2013syntactic}.
 
The distance between the filler and the gap can be made arbitrarily long (as long as human working memory capacity permits), unless a syntactic island intervenes, as in the following examples (taken from \citet{sprouse2013experimental}), in which the syntactic islands have been marked within square brackets, and the asterisks indicate unacceptable sentences (we will discuss the respective island effect examples in Italian in \autoref{sec:island_effects_in_italian}):

\begin{exe}
	\ex{What does Susan think that Lily said that Sarah heard that John bought \_?}\label{very_long_distance_1} 
	\ex{\textsc{Whether island:} \\ *What does Susan wonder [whether John bought \_]?} \label{whether_island_1}
\end{exe}

It is worth mentioning here that island effects have been found to decrease with repeated exposure, that is, an individual repeatedly exposed to an island structure such as the one in (\ref{whether_island_1}), will find the resulting sentences acceptable or less unacceptable \citep{chaves2014subject}.

Island effects are usually named after the type of syntactic structure that creates them \citep{sprouse2013experimental}. The sentence in (\ref{whether_island_1}) exemplifies an extraction from a wh-island, specifically of the whether-island sub-type, that is from an embedded clause headed with the complementizer “whether” \citep{wilcox2018rnn}. Whether islands are among the most easily intelligible types of islands for learners \citep{pearl2013syntactic}, along with Adjunct islands (\ref{adjunct_island_1}), Complex Noun Phrase islands (\ref{complex_np_island_1}) and Subject islands (\ref{subject_island_1}), which are the four types on which we'll focus in the present thesis. The following examples are taken from \citet{sprouse2016experimental}:

\begin{exe}
	\ex{\textsc{Adjunct island} \\ *What does Susan worry [if John buys \_]?} \label{adjunct_island_1}	
	\ex{\textsc{Complex NP Island} \\ *What did Susan make [the claim that John bought \_]?}\label{complex_np_island_1} 
	\ex{\textsc{Subject island} \\ *What did Susan think that [the speech about \_] interrupted the TV show?}\label{subject_island_1}
\end{exe}

In the adjunct island example above (\ref{adjunct_island_1}), there is the extraction of a constituent (the direct object) from an embedded sentential adjunct clause. In (\ref{complex_np_island_1}), there is an extraction from a complex noun phrase (“[the claim that John bought \_”) that serves as the direct object of a subordinate clause. In (\ref{subject_island_1}), there is an extraction from a propositional phrase that modifies a noun phrase (“[the speech about \_ ]”) that occurs in the subject position (for the verb “interrupted”) of a subordinate clause \citep{wilcox2018rnn, sprouse2016experimental}.

(\ref{whether_island_1_it}-\ref{subject_island_1_it}) are analogous examples in Italian for the four island types that we assessed in the present work:

\begin{exe}
	\ex{\textsc{Whether island}} 
	\begin{xlist}
		\sn\gll *Cosa ti domandi [se io abbia comprato \_]?\\
		What yourself wonder if I have bought\\
		\glt `What do you wonder if I bought?'\label{whether_island_1_it}
	\end{xlist}
	\ex{\textsc{Adjunct island}} 
	\begin{xlist}
		\sn\gll {*Che cosa} Gianni è partito per Parigi [dopo aver fatto \_]?\\
		What Gianni is left for Paris after having done\\
		\glt `What did Gianni leave for Paris after packing?'\label{adjunct_island_1_it}
	\end{xlist}
	\ex{\textsc{Complex NP Island}} 
	\begin{xlist}
		\sn\gll *Cosa hai fatto [{l’affermazione} che il tuo amico avrebbe mandato \_]?\\
		What have done {the statement} that the your friend have sent\\
		\glt `What did you make the claim that your friend sent?'\label{complex_np_island_1_it}
	\end{xlist}
	\ex{\textsc{Subject Island}} 
	\begin{xlist}
		\sn\gll *Di chi pensi che [la {moto \_]} abbia urtato {l'auto} di Chiara?\\
		of who think that the motorbike have hit {the car} of Chiara\\
		\glt `Who do you think the motorike of hit Chiara's car?'\label{subject_island_1_it}
	\end{xlist}
\end{exe}

Island effects in Italian were first studied by \citep{rizzi1982violations}, which observed some cross-linguistic variation with respect to English. However the above four island subtypes, with  wh-dependencies, are in effect both in English and Italian \citep{rizzi1982violations, sprouse2016experimental}.

% "1 In the syntactic literature it is common to use the label “complex NP island effects” to refer both to extraction from the clausal complement of a noun, as in (2b), and from a relative clause as in (2e). In this paper we use a stricter terminology and use “complex NP island” to refer only to the clausal complement of a noun. See Cecchetto and Donati (2015) for a nonstandard view of this construction."  \citep{sprouse2016experimental}

% todo: variations of these phenomena, differences smong them, mentioned by Sprouse
% TODO: add equivalent examples also for italian
% for wh-dependencies, and subject islands in italian, is there a restriction to PPs only, or also subjects and direct objects?

% todo: mention here that there are other types of dependencies possible other than wh-dependencies.
% It should be noted here that although island effects are most commonly ..
%"Though island effects are typically exemplified by wh-dependencies, it should be noted that island effects arise with several different types of long-distance dependencies in human languages, such as relative-clause formation (3), topicalization (4), and adjective-though constructions (5):" \citep{pearl2013syntactic}
% (FILLER-GAP DEPENDENCIES, among which WH-questions and relative clauses)
% "we focus here on a single type of long-distance dependency (WH-dependencies in English)" \citep{sprouse2012test}
% other dependencies other than wh-dependencies
%"Though island effects are typically exemplified by wh-dependencies, as in (2), the same effects crucially arise with several different types of long-distance dependencies in human languages: for instance rc-dependencies (3), dependencies between the topicalized preposed constituent and the lower gap (4), and dependencies between the preposed adjective and the lower gap in adjective-though constructions (5). " \citep{sprouse2016experimental}
%


 


% importance in linguistics

% mention that it is debated in linguistics ..what causes island effects .. their nature
% (and that experimental psycholinguistic studies, sprouse, have shown, confirmed, some predictions/ assessed some theories predictions ..)

% "As an acceptability-based phenomenon, the source of island effects has long been a topic of debate within the linguistic and psycholinguistic literature. The problem lies in the fact that acceptability judgments are a behavioral response that is the result of successful sentence processing (Chomsky 1965, Schutze 1996, Sprouse and Almeida 2013), and as such could be influenced ¨ by any of the cognitive systems that are implicated in successful sentence processing, from the multiple mental representations that can be used to characterize a sentence (e.g., phonological, morphological, syntactic, semantic, pragmatic), to the different components of the parsing system that must be deployed during normal sentence comprehension (e.g., structure-building operations, ambiguity resolution heuristics, working memory systems). In short, this is the classic problem of cognitive science (mapping observable behavioral responses to unobservable cognitive constructs), exacerbated by the complexity and multi-level nature of human language. The primary empirical goal of this volume is to bring the techniques of experimental syntax (broadly construed) to bear on this particular instantiation of the cognitive science problem, and move the field one step closer to identifying the source of island effects."  \citep{sprouse2013experimental}
% "there is substantial variability within the class of grammatical approaches. There are syntactic approaches to island effects, which posit syntactic constraints known as island constraints on the formation of WH-dependencies, such as Ross’s classic COMPLEX NP CONSTRAINT (Ross 1967), Chomsky’s SUBJACENCY CONDITION (Chomsky 1973, 1986), and Huang’s CONDITION ON EXTRACTION DOMAINS (Huang 1982). There are also semantic approaches to island effects,  for example, the algebraic semantics approach to weak islands by Szabolcsi and Zwarts (1993), the event structure account of adjunct islands by Truswell (2007), and the presupposition failure account of negative islands, factive islands, and others by Abrusán (2011). Some other grammatical approaches are more pragmatic in nature (ErteschikShir 1973, Kuno 1976, 1987, Kuno \& Takami 1993, Goldberg 2007)."  \citep{sprouse2012test}

% importance of island effects in linguistics
% Since when "island effects were first investigated \citep{chomsky1965aspects, ross1967constraints} there have been literally hundreds of articles in dozens of languages devoted to the investigation of island effects, resulting in various proposals regarding the nature of island constraints and the cross-linguistic variability of island effects". \citep{pearl2013syntactic}

% "Though most of this literature is beyond the scope of the present article, it does serve to underscore\textbf{ the central role that syntactic island effects have played in the development of (generative) syntactic theory}" \citep{pearl2013syntactic}

% "Between the centrality of syntactic island effects as a topic of research in (generative syntactic theory and the reliance on a UG-based mechanism for their acquisition, it seems clear that syntactic island effects are an ideal case study in the role of innate, domain-specific learning biases in language acquisition."  \citep{pearl2013syntactic}

% mention phenomena that ameliorate the acceptabily of island effects: parasitic gaps , resumptive pronouns


\section{Island effects in Italian}\label{sec:island_effects_in_italian}

As an introductory description of island effects, we list here examples of the four types of islands in Italian for which we conducted experiments. We give a fuller description of our test suite in  \autoref{sec:testsuite_design}.



% "For this article, we decided to test syntactic island effects in English and Italian because these two languages have been cited as evidence for cross-linguistic variation in syntactic island effects (e.g., Rizzi 1982)" \citep{sprouse2016experimental}
% "The seminal study on island effects in Italian is by \citep{rizzi1982violations}, who first observed that Italian does not exhibit the same set of island effects as English." \citep{sprouse2016experimental}


%\begin{exe}
%	\ex \gll Tu pensi	che io abbia comprato il libro.\\
%			You	think 	that I have bought the book\\
%	\glt `You think I bought the book'\label{no_filler_gap_1_it}
%\end{exe}

%\begin{exe}
%	\ex{\textsc{Whether island}}
%		\begin{xlist}
%				\ex\gll *Cosa ti domandi se io abbia comprato \_?\\
%					What yourself wonder if I have bought \\
%					\glt `What do you wonder if I bought?'\label{whether_island_1_it}
%				\ex ..
%		\end{xlist}
%\end{exe}

% (Some examples taken from \citep{sprouse2016experimental})




"The other examples in (2) demonstrate that most other island types do not involve wh-words or phrases therefore can be investigated in Italian in wh-interrogatives without modification"  \citep{sprouse2016experimental}


"As mentioned in Sect. 2.2, two different types of wh-islands were used in order to circumvent the double-wh-prohibition in Italian (Rizzi 1982): for wh-dependencies, which necessarily involve one wh-word, whether-islands were used in English and se-islands (‘if’) were used in Italian; for rcdependencies, which do not unequivocally involve wh-words (or at least not the same wh-words as wh-interrogatives),6 wh-islands involving full wh-words were used." "6 Italian relative clauses are introduced by the same complementizer as embedded declarative clauses or by two different series of relative pronouns that are different from the wh-words that occur in wh-interrogative
clauses" \citep{sprouse2016experimental}



"The cross-linguistic variation of island effects in English and Italian" \citep{sprouse2016experimental}

"As we will see in the next section, the variation in island effects between English and Italian involves both interrogative clause formation and related wh-dependencies (2) and headed relative clause formation and related rc-dependencies (3)"  \citep{sprouse2016experimental}

"One fact worth noting is that Rizzi (1982) focused on relative clause formation as opposed to wh-interrogative clause formation in his study of island effects in Italian. This contrasts with most studies of island effects in English, which have tended to focus on wh-interrogatives rather than relative clauses."  \citep{sprouse2016experimental}
% does the testsuites by Wilcox focus only on relative clauses islands? (see the mention of controlling verbe tense variation and other factors)
% "Rizzi (1982) offers a principled explanation for this choice, at least with respect to wh-islands: " ("chi ha incontrato chi" era giudicata inaccettabile da Rizzi, ma ora è accettabile)

% "see also Stepanov 2007 for a broad review of the languages that demonstrate adjunct islands" \citep{sprouse2016experimental}


"Because Italian raises the possibility of variation between wh-interrogative and relative clause formation (cross-construction variation), we will investigate both constructions in this study by constructing whislands for rc-dependencies with full wh-words, and by constructing wh-islands for wh-dependencies with whether in English and se (‘if’) in Italian." \citep{sprouse2016experimental}


"all of the rc-dependencies in Italian had to be constructed with oblique (PP) argument gaps because Italian restrictive relatives can only be introduced by a relative pronoun when the head of the relative is an oblique argument (subjects and direct objects can only be introduced by the complementizer che ‘that’). This led to a systematic difference between Italian rc-dependencies, which were constructed with oblique argument gaps, and English rc-dependencies, which were constructed with direct object gaps."  \citep{sprouse2016experimental}


"Italian subject islands involve PP extraction, while English subject islands involve DP extraction" \citep{sprouse2016experimental}

"the lack of preposition stranding in Italian meant that the gap locations of the oblique (PP) arguments were not unambiguously signaled. For wh-islands, complex NP islands, and adjunct islands, we were able to minimize potential ambiguity through the careful selection of predicates. However, a pilot experiment suggested that this was not possible for subject islands because of the presence of a second NP (the object) that could be modified by the displaced PP."  \citep{sprouse2016experimental}
"To remedy this, we decided to modify the factorial definition of subject islands slightly. Whereas the factor GAP-POSITION in the canonical factorial definition in (12) varies between MATRIX SUBJECT and EMBEDDED-SUBJECT gap positions, the modified subject island definition varies between EMBEDDED-OBJECT and EMBEDDED-SUBJECT gap positions. This allows us to include a PP adjunct with both the subject and object NP, minimizing the potential for the errant interpretation of the gap location of the displaced PP. This modified design closely resembles the definitions used in traditional syntax studies, but crucially results in a non-monotonic interaction rather than a monotonic interaction (the non-parallel lines have slopes in opposite directions). In order to minimize the differences caused by this modification, subject islands in both languages and both dependency types used the modified definition, but all other island types and dependency types used the canonical design." \citep{sprouse2016experimental}


% NB: in our dataset, we mix multiple types of ajdunct clauses: conditional clauses as in Sprouse (introduced by "se"), temporal adjuncts (introduced by "prima/dopo/mentre/quando"), and one ..adversative adjunct (introduced by "ma")
" our choice to use conditional clauses as the adjunct structure in adjunct islands " "it would be interesting to test other types of adjuncts (e.g., causal adjuncts and temporal adjuncts). In this case, we chose conditional adjuncts because previous studies have demonstrated that conditional adjuncts are not transparently reducible to a processing complexity effect (Sprouse et al. 2012), therefore conditional adjuncts are a particularly good candidate for investigations of crosslinguistic variation in grammatical theories of island effects. " \citep{sprouse2016experimental}

% " the factorial design allows us to test any structure that we may be interested in, and crucially isolate a superadditive component that goes above and beyond the cost of the structure itself. This means that there is no way for the choice of structure to lead to a confound, as long as we respect the properties of the factorial design. In fact, in principle, the factorial design could be used to identify completely novel island effects this way" \citep{sprouse2016experimental}

"In the end, the differences between English and Italian with respect to piedpiping/preposition-stranding, and our decision to focus on restrictive relatives introduced by relative pronouns led to two systematic differences in materials that should be noted: (i) in order to ensure the correct gap location, subject islands use a modified factorial design (while wh-islands, complex NP islands, and adjunct islands use the canonical factorial definition),"  \citep{sprouse2016experimental} 
"and (ii) rc-dependencies in English involved direct object gaps, while rc-dependencies in Italian involved oblique (PP) argument gaps" \citep{sprouse2016experimental}

"Beyond these two systematic differences, we attempted to keep all other aspects of the materials uniform whenever possible, and to distribute any known possible confounds across the factorial design to take advantage of the subtraction logic. Each factorial island design consists of 4 lexically matched conditions, which helps to \textbf{minimize differences across conditions due to lexical content}. " \citep{sprouse2016experimental}

% ..the hegemonich generativist linguistics ..

% construction of the Sprouse dataset, controlling confounds, variability, ..
"Because Sprouse, Schütze, and Almeida (2013) created multiple tokens of each sentence type, we were able to use a distinct token for each filler sentence type for each of the four English experiments. While this introduces a small amount of variability across the four English experiments, it has the
benefit of making each of the four experiments completely (lexically) distinct. " \citep{sprouse2016experimental}

% dataset construction ..

"For the four Italian experiments, we translated each of the 32 English fillers into Italian and then used our own judgments to verify that the distribution of acceptability remained as intended (1:1 acceptable to unacceptable, relatively evenly distributed across the complete range of acceptability). Because the four Italian experiments were administered in person, only one set of filler items was used in all four experiments. The full list of fillers is available along with the materials on the first author’s website." \citep{sprouse2016experimental}

" The task in all eight experiments was a 7-point Likert scale task, with 1 at the low end and 7 at the high end of acceptability. The instructions for the task were the same as Sprouse, Schütze, and Almeida (2013), which are available on the first author’s website." \citep{sprouse2016experimental}

"we constructed linear mixed effects models with items and participants included as random factors on each of the island types using GAP-POSITION and STRUCTURE as fixed factors. This is comparable to a repeated-measures two-way ANOVA, but with participants and items entering the model simultaneously. We then calculated p-values for the two main effects and the interaction term using likelihood ratio tests. We decided to use linear mixed effects models because of their current popularity among some experimentalists; however, it should be noted that the theoretical appropriateness of treating the items in acceptability judgment experiments as a random effect is far from settled (see Wike and Church 1976 and other articles in that volume). As such, these statistical tests may be overly conservative (i.e., the p-values reported here may be too high)."  \citep{sprouse2016experimental}

"Finally, we calculated differences-in-differences scores for
each participant, and then calculated mean differences-in-differences scores for each
island as a (non-standardized) effect-size measure for each island type."  \citep{sprouse2016experimental}

"For English wh-dependencies, the four experiments revealed significant superadditive interactions for whether, complex NP, and adjunct islands, and a nearlysignificant interaction (p = .062) for subject islands. Given that subject islands with English wh-dependencies have demonstrated significant interactions in at least three previous studies (Sprouse 2007; Sprouse et al. 2011, 2012), we take this nearly significant result to be a consequence of the modified factorial design used here. We therefore choose to interpret the nearly significant result as theoretically significant. As such, these results replicate previous findings using the factorial definition of island effects (Sprouse 2007; Sprouse et al. 2011, 2012). " \citep{sprouse2016experimental}

% adjunct islands with rc-dependencies can be explained by processing difficulty only "For English rc-dependencies, the four experiments revealed significant super-additive interactions for wh, complex NP, and subject islands. \textbf{In contrast, adjunct islands demonstrated nearly perfect linear additivity} (a p-value of .992 and DD score of .01). One interesting property to note is that the island-violating sentence in the adjunct island design is rated relatively unacceptable (around -.75). This unacceptability could explain why adjunct islands have been assumed to be present for English rcdependencies, as it is only after using the factorial design that it becomes clear that this unacceptability can be completely explained by the linear sum of the effect of long-distance dependencies and the effect of island structures." \citep{sprouse2016experimental}

% doubt: nella tabella 1.1 \citep{sprouse2013experimental} sulla cross linguistic variation degli island effects, mescola dependency types (wh, rc) e island types (complex np, subject, adjunct)
% (tuttavia discuteva il fatto di usare whether come scelta particolare di design, piu' che come tipo di island..

% chiarire quanto visto nel paper di Wilcox o Hu, dove si vede che le subject islands sarebbero un tipo particolare di complex np

% todo: try to re-list, in order from "more prototypical" to less, the island effects: whether, adjunct, complex np, subject


% "7 One logically possible explanation for the variation observed in subject and adjunct islands is that the materials were confounded in the items that showed the variation. 
% For example, the lack of subject island effects with Italian rc-dependencies could be explained if those materials (and only those materials) contained post-verbal subjects instead of pre-verbal subjects. "
% "And the lack of adjunct island effects with English rc-dependencies could be explained if those materials (and only those materials) contained complement if -clauses instead of adjunct if -clauses. We believe that this sort of explanation (in which the results are the consequence of a confound) is extremely unlikely due to the careful nature of our materials construction, therefore in the discussion that follows we will take the results at face value. We have posted the entire set of materials on the first author’s website so that interested readers can assess the likelihood of these confounds for themselves." \citep{sprouse2016experimental}

% teasing apart properties, disentangling confounds:
% "The two dependency types tested in our experiments vary not only by type (wh vs. rc), but also by featural specification of the head of the dependency. In particular, wh-dependencies are introduced by bare wh-words while rc-dependencies are introduced by a combination of a nominal head followed by a wh-word. This raises the question: Was the variation in island effects observed in our experiments due to the dependency type or due to the featural specification of the heads?8 To attempt to disentangle these two properties in English, we decided to test wh-dependencies that are introduced by complex wh-phrase in which a wh-word combines with a nominal (e.g., which car) with all four island types. In principle, complex wh-phrases might behave differently from bare wh-words for two reasons. "
% "irst, they are more likely to be D-linked, i.e., their interpretation crucially depends on contextually salient sets of individuals. Second, intervention effects for complex wh-phrases have been observed to be different from intervention effects with bare wh-words, at least in child grammar (see Friedmann et al. 2009). These two properties of complex wh-phrases have led to the use of at least two terms in the literature to refer to these phrases: D-linked wh-phrases, which tends to be used to highlight the D-linking property, and lexically restricted wh-phrases, which tends to be used to highlight the contribution of the noun to Relativized Minimality effects (e.g., Friedmann et al. 2009)." \citep{sprouse2016experimental}

% formulating a prediction (expected results if an hypothesis holds or is rejected) 
% "The overlapping similarities between complex wh-phrases and bare wh-words on one hand, and complex wh-phrases and relative clause heads on the other, make the following predictions. If it is the dependency type that is driving the variation, then wh-dependencies with complex wh-phrases will display the same pattern of island effects as wh-dependencies with bare wh-words; if it is the featural specification that is driving the variation, then wh-dependencies with complex wh-phrases will display the same pattern of island effects as rc-dependencies (regardless of whether it is ultimately D-linking, intervention, or even processing difficulty issues that are underlying the featural specification effect)" \citep{sprouse2016experimental}
% "To test this question we simply changed the bare wh-words in the first four English experiments to complex wh-phrases (the full set of materials is on the first author’s website). We kept all other details of the experiment identical, including the number of participants tested (56 for each of 4 experiments), the method of recruitment (Amazon Mechanical Turk), the details of the surveys (e.g., the order of presentation of items), and the analysis of the results (z-score transformations and linear mixed effects models). "  \citep{sprouse2016experimental}
% "One welcome consequence of this minimal change in experimental materials is that these four experiments serve as both a test of complex wh-dependencies and a replication of the rc-dependency results reported in Sect. 5 (because the rc-dependency materials are unchanged in these new experiments). Given that it was the rc-dependency results that differed from previous reports in the literature, this replication is an important step in establishing the reliability of these results"   \citep{sprouse2016experimental}



% "To our knowledge this is the first published report of the results of using the factorial design to test island effects with complex wh-phrases. As such, a couple of notes about these results are in order."  \citep{sprouse2016experimental}

% results analysis that could be useful also in the thesis analyzing the plots for the models scores (but keeping in mind that the purposes are different: assessing if island effects ..exists for Sprouse, and assessing if LM models learn them for this thesis ..)
"That being said, whether islands and complex NP islands do show a specific type of amelioration effect: the island-violating sentences for whether and complex NP islands are rated higher with complex wh phrases (mean z-scores near 0) than they are with bare wh-words (mean z-scores near -.5; see Fig. 2). These rating increases lead to a concomitant decrease in effect sizes (DD scores): whether and complex NP islands with bare wh-words have DD scores of about 1.15 and 1.05 respectively, while with D-linked wh-phrases they have DD scores of about .6 and .5 respectively. This suggests that there is a type of amelioration effect on island-violating sentences, but not enough to completely eliminate the island effect itself. "  
"This amelioration appears to be specific to the \textbf{island-violating sentences}, as there does not appear to be much of a difference in the other three (grammatical) sentences in the factorial design.  In addition, there appears to be no amelioration effect of any kind for subject and adjunct islands. Taken together with the specific effect on island-violating sentences, this result accords well with the \textbf{distinction between strong and weak islands} in the literature (for a review see Szabolcsi 2006). " "It is also interesting to note that rc-dependencies do not show this amelioration effect. This result suggests that the amelioration is specific to D-linking, and not a general effect of featural specification. "
\citep{sprouse2016experimental}

"\textbf{distinction between strong and weak islands} in the literature (for a review see Szabolcsi 2006)" \citep{sprouse2016experimental}
% so from weakest to strongest ("more prototypical) islands, the order should be:
% whether, complex np, subject, adjunct (swap the last 2?)
% (protypical in this case in the reverse sense, since it makes sentences less acceptable/more marginal)
% and from weaker to strongest dependency types: wh, rc

"the altered factorial design for subject islands (necessitated by the \textbf{lack of pied-piping in Italian}), " \citep{sprouse2016experimental}

"This result may be a consequence of the altered factorial design for subject islands (necessitated by the lack of pied-piping in Italian), which may contain a \textbf{small garden-path effect} in the third condition (island-object), resulting in an underestimate of the size of the subject island effect. " \citep{sprouse2016experimental}

"we focus solely on syntactic theories of islands effects, setting aside non-syntactic theories, such as semantic and pragmatic theories of island effects, because the variation observed in our experiments does not revolve around island types that have figured prominently in the non-syntactic islands literature.9 "
% "9Although we did test wh-islands, which have figured prominently in semantic approaches to island effects such as Szabolcsi and Zwarts (1993) and Abrusán (2011), we did not find any variation in their presence, so our results do not impact debates between syntactic and semantic approaches. We did not test relative clause islands (which have figured prominently in pragmatic approaches to island effects such as ErteschikShir 1973 and Goldberg 2006), so our results do not contribute to that discussion. Finally, the conditional adjunct islands that we tested are not the same type of adjunct island in the semantic approach of Truswell (2007)." \citep{sprouse2016experimental}

% so for those island types that had negative results (no island effect) in sprouse paper.. should we change the condition for accuracy? we should also change the DD score for subject islands (but is this only in rc ones?)

% "On the one hand, there are approaches which tie island effects to the structural distinction between complements and non-complements (such as CED and structure-building approaches; cf. Sects. 7.1 and 7.2 below); on the other hand, there are approaches that trace subject and adjunct islands to locality constraints, adopting the idea that the application of movement operations are limited over certain portions of the syntactic structure (such as Subjacency, Barriers, Phases and possibly Relativized Minimality; cf. Sects. 7.3–7.6 below). We will show that our results call for modifications of all existing syntactic theories of island effects, with some of the locality-based theories looking more promising (Subjacency, Barriers, Phases, Relativized Minimality) than theories based on the complement/noncomplement distinction (CED, Structure-building)" \citep{sprouse2016experimental}

% (so there is an impact on linguisti theory from psycholinguistic studies)
"Although we leave these deeper questions to future research, we would like to note that these questions serve as an interesting example of how cross-linguistic experimental work can lead to a series of new research directions in theoretical syntax."  \citep{sprouse2016experimental}
"Taken as a whole, these results suggest that cross-linguistic experimental work has the potential to both reveal previously unobserved effects (e.g., the lack of adjunct islands with rc-dependencies in English), as well as better isolate previously observed effects (e.g., the D-linking effect on certain island-violating sentences). As such, we believe that formal experimental work deserves a prominent place in the cross-linguistic syntactician’s toolkit." \citep{sprouse2016experimental}

"The second assumption, that the specifier of CP complements of NP is not a viable landing site, has also always been necessary to accommodate complex NP islands within the original Rizzi (1982) analysis of Italian. This assumption has had no empirical consequence for English, because movement from a complex NPs would cross at least two bounding nodes in English either way. Our results do not impact this assumption at all: Italian requires this assumption under both Rizzi’s original analysis and our revised analysis." \citep{sprouse2016experimental}


\subsection{Unprocessed quotes}


% TODO, additions: the argument that island effects can be learnerd only with native bias, has been ..countered by evidence from statistical/neural language models that are able to learn at least a subset of them (refs ..)
% Argument that these models use much more data, but also consider that the human brain is much larger in terms of .."parameters" (neuron, synapses), and larger models tend to need less data to learn.

% so that research question (relevant for linguistics and language acquisition theories) has already been ..at least in part addressed
% interest in testing language models for island effects: is there actually an interest for these models to be able to learn them? Since they are such rare phenomena.. one could argue that a language model could be perfectly ..useful in applications, if it knows the other (and most commonly used) parts of syntax.

% but still ..knowing which island constraints are learned, and which are not .. can gives insights in understanding these models, what and how they learn (their internal rapresentations and the dynamics of the learning process), and what they don't. (refer to quote in the intro for usefuless in explainable AI etc.)

% " Many researchers have taken these facts to suggest that h plex structural constraints on the rules that create long constraints are referred to as island constraints, after Ro proach to explaining the patterns in 1 and 2 has had far- architecture of the language faculty, as these grammatic motivation for abstract,  complex theories of grammar " \citep{sprouse2012test}






% "subject relatives .. easier to process than the object relatives"

% "catalog of the constructions that demonstrate them"
% "Even from the brief introduction to island effects presented above, it should be clear that identifying the source of island effects requires much more than a simple catalog of the constructions that demonstrate them." \citep{sprouse2013experimental}

% "Experimental syntax provides a set of tools that goes beyond the traditional acceptability judgment experiments that have been used (to good success) in the existing literature. " "reviewing the complex patterns of island effects, both across languages and across construction types, that have been previously reported in the syntax literature" \citep{sprouse2013experimental}

% "use some of these complex patterns to tease apart the role of different levels of linguistic representation and processing in explaining the unacceptability of island effects. Second, these patterns present a list of phenomena that any comprehensive theory of island effects must explain. It is not enough for a theory of island effects to simply explain the unacceptability of island effects in one language or one construction; a comprehensive theory must also explain the complex pattern that is observed across languages and across constructions."  \citep{sprouse2013experimental}



% " The original Subjacency analysis [by Chomsky 19xx] was not without problems, even for English. It could not account for Complex NP, Relative Clause, and Adjunct islands without additional assumptions. Furthermore, it wrongly predicted that movement out of NPs in object position should be unacceptable (i.e., an Object island to parallel Subject islands). Chomsky (1986) attempted to correct these problems, as well as unify the definition of bounding node from the Subjacency Condition and barrierfrom the Empty Category Principle. Although this attempt is now generally considered a failure, it remains a classic example of two of the primary goals of high-level syntactic theorizing: correcting empirical inadequacies of previous analyses while reducing the number of objects in the ontology of the theory."  \citep{sprouse2013experimental}

% summary: it is an open problem to ..detect what causes island effects (rather than just list eamples/a catalog of their types in a given languages)





%"licensing restrictions" \citep{sprouse2013experimental} (another name for island effects/constraints)






% "In chapter 8, Kluender and Gieselman use negative islands to investigate the factors that may contribute to island effects under a reductionist approach" \citep{sprouse2013experimental}

% "In chapter 9, Dillon and Hornstein use the semantic minimal pair of nouncomplement constructions and naked-infinitive constructions to isolate to what extent purely structural (i.e., not semantic) factors play a role in the acceptability of island effects" \citep{sprouse2013experimental}

% ". In chapter 10, Goldberg explores the possibility that island effects may be reducible to an information-theoretic conflict that arises when elements are extracted from backgrounded constituents" \citep{sprouse2013experimental}

% ". In chapter 11, Kush, Omaki, and Hornstein investigate to what extent the factors that allow extraction from relative clause islands in Swedish also ameliorate extraction from relative clause islands in English." \citep{sprouse2013experimental}

"The results of the supplemental experiments confirm that dependency type is the major source of variation, not featural specification, while providing a concrete quantification of what exactly the effect of complex wh-phrases on island effects is." \citep{sprouse2016experimental}

"Second, we want to apply the factorial design cross linguistically to see if it reveals patterns of variation that are distinct from the patterns revealed by informal methods. For this article, we decided to test syntactic island effects in English and Italian because these two languages have been cited as evidence for cross-linguistic variation in syntactic island effects (e.g., Rizzi 1982)" \citep{sprouse2016experimental}


"we want to apply the factorial design across dependency types within each language. For this article, we decided to test both wh-dependencies, i.e., dependencies between a wh-word/phrase and a gap within the interrogative clause that the wh-word/phrase introduces, and relative-clause dependencies (rc-dependencies, in short), i.e., dependencies between the head introducing a headed relative clause and a gap within the relative clause. Both dependencies have been reported to display syntactic island effects, and rc-dependencies have been reported to display crosslinguistic variation (Rizzi 1982)"  \citep{sprouse2016experimental}

% in discussing possibly different results in accuracies across different island types, it is ..important to note that different types of island ..effects have been shown to have different ..impacts on acceptability (see quote above; continuum from wh, less ..restrictive, to rc, the most)

% "We also present four supplemental experiments that test the effect of complex wh-phrases (e.g., which car) on the four island types in English in an attempt to tease apart the contribution of dependency type (i.e., wh-dependency vs. rc-dependency) and the contribution of featural specification of the head of the dependency (e.g., bare wh-words in wh-dependencies vs. the nominal head and wh-word in rc-dependencies)" \citep{sprouse2016experimental}
%
%"Section 2 reviews the previous empirical observations of island effects in English and Italian (we postpone the discussion of theoretical analyses of these observations to Sect. 6 so that we also discuss our results relative to previous analyses). " \citep{sprouse2016experimental}
%
%"Section 4 explains the logic and design of the eight primary acceptability judgment experiments. Section 5 presents the results of the eight primary experiments." \citep{sprouse2016experimental}
%

% in italiano: "dipendenze filler-gap", "dipendenze a distanza" (unbounded distance constructions, long-distance dependencies)

% difference btw "wh" and "whether"
% wh is the type of dependency, the filler made of a wh word (alternative fillers? "that" in relative clauses)
% rc dependencies?
% whether is a type of island 





\subsection{Processed quotes}

% "the wh-word or wh-phrase at the beginning of the sentence, which is often called the antecedent or the filler," \citep{sprouse2016experimental}

% "2. A BRIEF INTRODUCTION TO SYNTACTIC ISLAND EFFECTS" \citep{pearl2013syntactic}
% "One of the most interesting aspects of the syntax of human languages is the fact that dependencies can exist between two non-adjacent items in a sentence. For example, in English, Noun Phrases (NPs) typically appear adjacent (or nearly adjacent) to the verbs that select them as semantic arguments (e.g., "Jack likes Lily."). However, in English wh-questions, wh-words do not appear near the verb that selects them as semantic arguments. Instead, wh-words appear at the front of the sentence (la), resulting in a \textbf{long-distance dependency} between the wh-word and the verb that selects it (we will mark the canonical position of the wh-word, which is often called \textit{gap position}, with an underscore)."  \citep{pearl2013syntactic}

% "For our purposes, filler–gap dependency refers to a relationship between a filler, which is a wh-complementizer such as ‘what’ or ‘who’, and a gap, which is an empty syntactic position licensed by the filler. In example (1a), the filler is ‘what’ and the gap appears after ‘devoured’, indicated with underscores. If the filler were not present, the gap would be ungrammatical, as in (1b). 
% (1) a. I know what the lion devoured at sunrise. 
% b.*I know that the lion devoured at sunrise." \citep{wilcox2018rnn}

% def complementizer: a complementizer is a functional category that includes those words that can be used to turn a clause into the subject or object of a sentence (eg: "that", "che")

% names of islands, which take their name from the respective constructs ..


% "filler-gap dependency processing in sentences that involve long-distance dislocation of a constituent, as in wh-questions (1a) or relativization (1b)." (Omaki and Schulz)
% "In these constructions, the parser must identify the gap position— indicated by the underlines in (1)—to assign a thematic interpretation to the dislocated constituent (called the filler; boldface type in (1))." (Omaki and Schulz)

% "syntactic island constraints" \citep{pearl2013syntactic}

% "One of the defining characteristics of these long-distance wh-dependencies is that they appear to be unconstrained by length (Chomsky 1965; Ross 1967): the distance between the wh-word and the verb that selects it can be increased by any number of words and/or clauses (lb-d). Though there is clearly an upper bound on the number of words and/or clauses that an English speaker can keep track of during sentence processing, this restriction appears to be based on the limited nature of human working memory capacity rather than an explicit grammatical restriction on the length of wh-dependencies in English. Because of this syntacticians often describe wh-dependencies as \textit{unbounded} or \textit{long-distance} dependencies" \citep{pearl2013syntactic}



% "Section 2 introduces syntactic island effects and presents the formal acceptability judgment experiments (from Sprouse, Wagers \& Phillips 2012a) that were used to quantitatively define the target state of learn" \citep{pearl2013syntactic} \\

% "I shall still postulate an empty position, in accordance with filler-gap theories. In this way both subcategorized and nonsubcategorized gap structures can be described by a shared mechanism (the gap), even though the manner of its activation is different. A subcategorized gap is activated by a lexical cooccurrence frame, a nonsubcategorized gap by a phrase structure cooccurrence possibility sanctioned by general syntactic rules."  (Hawkins 1999)

% "Though it is true that wh-dependencies are unconstrained by length, they are not entirely unconstrained. Linguists have observed that if the gap position of a w/i-dependency appears within certain syntactic structures, the resulting sentence will be unacceptable (Chomsky 1965; Ross 1967; Chomsky 1973; Huang 1982; and many others);" \citep{pearl2013syntactic}

%"(2) a. *What did you make [the claim that Jack bought ]? \\
%b. * What do you think [the joke about ] offended Jack? \\
%c. *What do you wonder [whether Jack bought ]? \\
%d. *What do you worry [if Jack buys ]? \\
%e. *What did you meet [the scientist who invented ]? \\
%f. *What did [that Jack wrote ] offend the editor? \\
%g. *What did Jack buy [a book and ]? \\
%h. *Which did Jack borrow [ book]"  \\

%"Ross (1967) used the metaphorical term island to refer to these “gap-resistant” structures, evoking the idea that the wh-word could not move from the gap position inside the island to the front of the sentence.3" 3 While it is true that this was originally a theory-laden metaphor (invoking the idea of movement that is central to transformational grammar), the term itself has been adopted by nearly all linguistic theories, therefore we will continue to use it here. A historical note: Ross (1967) attributed island effects to the illicit application of “chopping” rules within islands. Movement from islands was permitted. The prohibition against movement from islands is proposed in later
%accounts that built on Ross’s earlier work, most especially Chomsky’s Subjacency Theory (1973, 1981, 1986)." \citep{sprouse2013experimental}

% "Building on this, we will use the term island effect to refer to the unacceptability that arises when a gap position occurs within an island.4" 4" We have chosen the term island effect over the more common island violation because the former is agnostic about the source of the unacceptability (the primary question driving this volume), while the latter specifically refers to the violation of a specific (likely grammatical) constraint." \citep{sprouse2013experimental}




% "Second, dependencies spanning these islands are \textbf{still somewhat intelligible} and so can provide a more nuanced assessment of unacceptability, rather than being complete "word salad." This is because these islands are the \textbf{more acceptable} incarnations of their particular types: Complex NP islands are more acceptable than Relative Clause islands, simple Subject islands are more acceptable than sentential Subject islands, Whether islands are more acceptable than Wh-islands with full wh-words in embedded spec-CP, and conditional Adjunct islands are more acceptable than causal Adjunct islands. Thus, a successful learner must accomplish a harder task than if these islands were the less acceptable varieties: the learner must realize that dependencies spanning these more acceptable islands are still ungrammatical when compared to grammatical dependencies, even though these island-spanning dependencies are still relatively intelligible." \citep{pearl2013syntactic}


% "It is also common in the literature to refer to island effects based on the structure that creates them: WH-islands (3a), Complex Noun Phrase islands (3b), Subject islands (3c), Adjunct islands (3d), Relative Clause islands (3e), and Sentential Subject islands (3f), although some island types are more commonly referred to based on the proposed constraint that they violate, as in Coordinate Structure Constraint violations (3g) and Left Branch Extraction violations (3h)." \citep{sprouse2013experimental}

% "Drawing on the metaphor that the relevant syntactic structures are islands that prevent the wh-word from moving to the front of the sentence, Ross (1967) called the \textbf{unacceptability that arises} in these constructions \textbf{\textit{island effects}} and the syntactic constraints that he proposed to capture them \textbf{island constraints}." \citep{pearl2013syntactic}


% "First, we focus here on a single type of long-distance dependency (WH-dependencies in English) and four island types: whether islands (2a), complex NP islands (2b), subject islands (2c), and adjunct islands (2d). However, it should be noted that island effects have been observed with many different structures, such as relative clause islands (2e), sentential subject islands (2f), coordinate structures (2g), left-branch extractions (2h), factive islands, and negative islands (for review see Szabolcsi \& den Dikken 2006), and many different types of long-distance dependencies, such as relativization (3a), topicalization (3b), and adjective-though constructions (3c), to name but a few" \citep{sprouse2012test}

% we ignore here "several other facets of island effects" .. "such as the existence of island effects without displacement of the WH-word (i.e. WH-in-situ: Huang 1982, Lasnik & Saito 1984), the amelioration of island effects when a second gap is added to the sentence (i.e. parasitic gaps: Engdahl 1983, Phillips 2006), and the constrained crosslinguistic variation in island effects (e.g. Rizzi 1982, Torrego 1984)." \citep{sprouse2012test}

% "the grammatical and reductionist traditions, which tend to define island effects in terms of comparisons of pairs of sentences"  \citep{sprouse2012test}


%
%"(3) a. I like the car that you think [that Jack bought ]. \\
%b. *1 like the car that you wonder [whether Jack bought ]. \\
%(4) a. I don't know who bought most of these cars, but that car, I think [that Jack bought \_\_]. \\
%b. *1 know who bought most of these cars, but that car, I wonder [whether Jack bough\_\_]? \\
%(5) a. Smart though I think [that Jack is ], I don't trust him to do simple math. \\
%b. *Smart though I wonder [whether Jack is ], I trust him to do simple math."   \\

\subsection{Quotes for other sections}

% The processing of filler-gap dependencies 

% "in the psycholinguistic literature (cf. Goodluck & Rochemont 1992 for a useful summary)" .. "t there appears to be a consensus on the following basic point. Filler-gap dependencies are difficult structures to process, and they are characterized by a heightened processing load and a constant effort to relate the filler to its appropriate gap. Identifying the gap is not easy. It is an empty element with no surface manifestation and its presence must be inferred from its immediate environment. At the same time the filler must be held in working memory, all the other material on the path from filler to gap must be processed simultaneously, and the gap must be correctly identified and filled. This basic insight has been central to psycholinguistic theorizing ever since Wanner and Maratsos (1978) provided experimental evidence for a measurable processing load within a filler-gap domain. Numerous subsequent experiments have refined it, and there has also been neurolinguistic support using event-related brain potentials or ERPs (Kluender & Kutas 1993a, b, King & Kutas 1992, 1993)"  (Hawkins 1999)

% "unfilled filler"
% "One important finding is that there appears to be a FIRST RESORT STRATEGY in parsing" " A gap is postulated as soon as it can be and is filled with the filler" "t is first interpreted as the direct object of ask, i.e. it is assigned to the first subcategorizer encountered. This interpretation is then revised as soon as Mary is parsed, since Mary has to be understood as the direct object, and which student is returned to its unfilled filler status until the preposition about is encountered, to which it can be assigned as a complement. Clifton and Frazier define this parsing process as the ACTIVE FILLER HYPOTHESIS."  \citep{hawkins1999processing}
% "As words in the path from filler to gap are processed there appears to be a constant effort to find the gap, relate the filler to it, and thereby unload the filler from working memory by resolving the uncertainty over gap location"

% resumptive pronouns
% "Third," "grammars can avoid a filler-gap structure altogether by conventionalizing structural alternatives, such as resumptive pronouns" \citep{hawkins1999processing}


% Ross's COMPLEX NP CONSTRAINT (buy quite a few languages are counterexamples to such a general constraint)
% subsequent reformulations in terms of subjacency (Chomsky 1981) and barriers (Chomsky 1986): also to these quite a few languages are counterexamples)
% "There is also a large amount of stipulation, and an absence of motivation or explanation for constraints of generality" \citep{hawkins1999processing} 
% "Why should complex NPs be less hospitable environments for gaps than other structures? Why are there so-called wH-islands? Why are resumptive pronouns found in place of gaps in some syntactic positions and not others? We are simply told that this is the way things are. It is also apparent that there are numerous factors that contribute to wellformedness in this area-syntactic, semantic, and lexical-yet no unifying principle has been proposed for why all of them should be relevant and for why they combine together in the ways they do to give us the variation we observe." (Hawkins 1999)

% "hard to process" argument for syntactic islands "Gaps in complex NPs are ungrammatical in many languages because they are hard structures to process" (Hawkins 1999)

\subsection{Language acquisition theories and Syntactic islands}

% language acquisition and UG

% "Furthermore, the predominant analysis of syntactic island effects in generative syntactic theory is well known to rely on innate, domain-specific learning biases. For example, in the Government and Binding framework of the 1980s, syntacticians proposed a syntactic constraint called the Subjacency Condition, which basically holds that the dependency between a displaced element (e.g., a vWi-word) and the gap position cannot cross two or more bounding nodes (Chomsky 1973; Huang 1982; Lasnik \& Saito 1984, and many others). The definition of bounding nodes can vary from language to language in order to account for the various patterns of island effects that have been observed cross-linguistically. For example, the bounding nodes in English are argued to be NP (Noun Phrase) and IP (Inflection Phrase) (Chomsky 1973), while the bounding nodes in Italian and Spanish are argued to be NP and CP (Complementizer Phrase) (Rizzi 1982; Torrego 1984). Crucially, this framework assumes that the Subjacency Condition itself is part of UG, as are the possible options for bounding nodes (NP, IP, or CP). The language learner then simply needs to determine which bounding nodes are relevant for her specific language in order to learn syntactic island constraints. Although recent evolu tions of syntactic theory have terminologically abandoned Subjacency and bounding nodes, it has been argued that modern incarnations of syntactic constraints (such as phase impenetrability) are essentially formal variants of the original Subjacency analysis (Boeckx \& Grohmann 2007)."

% "potential problems posed by syntactic island effects for any theory of syntactic acquisition." "previous theories of syntactic island learning" "difficult questions about how the specific biases required by syntactic islands arise in the learner." \citep{pearl2013syntactic}
% "the syntactic theories of UG proponents." \citep{pearl2013syntactic}

% "to truly test the UG hypothesis, and in order for the resulting acquisition models to have a real impact on existing syntactic theories (Chomsky 1965), we need to choose a set of syntactic phenomena that are central to (UG-based) syntactic theories." \citep{pearl2013syntactic}
% "while the methodology for testing learning biases is relatively clear, the data required to actually perform those tests are still relatively scarce"   \citep{pearl2013syntactic}

% "Furthermore, given the relative infrequency of multiclausal WH-dependencies even in adult-directed speech, island effects raise difficult questions about how children could use their limited input to arrive at a grammar that includes long-distance dependencies that are nonetheless constrained by specific structural configurations. In this way, island effects provide a classic motivation for theories that assume domain-specific constraints on language acquisition (i.e. universal grammar)." \citep{sprouse2012test}


% "In chapter 5, Pearl and Sprouse address one of the motivations for the debate between grammatical and reductionist accounts: the claim that a grammatical approach to island effects necessitates innate linguistic constraints (i.e., Universal Grammar). Contrary to this claim, Pearl and Sprouse propose (and implement) a computational model that can learn island effects as a type of grammatical constraint from a corpus of child-directed speech without resorting to innate linguistic biases (i.e., UG)." \citep{sprouse2013experimental}


\section{Testsuite Design}\label{sec:testsuite_design}

\section{Defitions of the island structures tested here}
wh-dependencies only

\section{Cross-linguistic variation in island effects}

% "\textbf{distinction between strong and weak islands} in the literature (for a review see Szabolcsi 2006)" \citep{sprouse2016experimental, szabolcsi2006strong}
% so from weakest to strongest ("more prototypical) islands, the order should be:
% whether, complex np, subject, adjunct (swap the last 2?)

% is there a linguistic continuum in the cross-linguistic variation of island effects, from more prototipical to less ones?

%Table 1.1 in \citet{sprouse2013experimental}
%
%"Table 1.1 presents nine languages that are known to employ wh-movement in questions, and five of the most studied island effects: WH-islands (3a), Complex Noun Phrase islands (3b), Subject islands (3c), Adjunct islands (3d), and Relative Clause islands (3e). The diacritics indicate whether the specified language demonstrates that particular island effect: asterisks indicate that the island effect arises in that language, dashes indicate that the island effect does not arise in that language, and question marks indicate that the island effect arises for some sentence types, but not others." \citep{sprouse2013experimental}
%
%"We should note that Table 1.1 idealizes the empirical results to a considerable extent. There has been considerable work on these cross-linguistic differences and the differences noted here are not nearly as categorical as displayed. For example, many English speakers treat the wh-island violations discussed in Rizzi 1982b as acceptable (c.f. Grimshaw 1986)"  \citep{sprouse2013experimental}

% "Furthermore, it has long been noted that the degrees of unacceptability substantially differ across the various islands. For example, violations of the WH-island condition are generally less unacceptable than violations of the relative clause version of the Complex Noun Phrase Constraint"  \citet{sprouse2013experimental}
% "For discussion of these matters see chapter 3 (Hofmeister et al.), chapter 4 (Phillips), chapter 11 (Kush et al.), chapter 12 (Jurka), chapter 13 (Polinsky et al.)." \citep{sprouse2013experimental}

% "To the extent that Table 1.1 is accurate, the cross-linguistic variation it reports raises some very interesting questions for theories of island effects. For example, it has proven relatively difficult to characterize precisely the variability indicated by question marks; that is, island effects in certain languages that appear arise for some sentences, but not others. Furthermore, the mere existence of variability has proven challenging for approaches to island effects that postulate a source that is outside of the grammar (e.g., components of the sentence processing system), as grammatical theories have traditionally been the sole locus of cross-linguistic variation." \citep{sprouse2013experimental}

% "Whereas the first pattern in Table 1.1 primarily concerns Romance languages, the second pattern concerns Scandinavian languages (Swedish, Norwegian, Danish, and Icelandic). As first observed by Engdahl (1980) for Swedish, Scandinavian languages do not demonstrate any of these five island effects.6" \citep{sprouse2013experimental}
%"In either case, the existence of apparently island-less languages such as modern Scandinavian languages raises interesting challenges for any comprehensive theory of island effects." \citep{sprouse2013experimental}
%"6 Swedish is not bereft of apparent island effects. Rather it does not display island effects in all contexts where they are theoretically expected to appear (and as they do appear in English). For example, there are some unacceptable instances of extracting out of complex noun phrases, but others seem perfectly fine. For some discussion see chapter 11 (Kush et al.)."  \citep{sprouse2013experimental}

% " the argument/adjunct distinction that has been observed in so-called wh-in-situ languages such as Chinese (Huang 1982a), Japanese (Lasnik and Saito 1984), and Sinhala (Hagstrom 1998). In wh-in-situ languages, question formation does not involve displacement of the wh-word: the wh-word appears in the same position that the questioned constituent would appear in a declarative sentence (i.e., the gap-position in wh-movement languages)." \citep{sprouse2013experimental}

% "2.3 Resumptive pronouns"

% "Although most of the languages discussed so far exclusively employ gap positions as the foot of long-distance dependencies, about half of the world’s languages appear to allow a second option: resumptive pronouns. Resumptive" "pronouns are lexically indistinguishable from regular pronouns, but appear in the position that under other circumstances would be the gap position of a long-distance dependency (McCloskey 2006). When it comes to the interaction of resumptive pronouns and island effects, McCloskey (2006) identifies three types of languages." \citep{sprouse2013experimental}


% "The relevance of resumptive pronouns in free-variation languages for the theory of island effects rests in their dual nature (which McCloskey (2006) describes as Janus-like): whereas true gaps are canonically associated with long-distance dependencies that are sensitive to island effects, and true pronouns are canonically associated with a type of long-distance dependency that is not sensitive to island effects (i.e., binding relations), resumptive pronouns fall in between by allowing non-binding long-distance dependencies to cross island structures.8" "8It should be noted that the relevance of resumptive pronouns to island effects was first observed by Ross (1967). He noted that resumptive pronouns obviate island effects when present. As he also assumed that they are related to their antecedents via movement, he concluded that movement per se could not be island-sensitive. The approach discussed by McCloskey (2006) inverts this logic: binding is different from movement and the latter is island-sensitive while the former is not. Importantly both point to a conclusion of current interest: that overt gaps make a difference even if the dependency looks similar. The problem is not the dependency but how it is formed." \citep{sprouse2013experimental}

% "Type 3: Intrusive pronoun languages
% In the third type of language, exemplified by English, resumptive pronouns are not a grammatical option (14a versus 14b). However, native speakers tend to spontaneously produce resumptive pronouns inside of island structures as in (15a), apparently in an attempt to avoid the island effects that arise when gaps appear inside island structures (15b).10 (14) a. That’s the donkey that __ is from Brazil. b. *That’s the donkey that it is from Brazil. (15) a. *That’s the donkey that I don’t know where __ is from. b. ?That’s the donkey that I don’t know where it is from. Sells (1984) suggests that the English-type resumptive pronouns be called intrusive pronouns to distinguish them from the resumptive pronouns that appear in languages that allow resumption as a grammatical option. This, of course, raises the question of whether resumption is a unitary phenomenon, or whether there are in fact two, or even three, distinct types of resumptive pronouns in the languages of the world (see McCloskey 2006 for a discussion)."  \citep{sprouse2013experimental}

% "2.4 Parasitic gaps"
% "Parasitic gap constructions are long-distance dependencies in which the displaced element is associated with two gap positions: one gap position occurs in a licit gap location (i.e., not inside an island structure) while the other gap position occurs inside an island structure (Engdahl 1983). Whereas a single gap within an island structure results in unacceptability (16a and 17a), \textbf{the addition of another gap outside of the island structure seems to make the sentence acceptable} (16b and 17b):" \citep{sprouse2013experimental}

% "(16) a. *Which book did you laugh [before reading __]?
% b. Which book did you judge __true [before reading __parasitic]?
% (17) a. *What did [the attempt to repair __] ultimately damage the car?
% b. What did [the attempt to repair __parasitic] ultimately damage __true?"

% "The two gaps in a parasitic gap construction are often described as the true gap, which occurs outside of the island, and the parasitic gap, which occurs inside of the island. The name is a metaphorical reference to the fact that the parasitic gap could not exist without the true gap, much like a parasite cannot exist without a host."


%"The nature of the licensing restrictions on parasitic gaps is an active area of research, and as such a complete review is beyond the scope of this chapter (see Culicover and Postal 2001 for a collection of papers dedicated to parasitic gaps)."  \citep{sprouse2013experimental}
%
%" However, Culicover (2001) lays out several properties that any theory of parasitic gaps must accommodate, and therefore any theory of island effects must accommodate, three of which we will review here." \citep{sprouse2013experimental}

% "Although each of these properties of parasitic gaps receives an impressive amount of empirical support in the syntactic literature, there are also quite a few counterexamples to each property (see Culicover 2001 for a review). Each property (and counterexample) provides both an interesting question that could be addressed with experimental syntax techniques, and an interesting challenge for any comprehensive theory of island effects."  \citep{sprouse2013experimental}

% facts on island effects:
% (i) "The (potentially constrained) variation observed in Romance and Scandinavian languages with respect to types of island effects;" \citep{sprouse2013experimental}
% (iii)) The interaction of resumptive pronouns with island effects in Irish-type, Vata-type, and English-type languages;
% (iv) The existence of parasitic gaps that can (grammatically) appear inside of island structures, as well as the restrictions on their appearance." \citep{sprouse2013experimental}



\section{Factorial definition of Island Effects}


% "However, investigating the learning of syntactic island effects requires \textbf{a formally explicit definition} of the target state beyond the diacritics that are typically used to delineate unacceptable sentences in syntactic articles." \citep{pearl2013syntactic}

% "To that end, we decided to explicitly construct the target state from data from \citet{sprouse2012test}, who collected formal acceptability judgments for four island types using the magnitude estimation task: Complex NP islands (2a), (simple) Subject islands (2b), Whether islands (2c), and (conditional) Adjunct islands (2d). These four islands were selected by Sprouse, Wagers \& Phillips (2012a) for several reasons. First, they have been argued to be captured by syntactic constraints (e.g., Subjacency or the Condition on Extraction Domains), as opposed to the island types that have historically been captured with semantic constraints (e.g., factive islands, negative islands)." \citep{pearl2013syntactic}


% "Sprouse, Wagers \& Phillips used a (2x2 \textbf{factorial definition} of each island effect (shown in 6-9), which controls for the two salient syntactic properties of island-violating sentences: (i) they contain a long-distance dependency, and (ii) they contain an island structure. By translating each of these properties into separate factors, each with two levels (dependency GAP POSITION: matrix, embedded; STRUCTURE present in question: non-island, island), Sprouse, Wagers \& Phillips were able to define island effects as a superadditive interaction of the two factors—in other words, an island effect is the additional unacceptability that arises when the two factors are combined, above and beyond the independent contribution of each factor. Specifically, a syntactic island occurs when there is more unacceptability than what the EMBEDDED dependency and the presence of an ISLAND structure in the question contribute by themselves."\citep{pearl2013syntactic}

% "an acceptability judgment experiment that employs a factorial definition of island effects. First, we can isolate the effect of dependency length on acceptability by contrasting a sentence with a short WH-dependency, an extraction from a matrix clause (5a), with a sentence that contains a longer WH-dependency, an extraction from an embedded clause (5b). Similarly, we can isolate the effect of processing island structures by contrasting a sentence with an island structure (5c) with a sentence that does not contain an island structure (5a). Finally, we can measure the effect on acceptability of processing both long-distance WH-dependencies and island structures—the island effect itself—by combining both in a single sentence (5d)." \citep{sprouse2012test}

% " A matrix clause is a clause that contains another clause. Thus, the main clause in (37), the professor told the students, is a matrix clause since it contains another clause (that he was going to cancel the next class), which is said to be embedded inside the matrix clause:
% (37)
% The professor told the students that he was going to cancel the next class. ." (Martin J. Endley, Linguistic Perspectives on English Grammar. Information Age, 2010)

\section{Factorial Measurement of Island Effects in Psycholinguistic studies}


"A DD score is calculated for a two-way interaction as follows. First, calculate the difference (D1) between the scores in two of the four conditions. To make the DD scores as intuitively
meaningful as possible, we have defined D1 as the difference between the NONISLAND/EMBEDDED condition and the ISLAND/EMBEDDED condition. Second, calculate the difference (D2) between the scores in the other two conditions. For our purposes, D2 is the difference between the NONISLAND/MATRIX condition and the ISLAND/MATRIX condition. Finally, calculate the difference between these two difference scores. Intuitively, the DD score measures how much greater the effect of an island structure is in a long distance dependency sentence than in a sentence with a local dependency" \citep{sprouse2012test}

% todo: compare terminology: "DD score" in sprouse, while in Hu/Wilcox ..

\chapter{Related work}

The previous works closest to ours are those by \citet{wilcox2018rnn, hu2020systematic, sprouse2016experimental}, which use a factorial test design and test on island effects, on neural language models for \citet{wilcox2018rnn, hu2020systematic}, and on human subjects for \citet{sprouse2016experimental}

\citet{hu2020systematic} uses a factorial design with minimal pairs and obtains a percentage accuracy score. %, without the need for statistical significance
An item is considered to have been scored by the models when multiple conditions are met (eg. the unacceptable sentence is scored lower than the others, and a factorial effect measurement is greater than zero).
However, the phenomena tested by \citet{hu2020systematic} can be formulated in the stringent way of minimal pairs differing for just one word; which is not well-suited for island effects. \\
\citet{wilcox2018rnn, sprouse2016experimental} instead obtain a measure of statistical significance and a confidence interval. In the case of Sprouse, however, the format of sentences employed is not suitable for scoring language models, which require that all the sentences in an item are minimally different in terms of lexical content, and ideally should differ in just one word.
\citet{wilcox2018rnn} circumvents this issue by scoring separately items with different construct (i.e. one with and one without an island structure) which are lexically very similar but differ more than a minimal pair. For both items, they measure the effect of the filler-gap dependency phenomena, whose effect is expected to drop in the presence of an island construct, which blocks it. If the drop in the effect is statistically significant, they can conclude that the model has "learned" the island constraint. \\
Both \citet{wilcox2018rnn, hu2020systematic} tested only conventional left-to-right unidirectional language models, none tested on bidirectional language models like BERT.

%NB: the minimal pairs on Wilcox et al 2018, designed for  language model, are controlled for lexical content (indentical, basically), unlike those for human subjects in Sprouse et al. (..)

% then mention: minimal pairs, surprisal summands (PLL) for sentence scoring, .. Sprouse, ..
% mention methodological difference and problems


% wilcox instead uses a factorial design and a score of statistical significance for the measured effects (cross ..)
% (problem: we don't have training in the complex measurement they use).
% also Sprouse, in a psycholinguistic study, uses a test for statistical significance (..)

% problem: test item like those designed for humans by Sprouse, are not feasible to be used with current approaches of LM scoring.. they are not minimal pairs, which need to be controlled for lexical variation (basically they need to use the same words, or at least the same content words, with some variation in functional words)

% our test suites are problematic because they follow SProuse format, hence they are not suitable minimal pairs for LM scoring.
% that is, confound emerges, due to lexical differencies (and also variation in verbal tense and mood).
% Wilcox examples avoid raw wh questions ..


% TODO: brief summary of each of the main prev studies:
% wilcox, warstadt, linzen, marvin, salazar, Hu, ..

%"Some recent computational work explores the relation of acceptability judgments to sentence probabilities." \citep{lau2020furiously} 
%"Lau et al. (2015, 2017b) show that the output of unsupervised language models can correlate with human acceptability ratings. Warstadt et al. (2018) treat this as a semisupervised problem, training a binary classifier on top of a pre-trained sentence encoder to predict acceptability ratings with greater accuracy"\citep{lau2020furiously} 
%
%\citet{salazar2020masked}  "use PLLs to perform unsupervised acceptability judgments on the BLiMP minimal pairs set (Warstadt et al., 2020); BERT and RoBERTa models improve the state of the art (GPT-2 probabilities)
%by up to 3.9\% absolute, with +10\% on island effects and NPI licensing phenomena. Hence, PLLs can be used to assess the linguistic competence of
%MLMs in a supervision-free manner"

% Related work
% "Targeted evaluation: LM evaluation data sets using challenging prediction tasks have been proposed in the context of semantics and discourse comprehension (Zweig and Burges, 2011; Paperno et al., 2016)." \citep{marvin2018targeted}
% "Evaluation sets consisting of chalenging syntactic constructions have been constructed for parser evaluation (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011),"  \citep{marvin2018targeted}
% "and minimal pair approaches have been proposed for evaluating image captioning (Shekhar et al., 2017) and machine translation systems (Sennrich, 2017), but no data sets exist that target a range of syntactic constructions for language model evaluation." \citep{marvin2018targeted}

%\citet{salazar2020masked} "studied scoring with MLM pseudo-loglikelihood scores in a variety of settings" and found that "rescoring with PLLs can match or outperform comparable scores from large unidirectional language models (GPT-2)" They "attributed this to PLL’s promotion of fluency via self-consistency, as demonstrated by improvement on unsupervised acceptability judgements and by qualitative analysis."
%They also examined the numerical properties of PLLs \citep{salazar2020masked}.

% \citet{lau2020furiously}, try to normalize by sentence length and lexical frequency .. but actually shorter sentences can have lower likelihood. And this normalization adds nothing really to convert likelihood (..probability) to acceptability (a rare acceptable sentence might still have lower likelihood than a frequent sentence with a common mistake)
% (see ..salazar2020masked or Wilcox et al arguments on this; in particular, the fact that unifirectional models like Gpt, see a lowering curve in token surprisal as more of the sentence is revealed, and this might explain the effectiveness of the exponential normalization by sentence lenght; confound of sentence lenght with ..internal consistency of the sentence)(TODO: show the two plots by ..salazar2020masked  for bert and gpt, with flat vs descending curves)

%"Lau et al. (2017b) experiment with unsupervised language models to predict acceptability, and they obtained an encouraging correlation with human ratings" \cite{lau2020furiously}
%
%"Lau et al. (2017) compared the ability of different LMs to predict graded human acceptability judgments. The forced-choice approach used in the current paper has been shown to be effective in human
%acceptability judgment experiments (Sprouse and Almeida, 2017)." \citep{marvin2018targeted}
% "Warstadt et al. (2018) use a transfer learning approach, where an unsupervised model is finetuned on acceptability prediction."  \citep{marvin2018targeted}

%"Our work differs from those studies in that we do not advocate providing any explicit grammaticality signal to the LM at any point (“no negative evidence”)." \citep{marvin2018targeted}
%
%(masked rescoring)
%"Our work extends the closest previous works (Wang and Cho, 2019; Shin et al., 2019) with regards to experiments and tasks, as outlined in Section 2.1." \citet{salazar2020masked}

\section{Approaches for targeted linguistic evaluation of language models}

% "targeted linguistic evaluation" \citep{hu2020systematic}
% " targeted syntactic evaluation of language models" \citep{marvin2018targeted}

Linguists concerned with acceptability judgements, while "Computational language processing has traditionally been more concerned with \textit{likelihood}—the probability of a sentence being produced or encountered" \cite{lau2020furiously}
"The question of whether and how these properties are related is a fundamental one" \citep{lau2020furiously}



"Acceptability is closely related to the concept of grammaticality. The latter is a theoretical construction corresponding to syntactic wellformedness, and it is typically interpreted as a binary property (i.e., a sentence is either grammatical or ungrammatical). Acceptability, on the other hand, includes syntactic, semantic, pragmatic, and non-linguistic factors, such as sentence length. It is gradient, rather than binary, in nature (Denison, 2004; Sorace and Keller, 2005; Sprouse, 2007)" \citep{lau2020furiously}

"Overview of the approach"
"Grammaticality and LM probability" \citep{marvin2018targeted}

"How should grammaticality be captured in the probability distribution defined by an LM? The most extreme position would be that a language model should assign a probability of zero to ungrammatical sentences. For most applications, some degree of error tolerance is desirable, and  it is not practical to assign a sentence a probability of exactly zero.1 " "1Nor is it possible to have a threshold episilon such that all grammatical sentences have probability higher than episilon and all ungrammatical sentences have probability lower than episilon, for the simple reason that there is an infinite number of grammatical sentences (Lau et al., 2017)."  \citep{marvin2018targeted}

The Corpus of Linguistic Acceptability (CoLA) is the predecessor of Blimp minimal pairs approach \citet{salazar2020masked}, it has a training set and asks to label sentences as “acceptable” or not in isolation, with an absolute score \citet{warstadt2019neural}"  \citet{salazar2020masked}.

% "naturalistic approach vs a constructed dataset"
% acceptability judgements vs minimal pairs

In minimal pairs approaches, on the other hand, there is a relative comparison bewtween two sentences, and the model is considered to have given the right answer if it gives an higher score to the acceptable sentence than the unacceptable (or more marginal) one.

Another fundamental difference is that  minimal pairs scoring is usually done with the pretrained models out of the box, while acceptability judgments datasets like CoLA require the models to be finetuned on an additional dataset on an acceptability detection task.


%Minimal pairs vs sentence acceptability (with CoLA)
%acceptability: absolute score
%minimal pairs: relative score, compared btw two sentences
%also
% study by Lau et al is ..intermediate in comparing sentences that are not strictly minimal pairs
% Follow study from Trotta et al, and the original CoLA paper 

"This task is often employed to evaluate language models because the outputted probabilities for a pair of minimally different sentences are directly comparable, while the output for a single sentence cannot be taken as a measure of acceptability without some kind of normalization (Lau et al., 2016)"

"Accordingly, CoLA, but not datasets based solely on preferences between minimal pairs, may be used to evaluate models’ ability to make judgments that align with both native speaker judgments and the predictions of generative theories."

%"A language model can be evaluated on its ability to make human-like generalizations for specific syntactic phenomena (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018)"  \citep{hu2020systematic}


%"The targeted syntactic evaluation paradigm (Marvin and Linzen, 2018; Futrell et al., 2019) incorporates methods from psycholinguistic experiments, designing sentences which hold most lexical and syntactic features of each sentence constant while minimally varying features that determine grammaticality or surprise characteristics of the sentence."  \citep{hu2020systematic}

% "For example, given the two strings The keys to the cabinet are on the table and *The keys to the cabinet is on the table, a model that has learned the proper subject–verb number agreement rules for English should assign a higher probability to the grammatical plural verb in the first sentence than to the ungrammatical singular verb in the second (Linzen et al., 2016)." \citep{hu2020systematic}

%"Although some targeted syntactic evaluations, such as the example discussed above, involve simple comparisons of conditional probabilities of a word in its context, other evaluations are more complex." \citep{hu2020systematic}

%Methodologies for evaluation a Language Model linguistic knowledge:
%* Minimal pairs studies \citep{warstadt2020blimp, linzen2016assessing, marvin2018targeted, wilcox2018rnn}
%% TODO: review marvin2018targeted, wilcox2018rnn
%* "using probing tasks in which a classifier is trained to directly predict grammatical properties of a sentence  (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Tenney et al., 2019). "  \citep{warstadt2020blimp}	


% the scoring with an accuracy score of factorial items by Hu et al. lends itself to the integration into the BLiMP test suite. However, BLiMP is also constructed with templates of automatically generated minimal pairs (1000 for each phenomena), while in Hu et al there is in the order of 20 items per phenomena and they are manually constructed.

% note: the fact that BLiMP has 1000 pairs for phenomenon makes it problematic for some uses, because it's computationally expensive. Specifically, if one wants to use it as a validation set during training, to be used for ..early storpping or anyway to change training when certain syntactic phenomena have been learned by the model. There should be a smaller version of BLiMP for these uses. A smaller, manually crafted dataset of 20 sentences for phenomenon might be better suited for this use case.

% TODO: pros and cons of different approaches; currently open issues ..
%"labor-intensive nature of collecting such targeted minimal pairs." \citep{warstadt2020blimp}	

% inclusion, in LM testing, of "well-studied phenomena in linguistics such as control and raising, ellipsis, quantification, and countless others"  \citep{warstadt2020blimp}	

% ? also MDL (min descr lenght approaches?)

% brefly mention:
%Probing classifiers
%"using probing tasks in which a classifier is trained to directly predict grammatical properties of a sentence  (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018;
%Tenney et al., 2019). "  \citep{warstadt2020blimp}	

%
%"Although our method can indicate whether there is a link between fillers and gaps, \textbf{the relationship between language model probability and grammaticality is complex} (Lau et al., 2017) and\textbf{ interpreting our patterns in terms of grammaticality judgments would require auxiliary assumptions} that we don’t pursue here. To be clear: our goal is to investigate whether RNNs model the probabilistic dependencies between fillers and gaps \textit{at all}, not whether the outputs of such models can be used to \textbf{classify sentences as ‘grammatical’ or not}" \citep{wilcox2018rnn}

%"Assessing grammaticality judgments in a task which is not binary like number agreement raises\textbf{ important methodological questions}. We want the network to discriminate between (1a) and (1b). The acceptable Wh-question in (1a) might end with a question mark right at the gap (the object position of catch), but also continue with an adverbial, or even (at the coast of decreasing acceptability) a nominal containing a gap (the tail of). The ungrammatical case (1b) ends with a normal verb argument (the tail). 
%(1) a. Which mouse did the cat catch {? / last night ? / the tail of ? }
%b. *Which mouse did the cat catch the tail? The question is which NN measure best corresponds to the speaker’s perception of ungrammaticality in (1b), keeping into account that even in the theoretical and psycholinguistic literature there are no established metrics to measure ‘degrees of ungrammaticality’ (see Cowart 1997; Sorace and Keller 2005 for discussion)." \citep{chowdhury2018rnn}
%
%"Evaluation measures: We explored various possibilities, from global measures like perplexity (PPL) and non-normalized sentence cross-entropy loss (CEL), to local measures like the normalized log probability of a full stop LogPn(F S) or question mark LogPn(QM) after the current word." \citep{chowdhury2018rnn}
%
%"Perplexity and total cross-entropy loss: Both measures are based on the intuition that an ungrammatical sentence should ‘confuse’ the NN more than a corresponding grammatical one, and that this confusion will translate in a decreased ability to make correct predictions"  \citep{chowdhury2018rnn}
%"As it turns out, neither PPL or CEL are perfect ways to evaluate a language model for ungrammaticality, since they do not locate it at a specific point in the sentences. Yet, this could also be an advantage, since global measures like PPL/CEL can potentially catch parsing problems that arise earlier than expected, and record a perturbation in the NN’s later predictions as it recovers from an ungrammatical point earlier in the text. Therefore, these methods seem appropriate for a first exploration of this task." \citep{chowdhury2018rnn}
%
%The disadvantage of local measures: "as example (1a) shows, a sentence can always continue in unexpected ways. Moreover, in some cases of ungrammaticality the presence of individual words might not be a significant predictor." \citep{chowdhury2018rnn}

\subsection{Acceptability judgments (absolute scores)}
%..
%CoLA, ItaCoLA

% "To understand the relationship between sentence acceptability and probability, we conduct experiments with unsupervised language models to predict acceptability. " \citep{lau2020furiously}

% On acceptability judgments
"Acceptability judgments are the main form of behavioral data used in generative linguistics to measure human linguistic competence (Chomsky, 1965; Schutze, 1996)." \citep{warstadt2020blimp}	

% binary vs graded measurement used in psycholinguistic studies
..
% acceptability results from the cola paper:

% "Our results identify specific syntactic features that make sentences harder to classify, such as long distance dependencies (What do you think I ate?), and others that have little effect on difficulty, such as non-canonical argument structures like passives (The book was read)." (Warstadt and Bowman Linguistic Analysis of Pretrained Sentence Encoders with Acceptability Judgments)

% "Furthermore, some constructions highlight or minimize the differences between models. For example, GPT and BERT far out perform the BiLSTM on movement phenomena such as clefts (\textit{"It is Bo that left"}), yet have no advantage on sentences with adjuncts ("\textit{Sue exercises in the morning}")." 
% "We wish to exercise caution in interpreting these results since it is not clear to what extent an encoder’s failure on a particular phenomenon is due to a weakness of the encoder rather than the training data or probing classifier. However, this caveat applies to varying degrees to all probing resources. In this context of other similar linguistically informed datasets, our analysis set addresses the critical need for a evaluation task with wide coverage of linguistic phenomena"

% "Hence, unacceptable sentences in CoLA tend to be maximally similar to acceptable sentences and are unacceptable for a single identifiable reason."   (Warstadt and Bowman Linguistic Analysis of Pretrained Sentence Encoders with Acceptability Judgments)

% note the processing that was done for CoLA and ItaCoLA, ie editing less common words by checking their occurence, respectively, in the BNC and .. (a tool .. for it), noting that we ..did not check that ..for time constraints ..

% " Our large sentence encoders are limited to 100-200 million tokens of training data, which is within a factor of ten of the number of tokens human learners are exposed to during language acquisition (Hart and Risley, 1992).11  11Hart and Risley (1992) find that children in affluent families are exposed to about 45 million tokens by age 4." (Warstadt et al 2019 Neural Network Acceptability Judgments)
% Hart and Todd R. Risley. 1992. American parenting of language-learning children: Persisting differences in family-child interactions observed in natural home environments. Developmental Psychology, 28(6):1096.


% "Addressing this problem will likely involve new forms of regularization to mitigate this overfitting and, more importantly, new pretraining strategies that can help the model better learn the fundamental ingredients of grammaticality from unlabeled data" 


% Limits on the CoLA approach: "the need to train a supervised classifier on CoLA data prior to evaluation." \citep{warstadt2020blimp}	
% "This is because CoLA is designed for binary acceptability classification, and there is no generally accepted method for obtaining binary acceptability predictions from unsupervised models like LMs.2""2Though see \citet{lau2017grammaticality} for some promising proposals for normalizing LM probabilities to correlate with gradient acceptability" \citep{warstadt2020blimp}	

% other limit of fintuning for acceptability:
% adapting language models with finetuning to perform downstream tasks doesn’t necessary reflect knowledge that is already present in the LMs  \citep{warstadt2020blimp}
% "Warstadt and Bowman (2019) measure phenomenon-specific performance on CoLA for several pretrained sentence encoding models: an LSTM, GPT (Radford et al., 2018), and BERT. However, the use of supervision prevents making strong conclusions about the sentence encoding component, since it is not possible to distinguish what the encoder knows from what is learned through supervised training on acceptability data." \citep{warstadt2020blimp}




% "phenomena that linguists consider to be sensitive to hierarchical syntactic structure (Everaert et al., 2015; Xiang et al., 2009): subjectverb agreement (described in detail in Sections 4.1 and 4.2), reflexive anaphora (Section 4.3) and negative polarity items (Section 4.4)."   \citep{marvin2018targeted}


% "The Supplementary Materials provide a full description of all our templates.3" 3The code, the data set and the Supplementary Materials can be found at https://github.com/ BeckyMarvin/LM_syneval.  \citep{marvin2018targeted}

% "more work is needed to find methods for extracting reliable Boolean acceptability judgments from unsupervised language models. Our approach of fitting a threshold to the models of Lau et al. (2016) gives encouraging results, but these models are ultimately not as effective as supervised models. An alternative adopted by Linzen et al. (2016) and Marvin and Linzen (2018) is to evaluate whether language models’ assign higher probability to the acceptable sentence in a minimal pair. However, this forced choice minimal pair task, as discussed in Section 2.3, cannot be applied to CoLA, which does not exclusively contain minimal pairs"

% "Interrogatives are quite challenging in general (major feature=QUESTION). Sentences with questionlike syntax may be difficult because they usually involve extraction of a wh-word, creating a longdistance dependency between the wh-word and its extraction site, which may be difficult for models to recognize." ()


\subsection{Minimal pairs (relative scores)}

% Minimal pairs studies \citep{warstadt2020blimp, linzen2016assessing, marvin2018targeted, wilcox2018rnn}

%"Following Linzen et al. (2016) and Gulordava et al. (2018), our desideratum for the language model is more modest [rather then absolute LM probabilities reflecting grammaticality]: if two closely matched sentence differ only in their grammaticality, the probability of the grammatical sentence should be higher than the probability of the ungrammatical one. "  \citep{marvin2018targeted}

% limits, compare whole sentences: "The prediction setting is only applicable when the locus of ungrammaticality is a single word, rather than, say, the interaction between two words; moreover, the information needed to make the grammaticality decision needs to be available in the left context of the locus of grammaticality. These conditions do not always hold. Negative polarity items (NPIs), for example,.."  \citep{marvin2018targeted}


% On minimal pairs and necessity of controlling factors btw the two sentences:
% "Evaluating LMs on minimal pairs .. the caveat that " 
"the LM probability of a sentence can only serve as a proxy for acceptability if \textbf{confounding factors impacting a sentence’s probability} such as \textbf{length} and \textbf{lexical content} are \textbf{controlled} for. It is with these considerations in mind that we design BLiMP" \citep{warstadt2020blimp}

% Minimal pairs approach (since 2016) as a supplement to the Perplexity measure/metric:
% "We propose to supplement perplexity with a metric that assesses whether the probability distribution defined by the model conforms to the grammar of the language. Following previous work (Lau et al., 2017; Linzen et al., 2016; Gulordava et al., 2018), we suggest that given two sentences that differ minimally from each other, one of which is grammatical and the other which is not, it is desirable for the model to assign a higher probability to the grammatical one" \citep{marvin2018targeted}

% (as noted in the BLIMP paper itself, the minimal pairs format might not be suitable for testing some phenomena.. ie chunking, some aspects of island effects..: "Our implementation of these phenomena is often
% narrower than the linguistic definition because of the particular constraints described above."  \citep{warstadt2020blimp}	
% (more extensive list of linguistic phenomena, and how they can be ..encoded in a Blimp format, and which can't. Each phenomena has more complementaty forms in which is tested in Blimp; phenomena need to be more fine grained, ie, for island effects, all subtypes; even subtypes of phenomena might even multiple aspects/linguistic facts..)

% general note on grammaticality notation: "Following and building on linguistic traditions, we annotate examples as follows. Examples marked with a * violate a well-established grammatical constraint, and are ungrammatical. Examples marked with ? or ?? are not necessarily ungrammatical, but are marginal: for example, they may require an unusual interpretation of a word in order for the sentence to be grammatical. (More ?’s is roughly intended to indicate more severe marginality). Examples marked with ! are not ungrammatical, but induce severe processing difficulty that is measurable in real-time human sentence processing" \citep{hu2020systematic} 


% "One limitation of our approach is that 
% it is not always clear what constitutes a minimal grammaticality contrast" \citep{marvin2018targeted}
% for instance on NPI manipulations: "the members of the contrasts differed not only in their syntactic structure but also in low-level n-gram probabilities, making the performance on this particular contrast harder to interpret."


"Since most minimal pairs [in BLiMP] only differ by a single word, the effect of length on log probabilities and PLLs (discussed in Section 4.3) is mitigated"  \citep{salazar2020masked} 

\subsection{Factorial approaches}

% Sprouse, Wilcox, Hu, ..


% (Factorial design as a supplement to minimal pairs assessment)
Factorial experimental setup example: (..)
%" A targeted syntactic evaluation for garden-pathing is provided by comparing surprisals at the disambiguating word found in the set of four examples below (Futrell et al., 2019):"\citep{hu2020systematic}
%"(A) The child kicked in the chaos found ..."
%(B) The child forgotten in the chaos found . . .
%(C) The child who was kicked in the chaos found . . .
%(D) The child who was forgotten in the chaos found . . .
%
%"Successful human-like generalization involves \textbf{three criteria}: 
%(i) found should be less surprising
%(i.e., more probable) in B than A; 
%(ii) found should be more probable in C than A; 
%(iii) the C–D surprisal difference should be smaller than the A-B surprisal difference--a 2 × 2 \textit{interaction effect} on surprisal--because the syntactic disambiguation effect of not reducing the relative clause was achieved by using a part-of-speech unambiguous verb"\citep{hu2020systematic}

\citet{hu2020systematic} use these controlled tests to "describe and test for human-like syntactic knowledge in language models"
The testing paradigm presented  by \citet{hu2020systematic} "\textbf{compare critical sentence regions instead of full-sentence probabilities}, and employ a 2 × 2 paradigm with a strict, multi-fold success criterion inspired by psycholinguistics methodology"  \citep{hu2020systematic}
This allows them "to factor out as many confounds as possible, such as the lexical frequency of individual tokens and low-level n-gram statistics."  \citep{hu2020systematic}

"Each test suite contains a number of ITEMS (typically between 20 and 30), and each item appears in several CONDITIONS: across conditions, a given item will differ only according to a controlled manipulation designed to target a particular feature of grammatical knowledge. " \citep{hu2020systematic}
"Each test suite contains at least one PREDICTION, which specifies inequalities between \textbf{surprisal} values at \textbf{pairs of regions/conditions} that should hold if a model has learned the appropriate syntactic generalization. We expect language models which have learned the appropriate syntactic generalizations from their input to \textbf{satisfy these inequalities} without further fine-tuning. We compute \textbf{accuracy} on a test suite as
the proportion of items for which the model’s behavior conforms to the prediction "  \citep{hu2020systematic}
% "Most of our test suites involve 2×2 designs and a success criterion consisting of a conjunction of inequalities across conditions, as in the garden-pathing example described in Section 2.2.1 Random baseline accuracy varies by test suite and is ∼25\% overall. Most of these test suites and criteria are designed so that n-gram models cannot perform above chance for n = 5 (sometimes greater)" \citep{hu2020systematic}
% NB: our test suites are not designed to avoid that 5-gram models could score well in some items ..
% Material and code from  \citet{hu2020systematic} https://github. com/cpllab/syntactic-generalization

%Description of factorial test suites by \citet{hu2020systematic} 
%"In this work we have assembled a large number of test suites inspired by the methodology of experimental sentence-processing and psycholinguistic research." \citep{hu2020systematic} 
%"Each test suite contains a number of \textbf{ITEMS}, and each item appears in several \textbf{CONDITIONS}: across conditions, a given item will differ only according to a controlled manipulation designed to target a particular feature of grammatical  knowledge. For each suite we define a \textbf{SUCCESS CRITERION}, which stipulates inequalities among conditional probabilities of sentence substrings."\citep{hu2020systematic} 
%
%(TODO: code refactoring and thesis renamings to follow this terminology by hu2020systematic: item, conditions, success criterion)

In \citet{hu2020systematic}  "a model’s accuracy for a test
suite is computed as the percentage of the test suite’s items for which it satisfies the criterion." 
% "In this appendix, we briefly describe each test suite and the criterion used to determine whether a given model succeeds on each item of the test suite." \citep{hu2020systematic} 

%Success criteria
%"Criteria involve inequalities among \textbf{conditional probabilities} of \textbf{sentence substrings} given the complete sentence context \textbf{preceding} the substring."  \citep{hu2020systematic} 
% sentence substrings alludes to masking more than one token, ..syntactic regions: "\textbf{surprisal} values at \textbf{pairs of regions/conditions} that should hold if a model has learned the appropriate syntactic generalization."
% for BERT it would be different (left and right contexts)

%"In describing criteria, we use P(·) for raw probabilities and S(·) for surprisals (negative logprobabilities), and leave the conditioning on preceding context implicit. For concision, we use subscripts on P and S to indicate the variant of the sentence within the test suite that we are referring to."   \citep{hu2020systematic} 

%"We provide \textbf{chance accuracy} on the assumption that the order of probabilities among conditions for a given item is random. In some cases, exactly determining chance accuracy may require further assumptions about the distribution of these probabilities; in this case we provide an upper bound on
%chance accuracy. "  \citep{hu2020systematic} 

% (A) The paintingN1 that the artistN2 who lived long ago paintedV2 deterioratedV1. [correct]
% (B) #The paintingN1 that the artistN2 who lived long ago deterioratedV1 paintedV2. [incorrect]
% PA(V2V1) > PB(V1V2) References Miller and Chomsky (1963);Wilcox et al. (2019a) Ethan Wilcox, Roger P. Levy, and Richard Futrell.2019a. Hierarchical representation in neural language models: Suppression and recovery of expectations. In Proceedings of the 2019 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP

In \citet{hu2020systematic}, "Test suites are written using a standard format that allows for flexible predictions which more \textbf{closely resemble those used in psycholinguistic studies}, specifically allowing for predictions about \textbf{interactions among multiple testing conditions}."

%"Performance on each test suite is reported as a Syntactic Generalization (SG) score." \citep{hu2020systematic}
% "We group test suites into six syntactic circuits based on the linguistic representations needed to achieve high performance on each suite"  \citep{hu2020systematic}


% when evaluations of some phenomena are more complex. Example of this with an evaluation of models’ “\textbf{garden-pathing}” behavior (Futrell et al., 2019) by \citet{hu2020systematic}

% "In this paper, we broaden and deepen this line of inquiry by examining what LSTMs learn about an unexplored syntactic relationship: the filler–gap dependency." \citep{wilcox2018rnn}
% "The filler–gap dependency is novel, insofar as learning it requires the network to generalize about the absence of material." \citep{wilcox2018rnn}

% on Wilcox et al. work:
% wilcox study only on wh-dependencies (a filler like ‘what’ or ‘who’) "a gap, which is an empty syntactic position licensed by the filler. "
%  (1) a. I know what the lion devoured at sunrise.
% b.*I know that the lion devoured at sunrise.

% TODO: test filler-gap examples in italian (which are propedeutic to testing restriction violations/island effects)
% first tell if the italian LM encode correctly filler gap contrasts like 1a-b because the issues on island effects might be confounded by those on filler gaps

% "There is also a semantic relationship between the filler and the gap, in the sense that “what” is semantically the direct object of “devoured”. In this work, we study the behavior of language models, and so we treat the filler–gap dependency purely as a licensing relationship." \citep{wilcox2018rnn}


"In this paper, we look for cases where the surprisal associated with an an unusual construction—such as a gap—is ameliorated by the presence of a licensor, such as a wh-word." \citep{wilcox2018rnn}
"If the models learn that syntactic gaps require licensing, then sentences with licensors should exhibit lower surprisal than minimally different pairs that lack a proper licensor." \citep{wilcox2018rnn}

"We test whether the LSTM language models have learned filler–gap dependencies by looking for a 2x2 interaction between the presence of a gap and the presence of a wh-licensor"

"We use experimental items where the gap is located in an obligatory argument position, e.g. in subject position or as the direct object of a transitive verb, \textbf{as judged by the authors}." \citep{wilcox2018rnn}
"The phrase with the gap is embedded inside a complement clause.  We chose this paradigm over bare wh-questions because \textbf{it eliminates do-support and tense manipulation of the main verb}, resulting in \textbf{higher similarity across conditions}" \citep{wilcox2018rnn}

%"We \textbf{measure surprisal in two places}: at the word immediately following a (filled) gap and summed over the whole region from the gap to the end of the embedded clause. We look at immediate word surprisal because a gap’s licitness should have \textbf{local effects} on network \textbf{expectation}. We look at whole-region surprisal because the presence of a filler also changes expectations about overall well-formedness of the sentence—\textbf{a global phenomenon}. Until the final punctuation is reached in (2b) there are \textbf{potential gap-containing continuations that render the sentence syntactically licit} (e.g. ‘with .’). Therefore, we might expect no large spike in surprisal at any one point, but small increases in surprisal when the network encounters filled argument-structure roles and at the end of the sentence. Measuring summed surprisal captures these distributed, global effects."  \citep{wilcox2018rnn}

"In the following experiments, we examine whether RNN language models have learned constraints on filler–gap dependencies by comparing the wh-licensing interaction in non-islands to that within islands. The strongest evidence for an island constraint would be if the wh-licensing interaction goes to zero for a gap in island position, implying that, in the distribution over strings implied by the network, the appearance of a whlicensor is totally unrelated to the appearance of a gap in the island position"
\citep{wilcox2018rnn}

"More generally, we can look for a weakened wh-licensing interaction for island vs. non-island positions, which would mean
that the network believes a relationship between
the wh-licensor and the island gap is less likely"  \citep{wilcox2018rnn}
"A positive but nonzero wh-licensing interaction
would be in line with human acceptability judgments, which do not always categorically rule out gaps in island positions (Ambridge and Goldberg, 2008), and with human online processing experiments, which have shown that gap expectation is
attenuated during processing of areas where gaps
cannot occur licitly, but does not always disappear entirely (Stowe, 1986; Traxler and Pickering,
1996; Phillips, 2006). Therefore, in this section we
take a significant reduction in the island relative to
the non-island case to constitute evidence that the
model has ‘learned’ the constraint." \citep{wilcox2018rnn}

\section{Linguistic evaluation measures for language models}

% move to appropriate section: "off-the-shelf models."  \citep{hu2020systematic} 

% move to appropriate section:
"To accommodate sentence length and lexical frequency we experiment with several simple normalization methods, converting probabilities to acceptability measures (Section 3.2)." \citep{lau2020furiously}

% acceptability judgements, LM probability of a sentence, likelihood, ..



\subsection{Surprisal}

In probabilistic language models, garden-path disambiguation effects "are well captured by word negative log probabilities, or \textbf{SURPRISALS} (Hale, 2001): $S(w|C) = -log2 p(w|C)$, " \citep{hu2020systematic}
Surprisals "are independently well-established to predict human incremental processing difficulty over several orders of magnitude in word probability (Smith and Levy, 2013). " \citep{hu2020systematic}


"evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs)," \citep{salazar2020masked}
called ..LP by \citep{lau2020furiously} and .. by prev study .. (2016)
" pseudo-loglikelihood scores (PLLs) from MLMs (Wang and Cho, 2019), " \citep{salazar2020masked}

"PLL’s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP) " \citep{salazar2020masked}
% PLLs and their associated pseudo-perplexities (PPPLs)   \citep{salazar2020masked}

"To score a sentence, one creates copies with each token masked out. The log probability for each missing token is summed over copies to give the
pseudo-log-likelihood score (PLL)." (Figure from \citet{salazar2020masked})
"given by summing the conditional log probabilities log PMLM(wt j Wnt) of each sentence token (Shin et al., 2019). These are induced in BERT by replacing wt with [MASK]" \citep{salazar2020masked}

PLL’s summands  are conditional probabilities   \citep{salazar2020masked}
"Log probabilities model the joint distribution; PLL does so as well, albeit implicitly (Appendix B)"  \citep{salazar2020masked}
% one could use PLL as a “prior” or regularizer for scores given by discriminatively-finetuned BERT models in tasks like passage re-ranking (Nogueira and Cho, 2019) 

" domain shifts can be visibly observed from the positionwise scores log PMLM(wt j Wnt)" \citep{salazar2020masked}

" by learning g(W ) first. They argue \textit{g} expresses \textbf{fluency}; fixing\textit{g} early allows f(W ; X) to focus its capacity on \textbf{adequacy} in encoding the source, and thus specializing the two models. With this perspective in mind, we compare log PLM and PLL as candidates for log g."  \citep{salazar2020masked}
"In this work we interpret fluency as linguistic acceptability ..  informally, the syntactic and semantic validity of a sentence according to human judgments (Schutze ¨ , 1996)" \citep{salazar2020masked} "Its graded form is well-proxied by neural language model scores (log PLM) once length and lexical frequency
are accounted for (Lau et al., 2017)."  \citep{salazar2020masked}


\subsection{Issues with perplexity}

Perplexity: 
"perplexity on large benchmark datasets like WikiText-103 (Merity et al., 2016) has remained the primary performance metric, which cannot give detailed insight into these models’ knowledge of grammar." \citep{warstadt2020blimp}
"the most widespread currency of evaluation for language models is perplexity-how well, on average, a model predicts a word in its context" \citep{hu2020systematic}
%"Standard language models are trained to predict the next token given a context of previous tokens. Language models are typically assessed by their perplexity, the inverse geometric mean of the joint probability of words w1; : : : ; wN in a held-out test corpus C:" \citep{hu2020systematic}
% perplexity has been found to correlate with human gaze duration while reading  \citep{hu2020systematic}

% "Language models are typically evaluated using perplexity: it is considered desirable for an LM to assign a high probability to held-out data from the same corpus as the training data" \citep{marvin2018targeted}
% Perplexity issues: 
"In previous work, perplexity and syntactic judgment accuracy have been found to be partly dissociable (Kuncoro et al., 2018; Tran et al., 2018)" \citep{marvin2018targeted}
\citet{hu2020systematic} found a dissociation between perplexity and syntactic generalization performance.
% "we find a substantial dissociation between perplexity and SG score." \citep{hu2020systematic}
%"information-theoretic metrics, such as perplexity, "  \citep{hu2020systematic}

%"This measure conflates multiple sources of success (or failure) in predicting the next word: common collocations, semantics, pragmatics, syntax, and so on." 
"The quality of the syntactic predictions made by the LM is arguably particularly difficult to measure using perplexity: since most sentences are grammatically simple and most words can be predicted from their local context, perplexity rewards LMs primarily for collocational and semantic predictions." \citep{marvin2018targeted}

% (observation: a problem with perplexities is that it mixes knowledge of language with actually studying a domain to be able to predict the content; studiare a memoria il testset)

% (learning language and having good domain adaptability, learning a new domain) (as for adding semantic structure, learning a new domain involves learning patterns in that domain, understanding it; like studying for an exam and drawing diagrams to organize/systematicize the knowledge; some diagrams/patterns/insights are provided by teachers, like in supervised learning.. but a characteristic of human ..intelligence/.. is the ability to infer/find/.. patters/diagrams/.. by oneself) (the ability to perform certain reasoning patterns, that can be used in multiple domains)


"a broad-coverage metric such as perplexity may not be ideal for assessing human-like syntactic knowledge for a variety of reasons. In principle, a sentence can appear with vanishingly low probability but still be grammatically wellformed, such as \textit{Colorless green ideas sleep furiously} (Chomsky, 1957).  "  \citep{hu2020systematic}
"While perplexity remains an integral part of language model evaluation, fine-grained linguistic assessment can provide both more challenging and more interpretable tests to evaluate neural models."  \citep{hu2020systematic}



%Perplexity vs minimal pairs, previous example of an LSTM vs and early attention-only model: the LSTM "learned more robust syntactic representations, but this advantage was not reflected in its average perplexity on the corpus, since syntactically challenging sentences are relatively infrequent." \citep{marvin2018targeted}


% \subsection{Sentence acceptability estimates}


% "Most of these [Psycholinguistic] studies suggest that coherent or natural contexts should increase acceptability ratings, given that the linguistic expressions used in processing become more activated. "  \citep{lau2020furiously}
% "Warner and Glass (1987) show that such syntactic contexts can indeed affect grammaticality judgments in the expected way for garden path sentences. " \citep{lau2020furiously}


% Refs/Lit review:
% Salazar et al. ..
% 2016 paper
% ..



\subsection{Pseudo-log-likelihood scores (PLLs)}

The Formula for the sentence acceptability estimates \( LP, PenLP \) from \citet{lau2020furiously} are: (..)

"pseudo-log-likelihood scores (PLLs)" "are computed by masking tokens one by one" and summing up the resulting log probabilities of each masked token \citep{salazar2020masked}
% (link with \citet{lau2020furiously} article, and the prev one for circa 2016, that use this same score/formula, but with different names?)



"Unlike log probabilities, PLL’s summands are more uniform across an utterance’s length (no left-toright bias), helping \textbf{differentiate fluency from likeliness}" \citep{salazar2020masked}


\citet{lau2020furiously} formula for BERT sentence acceptability:
"It is important to note, however, that sentence probability computed this way is not a true probability value: These probabilities do
not sum to 1.0 over all sentences. Equation (1), in contrast, does guarantee true probabilities" \citep{lau2020furiously}
% (do they really guarantee true probabilities, summing up to 1.0 over all sentences?)
On BERT formula:
"Intuitively, the sentence probability computed with this bidirectional formulation is a measure of the model’s confidence in the likelihood of the
sentence"\citep{lau2020furiously}
"To compute the true probability, Wang and Cho (2019) show that we need to sum the pre-softmax weights for each token to score a
sentence, and then divide the score by the total
score of all sentences. As it is impractical to
compute the total score of all sentences (an
infinite set), the true sentence probabilities for
these bidirectional models are intractable. We use
our non-normalized confidence scores as stand-ins
for these probabilities."\citep{lau2020furiously}

"Sentence probability (estimated either using
unidirectional or bidirectional context) is affected
by its length (e.g., longer sentences have lower
probabilities), and word frequency (e.g., the cat is
big vs. the yak is big)" \citep{lau2020furiously}

%(normalizing by dividing the sentence probability given by a unigram language model) - log P(S) / log P_u(S)   \citep{lau2020furiously}

%\subsubsection{Softmax, logistic function, and other activation functions applied to the models' output}
% \label{sec:softmax}
We calculated the LP and PenLP measures taking as a basis the models' outputs after the softmax activation function applied to the last layer. 

% As an alternative, for BERT-based models, we also experimented with using a logistic function rather then the softmax. We called the resulting scores LP-L and PenLP-L to distinguish them from the softmax based ones. Note that with the logistic function, for the masked word prediction scores, we still obtain a value between [0, 1] for each word in the model vocabulary, but this is not a probability since the sum of all the values is greater than one.
% TODO: consider normalizing so the sum is equal to 1?.
% see paper by .. , in which, instead of the top k scores, for predictions they use the ..top-p, that is, the top words that in aggregate reach a probability threshold p. NB: the purpose of such measure is for text generation, and it doesn't seem applyable to the present study (sentence scoring).

The formulas for LP and PenLP (taken from \citep{lau2020furiously})are:

\begin{displaymath}
	LP = \log P(s)
\end{displaymath}
\begin{displaymath}
	PenLP = \frac{LP}{((5+|s|) \big/ (5+1))^\alpha}
\end{displaymath}

Where \( P(s) \) is the probability of the sentence \( s \). The PenLP divides the \( LP \) estimate by a penalty term which depends on the sentence lenght \( |s| \), measured in number of tokens. Following \citet{lau2020furiously}, we use \( \alpha=0.8 \).

For BERT-like models, we use the estimate from \citet{lau2020furiously} of the log probability LP of a sentence s. It is estimated by masking each word in s, calculating the probability of the prediction of the masked word, and summing the log of all these probabilities:
\\ ..

Numerical properties of PLL
fixing |W|, flat graph for Bert, descending curve for Gpt  
"given fixed jWj one expects -log PMLM(wt j Wnt) to be in the same range for
all t. Meanwhile -log PLM(wt j W<t) decreases as t ! jWj, the rate of which was studied in recurrent language models (Takahashi and Tanaka-Ishii, 2018)." \citet{salazar2020masked}
"the outsized cost of the unconditional first unigram in Figure 3. "
% "This also explains why bi-SANLM was more robust than uni-SANLM at shorter and earlier positions (Shin et al., 2019); the difference is intrinsic to log probabilities versus PLLs, and is not due to model or data size" \citet{salazar2020masked}
% " domain adaptation (Section 3.5) affects PLL’s positionwise cross-entropies. Cased BERT spikes at position 1, as it observes a lowercase word where a capitalized word is expected. "  \citet{salazar2020masked}
"All MLMs spike at the final token of an utterance, before our appended period “.”. Terminal words are difficult to predict in general, " "Otherwise, the averaged cross-entropies are flat." \citet{salazar2020masked}
(TODO: insert the 2 figures from salazar et al.)

"averaged cross-entropies are flat.": "This, plus our success on BLiMP, suggest positionwise scores as a way of detecting “disfluencies” (at least, those in the form of domain mismatches) by observing spikes in cross-entropy; "  \citet{salazar2020masked}
"with log PLM, spikes are confounded by the curve in Figure 3."  \citet{salazar2020masked}
"In Appendix C, we plot sentence-level PLLs versus jWj and observe linearity as jWj ! 1, with spikes from the last word and lowercase first word smoothing out." "This behavior motivates our choice of $\alpha = 1:0$ when applying the Google NMT-style length penalty (Wu et al., 2016)  to PLLs, which corresponds to the asymptoticallylinear LPMLM = (5 + jWj)=(5 + 1). " \citet{salazar2020masked}
% "In contrast, autoregressive scores like PLM(W) integrate over the inverse power-law curve in Figure 3. We speculate that this explains the effectiveness of their hyperparameter α = 0:6, widely used in NMT baselines like ours, " \citet{salazar2020masked}

the Google NMT-style length penalty (Wu et al., 2016)   \citet{salazar2020masked}


\section{Test suites}

% section "Data set construction"   \citep{marvin2018targeted}

% \citet{marvin2018targeted} "use templates to automatically construct a large number of English sentence pairs (∼350,000)."

\subsection{CoLA}

On test suite development:
"investigates acceptability judgments in real textual contexts, " \citep{lau2020furiously} 
% cite criticism in other papers (marvin and linzen) about the limitation of natural sentences: do not isolate/contrast phenomena, confound, ..sparse, ..
"naturalistic approach vs a constructed dataset"
"syntactically challenging examples are sparsely represented in a corpus" "naturally occurring sentences are difficult to control for confounds" \citep{marvin2018targeted}

Corpora of sentences and their grammaticality. 
"The most recent and comprehensive corpus is CoLA (Warstadt et al., 2019b), containing 10k sentences covering a wide variety of linguistic phenomena provided as examples in linguistics papers and books."\citep{warstadt2020blimp}	
CoLA is included in the GLUE benchmark (Wang et al., 2018)

%..
%the meaning of cross entropy (average?)
%even in the case of gpt, can it be used as an estimation/approximation of sentence acceptability?
%or is this problematic/a ..forzatura
%only in the case of minimal pairs (in the strict sense of differing by 1 word), ..
%otherwise the ..cross entropy loss ..might mean, likelihood, sentence likelihood, and still, in natural language use, some frequenty incorrect sentences are more likely than rare ones
%
%Trotta et al 2021 Monolingual and Cross-Lingual Acceptability Judgments with the Italian CoLA corpus

%"An additional check was performed to manually control for typos and transcription errors."
..

\subsection{BLiMP}

"the Benchmark of Linguistic Minimal Pairs (BLiMP) \citep{warstadt2020blimp}, a
challenge set of 67k pairs which isolate contrasts in syntax, morphology, and semantics"  \citep{salazar2020masked} 
"BLiMP provides an unsupervised setting: language models are evaluated on how often they give the acceptable sentence a higher (i.e., less negative) score." \citep{salazar2020masked} 
% "This is equivalent to 2-best rescoring without sequence model scores (log f = 0)" \citep{salazar2020masked} 

\subsection{Other (Hu et al, Wilcox et al, ..)}
..


\section{Review of results from previous related work}


"Current models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2019) now learn to give acceptability judgments that approach or even exceed individual human agreement with CoLA." \citep{warstadt2020blimp}	

"We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. " \citep{salazar2020masked}


"Our circuit-level analysis reveals consistent failure on Licensing but inconsistent behavior on other circuits, suggesting that different syntactic circuits make use of different underlying processing capacities."  \citep{hu2020systematic} 
% " Section 4 investigates whether LSTM language models are sensitive to " 
%"various constraints: wh-islands, adjunct islands, complex NP islands, and subject islands. We find that the [LSTM] language models are sensitive to some but not all
%of these constraints. " \citep{wilcox2018rnn}

\citet{warstadt2020blimp} (which did not test BERT): "current neural LMs appear to acquire robust knowledge of morphological agreement and some syntactic phenomena such as ellipsis and control/raising. They show weaker evidence of knowledge about argument structure, negative polarity item licensing, and the semantic properties of quantifiers. \textbf{All models perform at or near chance on extraction islands}. Overall, every model we evaluate falls short of human performance by a wide margin"
"ISLANDS are the hardest phenomenon by a wide margin. Only GPT-2 performs well above chance, and it remains 14 points below humans." \citet{warstadt2020blimp}

%"Some semantic phenomena, specifically those involving NPI LICENSING and QUANTIFIERS, are also challenging overall"
%
%what might the reason for lower perf on island effects: lower frequency of the phenomena, ..


"We find, in accordance with Wilcox et al. (2018), that LMs do represent long-distance wh-dependencies, but we also conclude that their representations differ fundamentally from humans’." \citet{warstadt2020blimp}
"evidence that increased dependency length and the presence of agreement attractors of the kind investigated by Linzen et al. (2016) and Gulordava et al. (2019) reduce performance on agreement phenomena." (on LSTM) \citet{warstadt2020blimp}
"Although some models approach human performance in ordinary filler-gap dependencies, they are exceptionally poor at identifying island violations overall" \citet{warstadt2020blimp}

\citet{warstadt2020blimp} : "strong conclusions about how these models represent wh-dependencies are not possible using the forced-choice task compatible with BLiMP, and a \textbf{complete assessment of syntactic islands} is \textbf{best addressed using a factorial} design that manipulates both the presence of an island and an attempt to extract from it, as in \citet{kush2018investigating} or \citet{wilcox2018rnn}" 

%"2.2 Dependent variable: Surprisal" \citep{wilcox2018rnn}
%"surprisal values that an RNN assigns to words and sentences" \citep{wilcox2018rnn}
%
%"Surprisal is log inverse probability:
%S(xi) = -log2
%p(xi
%|hi-1)," \citep{wilcox2018rnn}
%"the probability is calculated from the RNN’s softmax activation. The logarithm is taken in base 2,
%so that surprisal is measured in bits." \citep{wilcox2018rnn}
%"The degree of surprisal for a word or sentence
%tells us the extent to which that word or sentence
%is unexpected under the language model’s probability distribution"
%". It is known to correlate directly with human sentence processing difficulty (Hale, 2001; Levy, 2008; Smith and Levy, 2013)" \citep{wilcox2018rnn}
%(rather than, also also as, correlating with acceptability, or likelihood of finding such a word/sentence)


% ? riformulare il testset usando, invece di domande ("bare wh-questions"), "complement clauses" come 
% "So che il leone ha divorato una gazella all'alba." [no wh-licensor, no gap]
% *"So cosa il leone ha divorato una gazella all'alba"  [wh-licensor, no gap]
% *"So che il leone ha divorato __ all'alba" [no wh-licensor, gap]
% "So cosa il leone ha divorato __ all'alba." [wh-licensor, gap]


% differenza per un modello bidirezionale come BERT: ..sorpresa minore laddove una delle parole mascherate può rendere la frase lecita/accettabile se sostituita con un'altra parola..
% difficoltà di controllare questo per tutte le parole della frase.. a meno che la struttura delle frasi sia molto standardizzata e la cosa sia uguale per tutte
% l'italiano inoltre introduce maggiore complessità per la maggiore libertà possibile nell'ordine delle parole



"These results demonstrate that neither [RNN] model has learned the subject constraint, categorizing PPs as either licit extraction
domains in all positions (the Google model) or
treating them like islands (the Gulordava model)"  \citep{wilcox2018rnn}

"Both [RNN] models failed to correctly generalize island constraints in two conditions: The Google model failed to learn that-headed Complex-NP Islands, the Gulordava model to learn Wh-Islands, and both failed to learn Subject Islands."\citep{wilcox2018rnn}

% comparison with Wilcox experimental design:
% the sentences were split into chuncks ("wh-licensing interaction summed across all words within each region"), and the surprisal was calculated per chunck not word or token. This suggest we should split the sentences into succh cunchs (also making them more uniform), and mask a whole chunk at a time.
% "by syntactic position"

% our contributions:
% see if our examples can be adapted to Wilcox format, complementing it, or giving more examples
% the translation into italian of Wilcox test set
% the experiments
% applicazione ai modelli BERT bidirezionali (Wilcox non aveva testato neanche Gpt2 mi pare, solo LSTM)

% todo: possibly avoid OOV words ("outliers")
% masking by syntactic chunk/region (try both, word/token based, and chunk based)

% problema rispetto all'inglese, variando tra le coppie minime:
% si è accorto che .. (that)
% si è accordo di che cosa .. (what)
% composto di 3 parole invece che 1 sola

% note analisi by Wilcox:
% "error bars represent 95% confidence intervals of the contrasts between conditions"
% "Following standard practice in psycholinguistics, we derive the statistical significance of the interaction from a mixed-effects linear regression model predicting surprisal given sum-coded conditions (Baayen et al., 2008). We include random intercepts by item"
% Baayen, R.H., D.J. Davidson, and D.M. Bates. 2008. Mixedeffects modeling with crossed random effects for subjects and items. Journal of memory and language 59(4):390–412.

%- evitano frasi with "bare whquestions because it eliminates do-support and tense manipulation of the main verb, resulting in higher similarity across conditions." Provare a riformulare le frasi precedendole con "So che..", ad es. per evitare cambiamenti verbali condizionale/presente

% - crossed random effects, make use of the lmer() function in R, 

% - are the values normalized?
% yes, subtracting the per item means:
% " computed by subtracting out the by-item means before calculating the intervals as advocated in Masson and Loftus (2003). 2"

% sinificatività statistica, come derivarla dai test col formato sprouse
% (test con R, ..)
% confronto design fattoriale wilcox e sprouse:
% in wilcox filler gap dependency on off (2 esempi) si può usare al posto di long e short distance non island. ..


%diminishing returns also found \citep{wilcox2018rnn}, the two compared models have a 10x size difference for params and training data.

"In other recent work, \citet{chowdhury2018rnn} tested the ability of neural networks to separate grammatical from ungrammatical extractions using similar metrics to ours, finding that their neural networks do not represent the unboundedness of filler–gap dependencies nor certain strong island constraints." 
"We believe the difference between our results and theirs is \textbf{due to experimental design}: They choose to measure the probability of the question mark punctuation as a proxy for the RNNs gap expectation, and use \textbf{sentence schemata} instead of hand-engineered experimental items." 

"While \citet{chowdhury2018rnn} conclude that the networks are not learning island-like constraints, but rather displaying sensitivity to syntactic complexity plus order, we demonstrate island-like effects where both the island and the non-island item are equally complex (in e.g. wh-islands). " \citep{wilcox2018rnn}

%"Note also that our work is focused on finding evidence that networks represent the probabilistic contingencies implied by island constraints, without attempting to directly model grammaticality judgments."  \citep{wilcox2018rnn}
%
%All experimental materials and scripts are available at https://osf.io/zpfxm/. \citep{wilcox2018rnn}


%"Overall, these experiments provide strong evidence that both models are learning the filler–gap dependency. Furthermore, both RNN models
%are learning the flexibility of the dependency, as they exhibit similar wh-licensing effects for all three argument roles tested." \citep{wilcox2018rnn}


% section name "2. Methods" \citep{wilcox2018rnn}
% also in \citep{wilcox2018rnn}, "Experimental design" section, where they motivate the choice of sentence paradigms, and what results are expected if the model learns the respective linguistic phenomena.

"While inconsistent with the formal linguistic literature on filler–gap dependencies, the negative values of all but one of the correlations are consistent with known effects in human sentence processing, where increasing distance between fillers
and gaps usually causes processing slowdown (Grodner and Gibson, 2005; Bartek et al., 2011)."  \citep{wilcox2018rnn}

"Our work shows these dependencies and their constraints can be learned to some extent by a generic sequence model with no obvious inductive bias for hierarchical structures. This is evidence against the idea that such an inductive bias is necessary for language learning, although the amount of data these models are trained on is much larger than the typical input to a child learner." \citep{wilcox2018rnn}


%Frequency effects

% "Even on simple cases, however, the RNN’s accuracy was sensitive to the particular lexical items that occurred in the sentence; this would not be expected if its syntactic representations were fully abstract."  \citep{marvin2018targeted}

% "Lexical variation and frequency: There was considerable lexical variation in the results; we have mentioned the surprising asymmetry between himself and herself above" \citep{marvin2018targeted}

% "an asymmetry in accuracy between himself and themselves on the one hand (100% accuracy in the multi-task RNN) and herself on the other hand (49% accuracy).5 This may be because himself and themselves are significantly more frequent than herself, and consequently the number representation learned for herself was not robust. Another possibility is that gender bias reduces the probability of an anaphoric relation between herself and words such as surgeon (Rudinger et al., 2018)." \citep{marvin2018targeted}
% "Accuracy varied by verb, ranging from is and are, which had 100% accuracy, to swims, where accuracy was only 60% (recall that average accuracy was 94%). This may be a frequency effect: either the LM is learning less robust number representations for infrequent verbs, or the tail of the distribution over the vocabulary is more fragile during word prediction" \citep{marvin2018targeted}

%  on normalizations by token frequency : "such corrections should arguably not be necessary in an LM that adequately captures grammaticality"  \citep{marvin2018targeted}

% "our results show that LSTM language models are far from matching naive annotators’ performance on this task, let alone performing at 100% accuracy."  \citep{marvin2018targeted}

% "Our results contrast with the results of Gulordava et al. (2018), who obtained a prediction accuracy of 81% on English sentences from their test corpus and 74% on constructed sentences modeled after sentences from the corpus. It is likely that our sentences are more syntactically challenging than the ones they were able to find in the relatively small manually annotated treebank they used." \citep{marvin2018targeted}

%"Compression effect" of acceptability (by humans) from presence or absence of contect: 
%"Tworecentstudiesthat explorethe impact of document context on acceptability judgments both
%identify a compression effect (Bernardy et al.,
%2018; Bizzoni and Lappin, 2019). Sentences perceived to be low in acceptability when judged
%without context receive a boost in acceptability
%when judged within context. Conversely, those
%with high out-of-context acceptability see a reduction in acceptability when context is presented. It
%is unclear what causes this compression effect" 
%(this is related to the fact that shorter sentences, with less semantic cues, receive lower likelihood/more surprisal)



% "To summarize, our first important result is the exceptional performance of bidirectional models." \citep{lau2020furiously}
% "Our second result is more tentative. Our experiments seem to indicate that model architecture is more important than training or model size. " \citep{lau2020furiously}

% but Lau et al results on acceptability correlate much less with human ones when using a testset of lingusitically crafted pairs of contrasts (instead of rountrim machine translation) "Although the correlations are generally lower, we want to highlight that these linguists’ examples are artificially constructed to illustrate specific syntactic phenomena, and so this constitutes a particularly strong case of outof-domain prediction. " \citep{lau2020furiously}
% althoug the linguistic examples are much shorter "—less than 7 words on average—than the natural texts" \citep{lau2020furiously} , and this might confirm ..our finding on difficulty of shorter sentences. 
% 

%Salazar scoring Bert on Blimp, Table 7 \citep{salazar2020masked}: "Unsupervised performance (forced choice accuracy) on BLiMP using log probabilities (GPT-2) or PLLs. Human scores from Warstadt et al. (2020). " \citep{salazar2020masked}

\citet{salazar2020masked} found that, on the Blimp testset, Roberta large learns island effects close to a human level (accuracy scores are 83.4\% from Roberta and 84.9\% by humans). But as pointed out in the original Blimp paper \citep{warstadt2020blimp}, island effects phenomena are probably better tested with a factorial design like \citep{wilcox2018rnn} % that ..controls/.. for lexical differences

% Salazar on the improvement of Berta over Gpt on island effects (which \citep{warstadt2020blimp} scored at ..near chance level) 
the improvement of RoBerta over Gpt on island effect, approaching human levels "suggests that the difficulty of these BLiMP categories was due to PLM decomposing autoregressively, and not intrinsic to unsupervised language model training, as the original results may suggest \citep{warstadt2020blimp}." \citep{salazar2020masked}

"For some intuition, we include examples in Table 8. In the subject-verb agreement example, BERT sees The pamphlets and resembled those photographs when scoring have vs. has, whereas GPT-2 only sees The pamphlets, which may not be enough to counter the misleading adjacent entity Winston Churchill at scoring time" \citep{salazar2020masked}
"Who does Amanda find while thinking about Lucille?
Who does Amanda find Lucille while thinking about?"
"GPT-2 and BERT both promote fluency, but GPT-2’s left-to-right biased scores appear to cause it to \textbf{overweigh common word sequences} at the expense of adequacy" \citep{salazar2020masked}

%"Consider a two-word proper noun, e.g., W = “San Francisco”: " "It is a highly-fluent but low-probability bigram and thus gets penalized by log PLM(W). Informally, PLL(W) expresses how likely each token is given other tokens (self-consistency), while log PLM(W) expresses the unconditional probability of a sentence, beginning with the costly unconditional term PLM(San)" "Both give similar probabilities P(Francisco | San) ~= e-1:0 ~= 37\%, but differ in the first summand"  \citep{salazar2020masked}
% in sequence models like ASR, "GPT-2 restores fluency using common and repeated words, at the cost of adequacy:" "One can view these as exacerbations of the rare word problem due to overconfident logits (Nguyen and Chiang, 2018), and of over-translation (Tu et al., 2016). " "Meanwhile, BERT rewards selfconsistency, which lets rarer but still-fluent words with better acoustic or translation scores to persist:" "and promotes the more globally-fluent Union by the Union of LiberCivities. We also see the under-translation (i.e., omission) of Liber being corrected, without being discouraged by the rare sequence LiberCivities." \citep{salazar2020masked}

% ensembling not effective for PLL and LP, they overlap: "Given the differences between PLLs and log probabilities, we explore whether ensembling both improves performance in Appendix D. Similar to the largely-dominant results of MLMs on BLiMP over GPT-2 (Section 4.1), we find that as the MLM gets stronger, adding GPT-2 scores has negligible effect, suggesting that their roles overlap."  \citep{salazar2020masked}


%"The fact that classification of sentences containing only one phenomenon yields better results holds for all categories except for Questions and Auxiliary"
%
%"Interestingly, Wh-islands violation
%does not appear in the chart on the right because
%this phenomenon is always accompanied with at
%least another annotation"
%
%"(Gao et al., 2020) shows that pre-trained LM performs worse in some tasks such as linguistic acceptability task and natural language inference task since pre-trained LM barely saw his data distribution." (Wang et al 2021 Entailment as few shot learner)

%"Prior work has demonstrated that heads induce grammar formalisms and structural knowledge (Zhou and Zhao, 2019; Lin et al., 2019; Luo,
%2021), and linguistic features motivate attention
%patterns (Kovaleva et al., 2019; Clark et al., 2019)."
%"Recent studies also show that certain heads can
%have multiple functional roles (Pande et al., 2021)
%and even perform syntactic functions for distant
%languages (Ravishankar et al., 2021)."
%(Cherniavskii et al. 2022)


%note on thesis:
%our contributions are as follows:
%a dataset for some specific island effects phenomena in italian
%experiments
%..
%(as in the ItaCoLA paper)


%"The poor performance of our models on contrasts involving agreement (Singular/Pl and Reflexive) is surprising in light of findings by (Linzen et al., 2016) that LSTMs can identify agreement
%errors easily even without access to sub-word information. We speculate that this is due to underrepresentation of the relevant examples in CoLA."

% "Our results are purely correlational, and do not mark whether a particular construction is crucial for the acceptability of the sentence. Future experiments following Ettinger et al. (2018) and Kann et al. (2019) can semi-automatically generate datasets by manipulating, for example, length of long-distance dependencies, inflectional violations, or the presence of interrogatives, while controlling for factors like sentence length and word choice, in order to determine the extent to which these features impact the quality of sentence representations."


\section{Our research questions and contributions}

% (separate chapter, or section of other chapter?)}

%Context, lit review ..
%the literature on syntactic tests.. intrinsic tests
%the purpose of syntactic/linguistic fine-grained tests
%
%research question it might ask:
%
%..use of a factorial design
%is that a complementary way to test language models?
%
%does these tests in particular tells us something more/new/.. about ..what these models learn about language/how they learn it?

%how does these tests compare to previous ones? on similar phenomena (english)
%between languages (english vs italian)
%in phenomena present in both english and italian vs new phenomena present in italian but not english
%(some of these might not be .."research questions", but ..ways to analyse/interpret our results.. therefore belonging more to methodology than research questions)
%
%(todo: run the english tests, using the Sprouse english datasets, seeing the accuracy results for these models (bert, roberta, gpt)
%
%(about the tests on Bert, and the question if it's getting cues from other words in the sentence, because it's a bidirectional LM unlike gpt..)
%
%TODO: add methodology section .. find refs on "methodology", gathering and analyzing data

%explain the decision of building a manual test set
%compare with blimp autogenerated test set
%detail ..which factor are controlled in our example
%following sentences schema from sprouse et al.

% quote umberto eco at the beginning on the section on methodology (translate it)

%our contributions wrt to trotta et al 2021:
%comparable number of examples (50), but for more phenomena (4 instead of 1)
%difference in the wh-islands phenomena
%NB in fig.1 (right) trotta et al also omit the wh islands results
%problem in trotta et al for sentences with multiple phenomena
%(check if some sentences in our dataset also show any of the other phenomena tagget in trotta et al)(if not, another advange of our contribution)

(TODO)

\chapter{Experimental setup}

% "Experimental setup" with list of models, details/description  \citep{marvin2018targeted}

\section{Tested models}

\textbf{BERT} (https://huggingface.co/dbmdz/bert-base-italian-xxl-cased): \textbf{81GB} (13 billion tokens) of training data  and from Wikipedia, OPUS and OSCAR corpora. Model 
424 MB.

\textbf{GePpeTto} (LorenzoDeMattei/GePpeTto): \textbf{13.8GB} of training data from Wikipedia and ItWac corpus. The model’s size corresponds to GPT-2 small, with 12 layers and 117M parameters. Vocab size 30k. 620k training steps.

\textbf{GilBERTo} (https://huggingface.co/idb-ita/gilberto-uncased-from-camembert): Trained on \textbf{~71GB} of Italian text (11.2 billion tokens) from the OSCAR corpus. Model size 420 MB

\section{Methodology}

%data gathering, creating dataset, balancing minimal pairs / control factors, usefulness of factorial design, ..

\section{Factorial test design}

\subsection{Introduction}

Here are two sample items for the adjunct island complex NP island phenomena from the test suite we developed for this thesis:

\begin{example}
	\textsc{Adjunct islands}
	\renewcommand{\labelenumi}{\alph{enumi}.}
	\begin{enumerate}
		\item \textsc{Short-NonIsland:} \\
		Chi dice che l’autore avrebbe inviato il libro all’editore? \\
		(`Who says that the author had sent the book to the publisher?')
		\item \textsc{Long-NonIsland:} \\
		Che cosa dici che l'autore avrebbe inviato all’editore? \\
		(`What do you say that the author had sent to the publisher?')
		\item \textsc{Short-Island:} \\
		Chi ha stampato l’illustrazione dopo che l'autore ha inviato il libro all’editore? \\
		(`Who printed the illustration after the author sent the book to the publisher?')
		\item \textsc{Long-Island:} \\				
		Che cosa il disegnatore ha stampato l’illustrazione dopo che l'autore ha inviato all’editore? \\
		(`What did the designer printe the illustration after the author sent to the publisher?')
	\end{enumerate}
	\label{adjunct_item_1}
\end{example}

\begin{example}
	\textsc{Complex NP islands}
	\renewcommand{\labelenumi}{\alph{enumi}.}
	\begin{enumerate}
		\item \textsc{Short-NonIsland:} \\
		Chi ha smentito che l'agenzia avrebbe diffuso il sondaggio? \\
		(`Who denied that the agency had released the poll?')
		\item \textsc{Long-NonIsland:} \\
		Cosa hai smentito che l'agenzia avrebbe diffuso? \\
		(`What have you denied that the agency had released?')
		\item \textsc{Short-Island:} \\
		Chi ha smentito la voce che l'agenzia avrebbe diffuso il sondaggio? \\
		(`Who denied the rumor that the agency had released the poll?')
		\item \textsc{Long-Island:} \\				
		Cosa hai smentito la voce che l'agenzia avrebbe diffuso? \\
		(`What have you denied the rumor that the agency had released?')
	\end{enumerate}
	\label{complex_item_1}
\end{example}

\begin{example}
	\textsc{Whether islands}
	\renewcommand{\labelenumi}{\alph{enumi}.}
	\begin{enumerate}
		\item \textsc{Short-NonIsland:} \\
		Chi pensa che io abbia riscosso il pagamento?
		\item \textsc{Long-NonIsland:} \\
		Cosa pensi che io abbia riscosso?
		\item \textsc{Short-Island:} \\
		Chi si domanda se io abbia riscosso il pagamento?
		\item \textsc{Long-Island:} \\	
		Cosa ti domandi se io abbia riscosso?			
	\end{enumerate}
	\label{whether_item_1}
\end{example}

As in \citet{sprouse2016experimental}, this factorial design is aimed at isolating two factors that could impact the acceptability of a sentence: the effect of having a long-distance dependency (e.g. a wh-dependency), and the effect of having a complex syntactic structure like a syntactic island. Each item as \autoref{adjunct_item_1} has four sentences given by the combination of these two factors. 

In \autoref{adjunct_item_1}, the Short-NonIsland sentence has a short distance dependency, because the arguments of the main clause verb "dice" are next to it. The Long-NonIsland sentence, on the other hand, has a long-distance dependency, because the interrogative "Che cosa" depends to the verb of the subordinate clause "avrebbe inviato", as a direct object "gap".
..

% describe / introduce wh-dependencies

In the present thesis, we focus on four phenomena of island effect structures: whether islands, complex np islands, subject islands, and adjunct islands, all based on wh-dependencies (we leave rc-dependencies like those treated in \citet{sprouse2016experimental}, for future work.)



\subsubsection{Using factorial sentences as minimal pairs}


In the present thesis, we also use the four sentences of each item of the factorial test design, to draw three minimal pairs, comparing the three acceptable sentences (long and short non-island, and the short-island types) with the unacceptable one (the long-nonisland sentence type). We then score these minimal pairs for accuracy, considering that a model scores accurately a pair of sentences if it gives an higher acceptability score to the acceptable sentence rather than the unacceptable one.

..

\subsection{Test suites}

For the present thesis, we develop a new test suite that follows the same paradigm form as the one in \citet{sprouse2016experimental}, but with an increased item number of 50 (from the original 8). As in the original test suite in \citet{sprouse2016experimental}, there are four island phenomena (whether islands, complex np islands, subject islands, and adjunct islands). Each phenomena is exemplified in 50 items, which in turn are composed by four sentences as in \autoref{adjunct_item_1}, covering the combinations of two factors: presence or absence of an island structure, and a short or long-distance dependency.



\subsubsection{Scores normalization}

To be more directly comparable with the results in \citet{sprouse2016experimental}, the scores (LP or PenLP) were then discretized into a 7-point likert scale (using bins rather than quantiles) and normalized into z-scores.

Since for humans the likert scale discretization and the z-score normalization were done normalizing all the score of each individual separately, we considered a model (like a Gpt-2 instance) with a particular sentence acceptability approximation (LP or PenLP) as the equivalent of a human subject, normalizing in the same scale all its scores across the four wh-dependency island effects phenomena of a particular test suite.





\chapter{Results}


\section{Accuracy results for island effects minimal pairs in Italian}

In \autoref{tab:accuracy_it_data} we see the accuracy scores of several Gpt and BERT-based models on an Italian island effects test suite developed for the present thesis. The minimal pairs are drawn from a factorial test design whose item are like \autoref{adjunct_item_1}. 
\\ Each acceptable sentence of an item like \autoref{adjunct_item_1} is compared with the unacceptable sentence, and the score is considered accurate if the acceptable sentence receives an higher score than the unacceptable one.

For instance, on the first row of results, we see the scores for the adjunct island phenomenon (with wh-dependencies), where the acceptable Short-NonIsland sentence gets scored higher than the unacceptable sentence (Long-Island) 96 \% of the times by the Gpt-2 model with the LP sentence acceptability estimate.



All models seem to struggle with the Short-Island sentence type of the Subject islands phenomenon, with the highest score being 68\%. (TODO: see which of the 50 sentences are scored higher/lower, and why).

On average, the Gpt-2 model with the PenLP scoring measure performs best, with a 86.1\% mean accuracy.

Although it's between different languages (English and Italian), the scores on \autoref{adjunct_item_1} seem to be higher than the ones on the BLiMP benchmark in \autoref{tab:accuracy_blimp}. 
% TODO: compare BLiMP data with accuracies on Sprouse English test suite.


\begin{table} \scriptsize 
	\begin{center}
		\begin{tabular}{p{0.095\linewidth}|p{0.099\linewidth}|c|p{0.05\linewidth}|c|p{0.04\linewidth}|p{0.04\linewidth}|p{0.04\linewidth}|c|p{0.04\linewidth}|c|p{0.04\linewidth}|}
			  &  & \multicolumn{2}{c|}{\textbf{Gpt2 (it)}} & \multicolumn{4}{c|}{\textbf{BERT (it)}}  & \multicolumn{4}{c|}{\textbf{GilBERTo (it)}} \\
			 \textbf{Pheno-menon} & \textbf{Sentence form} & LP & Pen LP & LP & Pen LP & LP-L & Pen LP-L & LP & Pen LP & LP-L & Pen LP-L \\
			\hline
			\multirow{3}{0.8cm}{Wh-adjunct}  
						& Short-N.I. & \textbf{96} & 92 & 94 & 90 & \textbf{96} & \textbf{96} & 86 & 70 & 86 & 86 \\ 
					   	& Long-N.I. & \textbf{98} & 86 & 68 & 42 & 60 & 60 & 64 & 34 & 4 & 2 \\ 
		  	 		    & Short-I.S. & 96 & 98 & \textbf{100} & 98 & \textbf{100} & \textbf{100} & 94 & 94 & 84 & 88 \\ 
		  	\hline
		  	\multirow{3}{0.8cm}{Wh-complex np} 
		  				& Short-N.I. & 90 & 92 & \textbf{100} & \textbf{100} & 96 & 96 & 74 & 76 & 88 & 88 \\ 
		  			  	& Long-N.I. & \textbf{100} & 42 & 96 & 92 & 70 & 64 & 62 & 28 & 32 & 28 \\ 
		  				& Short-I.S. & 38 & 88 & \textbf{100} & \textbf{100} & 96 & 96 & 46 & 82 & 88 & 88 \\ 		  			 
		  	\hline
		  	\multirow{3}{0.8cm}{Wh-subject} 
		  				& Short-N.I. & \textbf{98} & 90 & 26 & 6 & 28 & 28 & 70 & 46 & 28 & 22 \\ 
					  	& Long-N.I. & \textbf{100} & 98 & 86 & 56 & 78 & 74 & 76 & 50 & 24 & 20 \\ 
		  				& Short-I.S. & 40 & 56 & 62 & 60 & \textbf{68} & \textbf{68} & 52 & 56 & \textbf{68} & \textbf{68} \\ 
		  	\hline
		  	\multirow{3}{0.8cm}{Wh-whether} 
		  				& Short-N.I. & 91.5 & 94.9 & 94 & 90 & \textbf{96} & \textbf{96} & 91.5 & 94.9 & 89.8 & 89.8 \\ 
					  	& Long-N.I. & \textbf{100} & \textbf{100} & 66 & 40 & 60 & 58 & \textbf{100} & 98.3 & 78 & 78 \\ 
		  				& Short-I.S. & 59.3 & 96.6 & \textbf{100} & 98 & \textbf{100} & \textbf{100} & 37.3 & 69.5 & 93.2 & 93 \\ 	
		  	\hline
		  	\textbf{Average} & & 83.9\% & \textbf{86.1\%} & 85\% & 78.4\% & 81.5\% & 80.7\% & 71.1\% & 66.6\% & 63.6\% & 62.6\%    \\ 	
		  		  	
		\end{tabular}
		\caption{Accuracy percentages for Gpt-2 and BERT Italian models, on a test suite of 50 items per phenomenon developed for the present thesis. The Gpt2-it model is LorenzoDeMattei/GePpeTto. The BERT-it model is dbmdz/bert-base-italian-xxl-cased. The GilBERTo-it model (an Italian RoBERTa variant) is idb-ita/gilberto-uncased-from-camembert.}
		\label{tab:accuracy_it_data}
	\end{center}
\end{table}

\begin{table} \scriptsize 
	\begin{center}
		\begin{tabular}{p{0.095\linewidth}|p{0.099\linewidth}|c|p{0.04\linewidth}|c|p{0.04\linewidth}|p{0.04\linewidth}|p{0.04\linewidth}|c|p{0.04\linewidth}|c|p{0.04\linewidth}|}
			&  & \multicolumn{2}{c|}{\textbf{Gpt2 (it)}} & \multicolumn{4}{c|}{\textbf{BERT (it)}}  & \multicolumn{4}{c|}{\textbf{GilBERTo (it)}} \\
			\textbf{Pheno-menon} 
				& \textbf{Sentence form} & LP & Pen LP & LP & Pen LP & LP-L & Pen LP-L & LP & Pen LP & LP-L & Pen LP-L \\
			\hline
			\multirow{3}{0.8cm}{Wh-adjunct}  
				& Short-N.I.	& 25  & 75  & \textbf{87.5}	& \textbf{87.5 }	& 75 	& 75  		& 62.5  & \textbf{87.5} & \textbf{87.5} & \textbf{87.5}   	\\ 
				& Long-N.I.  	& \textbf{75} & \textbf{75} & \textbf{75} & \textbf{75}	& 62.5 	& 62.5 	& 50 & 62.5 & \textbf{75} & \textbf{75} \\ 
				& Short-I.S.  	& 25  &  50 & \textbf{100} 	& \textbf{100} 	& \textbf{100} 	& \textbf{100} 		& 12.5  & 50   & 50   & 62.5  	\\ 
			\hline
			\multirow{3}{0.8cm}{Wh-complex np} 
				& Short-N.I. & \textbf{100}  & \textbf{100} & \textbf{100} & \textbf{100}  & \textbf{100}  & \textbf{100}  		& 62.5 & 50   & 75   & 75   \\ 
				& Long-N.I.  & \textbf{100} & 50 & \textbf{100} & \textbf{100}  & \textbf{100}  & 87.5 	& 87.5 & 37.5 & 12.5 & 12.5 \\ 
				& Short-I.S.  & 37.5	& 75  	& \textbf{100} & \textbf{100}  & \textbf{100}  & \textbf{100}  		& 62.5 & 87.5 & 87.5 & 87.5 \\ 	  			 
			\hline
			\multirow{3}{0.8cm}{Wh-subject} 
				& Short-N.I.  & \textbf{100}  & 87.5 	& 62.5 & 12.5 & 37.5 & 37.5 	& 75   & 25   & 12.5 & 0  \\ 
				& Long-N.I.  & \textbf{100}   & \textbf{100}  	& 87.5 & 87.5 & 87.5 & 87.5 	& 87.5 & 25   & 12.5 & 0  \\  
				& Short-I.S.  & 37.5 & 37.5 	& 50   & 50   & 50	 & 50  		& 75   & 62.5 & 50   & 50  \\ 
			\hline
			\multirow{3}{0.8cm}{Wh-whether} 
				& Short-N.I.  & 87.5 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\ 
				& Long-N.I.   & \textbf{100}  & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & 87.5 & 87.5  \\ 
				& Short-I.S.  & 62.5 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100}	& 25  & 62.5 & \textbf{100}  & \textbf{100} \\ 
		  	\hline
			\textbf{Average} & & 70.8\% & 79.2\% & \textbf{88.5\%}  & 84.4\%  & 84.4\% & 83.3\% & 66.7\% & 62.5\% & 62.5\% & 61.5\% \\ 	
			
		\end{tabular}
		\caption{Accuracy results for Gpt-2 and BERT Italian models, on the Italian test suite from \citet{sprouse2016experimental}.}
		\label{tab:accuracy_it_data_sprouse}
	\end{center}
\end{table}

..
% \subsection{Discussion}
% ..

% TODO: table reproducitng BLiMP accuracy scores, and scoring the same dataset on more model including BERT-linke ones (in English)
% Compare with accuracy scores on BLiMP (English) and the factorial test suites, on the same phenomena

% (also add table with accuracy scores on Sprouse test suite, 8 items per phenomenon)
% (? table on DD accuracy? ..)

% \subsection{Comparison with BLiMP results on island effects in English}
% ..

% The score on the first row of \autoref{tab:accuracy_blimp} are taken from \citet{warstadt2020blimp}

\begin{table} 
	\begin{center}
		\begin{tabular}{c|c|c|c} 
			Model & \textbf{Adjunct} & \textbf{Complex NP} & \textbf{Wh} \\
			\hline
			Gpt2 (Warstadt et al 2020) 	& 91 			& 72 			& 77 \\ 
			Gpt2-large 					& 90.2 			& 72 			& 79.1 \\ 
	  		Gpt2-medium 				& \textbf{91.6} & 72.3			& 77.9 \\
	  		Gpt2 						& 91.3 			& 68.8 			& 82.2 \\
	  		BERT-large-cased (PenLP)	& 86.3			& 67.4 			& 69.4 \\
	  		BERT-base-cased (PenLP)		& 88.1 			& 56 			& 66.2 \\
	  		RoBERTa-large (PenLP)		& 86.9			& \textbf{82.3}	& \textbf{88.2} \\
	  		RoBERTa-base(PenLP)			& 84.9			& 75.3 			& 76.2 \\  
	  		RoBERTa-base(PLL)			& 80			& 47.7 			& 83.3 \\  
	  		 
		\end{tabular}
		\caption{Accuracy results on some extraction islands phenomena in the BLiMP English test suite. The score on the first row are taken from the original paper \citet{warstadt2020blimp} 
			\\ NB: RoBERTa-base with alpha = 0.8 in the penalty term  as in Lau et al
			\\ with with alpha = 1 ..
%			\\ TODO: show table with accuracy scores on Sprouse English test data, see how they compare with BLiMP English accuracy scores.
%			\\ TODO: rerun the BLiMP and Sprouse English tests with the RoBERTa model from \citet{bostrom2020byte}, which uses a more linguistically/morphologically plausible subword splitting, to see if performance improves.
		}
		\label{tab:accuracy_blimp}
	\end{center}
\end{table}

%Results, for comparison, on Blimp dataset (island effects only), for BERT models (..), as in paper from Salazar et al 2021.
%
%Results average (over .. phenomena), from Salazar et al:
%Bert base (cased) 73.6
%Bert large (cased) 77.0
%roberta base (cased) 80.7
%roberta large (cased) 83.4
%gpt2: 71.7 (higher than Blimp paper; different acceptability formula or model?)
%
%(NB. these are cased models: "We compute PLLs on the sentences of each pair using cased BERT and RoBERTa")

%From Blimp paper:
%overall
%Gpt2: 70.6
%Human: 84.9
%
%detailed:
%phenomenon											gpt2	human
%adjunct island 										91		94
%complex NP island									72		80
%coordinate structure constraint complex left branch	42		90
%coordinate structure constraint object extraction 	88		91
%left branch island echo question 					77 		91 
%left branch island simple question 					82 		99 
%sentential subject island 							35 		61 
%wh island											77		73

%our detailed accuracy results on cased BERT-like models (with PLL without sentence lenght penalty term)(NB. In most cases, we don't find any difference in accuracy btw using the penalty term or not. Among the below phenomena, only for the left branch island echo question there is a difference, of 2.6 perc points lower, when using the penalty term):
%phenomenon											bert (base)	bert(large)	roberta (base) 	roberta (large)
%adjunct island 										88.1		86.3		84.9			86.9		
%complex NP island									56			67.4		75.3			82.3
%coordinate structure constraint complex left branch	81.4		91.1		91.2			92.0
%coordinate structure constraint object extraction 	92.1		89.0		88.4			90.5
%left branch island echo question 				 	56.8 		61.5(59.5)	71.8(71.1)		72.3 (72.9)
%left branch island simple question 					96.5		97.6		98.7			97.5
%sentential subject island 							51.5		53.7		58.9			57.6
%wh island											66.2		69.4		76.2			88.2
%average												73.575		77			80.675			83.4

%compare with our results for similar phenomena on italian bert-like models:
%
%
%(what about adjusting the testsset, or better, creating a copy, where sentences are more balanced for lenght, token frequency, semantic saliency/..)
%
%(possibly a way to estimate token frequency in corpus, is to get the model output for a sentence made of a single word, mask it, and see its softmax. Althought this is in fact context-less, or estimating the probability of finding that word by itself.. check for common words to see if this seems actually rapresentative of word/token frequency)
%
%also tweak the sentence scoring method, to check when a subword is split, and mask the whole word (all the word tokens)..
%(problem: we are still indicating how many tokens we expect in the masked position.. it would be ideal to just have a special token for "masked word", and the model also has to guess how many tokens is made of)

%\begin{lstlisting}[language=Python]
%
%tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
%[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
%\end{lstlisting}

\section{Factorial tests results}

% Surprisal ( $\displaystyle\-log P_{MLM} (w_t | W_{\setminus t}) $)

% todo: regenerate Sprouse's plots from the published score data?

% Fig.1 Results from Sprouse et al (2016)
% Fig.2 Results of acceptability judgements to the same stimuli from Sprouse et al. 2016 from an Italian Gpt-2 model (GePpeTto) with the PenLP acceptability ..score. 
% Fig.3

\subsection{Discussion on plots from Gpt-2, softmax and PenLP}

In this section we discuss the results and plots of the scores obtained from the Gpt-2 model outputs with the softmax activation function and the PenLP sentence acceptability measure. 

We choose to focus on this combination because it seems to produce in general more accurate results as seen in \autoref{tab:accuracy_it_data}. 

We include in the appendix the plots for the BERT models and the other acceptability measures (LP and PenLP based either on the model outputs after softmax of logistic activation functions).

% TODO: also model outputs based on a logistic function, but as a probability (divide for the sum of all the scores, but without the skewed exponential effect of the softmax)
..


\subsubsection{Complex np islands}

In the middle and right images on \autoref{fig:wh_complex}, we see that, for complex NP items, the long non-island sentences on average get scored by the Gpt-2 model with a lower acceptability than by the human subjects (left image). This seems to be due to the long distance dependency effect, that gets exacerbated as a sentence increases in lenght (in these test suites, the complex NP islands examples have longer sentences).

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/Chapter1/combined_wh-complex.png} 
	\caption{Comparison of plots for wh-dependencies complex NP islands} 
	\label{fig:wh_complex} % this internally labels the figure for future referencing.
	\medskip
	\small
	The first plot on the left shows the scores on humans subjects published in \citet{sprouse2016experimental} for Italian complex NP islands with wh-dependencies. For each line, the left-most edge represents the score for the short-distance dependency sentence, the right-most the long-distance dependency. The plot in the middle shows the scores from a Gpt-2 model \citep{de2020geppetto} on the same test suite used for the first plot. The plot on the right shows the scores from the same Gpt-2 model but on the expanded test suite developed for the present thesis.
\end{figure}

Doing some variations experiments (see  \autoref{tab:compare1}), we found that by replacing the main clause verbs with "intuire che"/"avere l'intuizione che" ("to sense that" / "to have the intuition that") or "avvertire che"/"avere il sentore che" ("to feel that" / "to have an inkling that") or "percepire che"/"avere la percezione che" ("feel that"/"to have the feeling that"), the island restriction violation seem to have no effect, and the Long-Island sentence becomes more acceptable than the same sentence without the island structure (Long-NonIsland).
 
Indeed, the sentence \textit{Cosa hai avuto l'intuizione che il portavoce avrebbe confermato?} (\textit{`What did you have the intuition that the spokeperson had confirmed?'}), seems acceptable despite extracting from a complex NP construct. 

An hypothesis, whose demonstration we leave for future work, is that this is due to main clause expressions in which the subject has the semantic role of an \textsc{experiencer} rather than an \textsc{agent}, which could be a condition for Complex NP island restrictions to enter into effect or not. 
% "avrebbe ppt" (condizionale passato)
% In fact, it seems that the Gpt-2 model correctly captures the fact that with these constructs the long nonisland sentences become acceptable.

\begin{table} \scriptsize 
	\begin{center}
		\begin{tabular}
			{p{0.3\linewidth} p{0.08\linewidth} p{0.3\linewidth} p{0.08\linewidth} p{0.08\linewidth}|} \\
			\multicolumn{2}{c}{\textbf{\textsc{Long-NonIsland}}} & \multicolumn{2}{c}{\textbf{\textsc{Long-Island}}}  &   \\
			\textbf{text} & \textbf{PenLP} & \textbf{text} & \textbf{PenLP} &  \textbf{Diff}  \\
			\hline
			\textit{Cosa hai messo in dubbio che il portavoce avrebbe confermato?} & -31.65
			& \textit{Cosa hai messo in dubbio la previsione che il portavoce avrebbe confermato} & -33.21 & 1.56 \\ 
			\textit{Cosa hai intuito che il portavoce avrebbe confermato?} & -32.36 
			& \textit{Cosa hai avuto l'intuizione che il portavoce avrebbe confermato?} & -29.99 & -2.37 \\ 	
			% TODO: use the other verbs too:  "avvertire che"/"avere il sentore che" ,  "percepire che"/"avere la percezione che" 
			\textit{Cosa hai detto che Gianni avrebbe sollevato?} & -33.74 
			& \textit{Cosa hai riferito il fatto che Gianni avrebbe sollevato?} & -36.64 & 2.90 \\ 
			\textit{Cosa hai intuito che Gianni avrebbe sollevato?} & -34.31 
			& \textit{Cosa hai avuto l'intuizione che Gianni avrebbe sollevato?} & -30.52  & -3.79 \\ 
			
			\textit{Cosa hai messo in dubbio che io avrei vinto?} & -26.48 & 
			\textit{Cosa hai messo in dubbio la previsione che io avrei vinto?} & -30.17 & 3.69\\ 				
			\textit{Cosa hai intuito che che io avrei vinto?} & -29.49 & 
			\textit{Cosa hai avuto l'intuizione che io avrei vinto?} & -24.45  & -5.04 \\ 				

		\end{tabular}
		\caption{Comparing acceptability variations among sentences in the complex NP dataset. A positive difference indicates that the Long-NonIsland sentence is more acceptable than the Long-NonIsland one, as expected.}
		\label{tab:compare1}
	\end{center}
\end{table}


\bigskip
% TODO: insert plot of variations
With other variations experiments, we found that replacing the main clause verb with "sapeva che"/"conosceva che" ("he knew that", see examples in table ..TODO), increases the acceptability of the long-non island sentences; on the other hand, this variation results in a lower acceptability to the short island sentence, compared to the scores from human subjects, and by this way the non-island and island line end up being almost parallel, with the DD score close to zero (which indicates an almost absent island effect).
\\ 
%TODO: add table and plots of senteces with  "sapeva che"/"conosceva che" 

% todo: show tables with examples "sapeva che" % todo: numbered examples like formulas
% todo: show the plots for "intuizione che", "sapeva che" (two subfigures, each numbered/lettered to be referenced)

% examples of complex np items before and after the "avuto l'intuizione che" variation: ..
% show the plots for the items using this construct ("avuto l'intuizione che" ), it seems strange scores are given
% (the island line is higher than the non island one, but they are all acceptable sentence and there is no island restriction anymore)

% still the problem overall in the orginal plots seems an overly low score for the long non island sentences
% that is, is the long distance depencency that receives a too loo score .. (lenght effect?)
% while the structure effect ..
% albeit it's correct, like humans scores, that the long non island has less acceptability than the short island
% then how is the long distance dependency (lenght effect) scored for the other island phenomena?
% the difference with the sentences for the other phenomena seem to be that they have a simpler main clause, like "cosa pensi che" (present) + "abbia ppt"  (congiuntivo passato), while for the complex np, is "cosa hai smentito/annunciato/raccontato/sostenuto che" (passato prossimo) + "avrebbe ppt" (condizionale passato)
% a comparison example could be to turn the other 3 island phenomena items into passato prossimo for the main clause
% but was this discrepancy also in Sprouse data, and is this the reason for their plots differences too? (so the gpt2 model just accentuates this drop in acceptability?)

% for future work: automatic generation of examples from templates, like in BLiMP, to control and test more easily for more factors (es. verbs moods and tenses)
% to test for frequency effect: use some rare verb moods/tenses?

% also find an explanation for the other misclassified long non island sentences that use other constructus (with proper agent/patient semantic roles)

% table..

% Mostra una non robustezza del modello nell’apprendimento di strutture sintattiche / or of this use for scoring minimal pairs, non generalizzazione sintattica, in quanto basta una variazione lessicale, anche tra ..parole frequenti, ..per ..alterare ..quale tra due frasi coppie minime sia piu o meno accettabile.
% However, we refer back to section .., about the interpretation/meaning of the Gpt-2 loss ..output/score.


% TODO: table with example items with different constructs, and their scores
% TODO: plots of the whole test suite for this phenomenon with the same variation for all items (using a different verb)



\subsubsection{Whether islands}

From \autoref{fig:wh_whether} for whether islands with wh-dependencies, we can see that the Gtp-2 model, with the PenLP sentence acceptability estimate, compared with the results from humans gives higher scores for all sentence types. This difference is more pronounced in the scores performed by Gpt on the original test suite from \citet{sprouse2016experimental} (middle image). 

The slope of the lines (both for island and non-island sentence structures) is however quite similar to the human scores. \\


\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/Chapter1/combined_wh-whether.png} % width= 0.8\textwidth
	\caption{Comparison of wh-dependency whether islands} 
	\label{fig:wh_whether} % this internally labels the figure for future referencing.
\end{figure}

..

% TODO: table showing accuracy scores for the 4 sentences of an item; comparison on variations across multiple items

Experimenting with other sentence variations for whether islands, we found that a variation that aligns the plots more to those from the humans' scores, is replacing the personal pronouns (like "io") with proper nouns (like "Gianni") or common nouns (like “il parlamentare”, or “lo studente”)  like in this example: 

\begin{example}	\textsc{Whether islands, Long-Island sentence type}
	\renewcommand{\labelenumi}{\alph{enumi}.}
	\begin{enumerate}
		\item 
		Cosa ti domandi se io abbia riscosso?
		\item 
		Cosa ti domandi se Gianni abbia riscosso?
		\item 
		Cosa ti domandi se il parlamentare abbia riscosso?
	\end{enumerate}
	\label{variation_2}
\end{example}

% This might be the reason for the discrepancies in some of the 4 phenomena: some make more use of ..nomen agentis, while others rely more on personal pronuns. Try this replacement this in all 4 test sets.
%TODO: put scores values and/or to quantify the effect of these variations.

TODO: test sentence variations using less common verb tenses and moods (but consistently across an item or even all items of a suite)
% Not much difference seems to derive by using less common (..) verbs
% (examples in which both sentences in a pair also specify the direct object, see how this affects the score? Like long nonisland vs long island ..)


\subsubsection{Subject islands}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/Chapter1/combined_wh-subject.png} 
	\caption{Comparison of wh-dependencies subject islands} 
	\label{fig:wh_subject} % this internally labels the figure for future referencing.
\end{figure}

In \autoref{fig:wh_subject}, the middle image of Gpt scores on Sprouse data shows significantly lower scores (compared to the human scores on the left image) for the sentences with an island structure (dotted line). % check the sorting of sentences to see why). 
However, on the right image (Gpt scores on our test suite) they are similar to the human scores. \\

For the non island line, the Short-NonIsland is scored lower then on human subject by Gpt for both testsuites. The Long-NonIsland scores instead are similar  (around 1.0). % but lower on sprouse data. \\
The slope of the lines on the right image is similar to the one for the human scores (left image). %  oneon the model scores, both for sprouse and new data, but steeper compared to acceptability results on human subjects.


\subsubsection{Adjunct islands}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/Chapter1/combined_wh-adjunct.png} 
	\caption{Comparison of wh-dependencies adjunct islands} 
	\label{fig:wh_adjunct} % this internally labels the figure for future referencing.
\end{figure}


In \autoref{fig:wh_adjunct} the slope of both lines on the new test suite (right image) is similar to the human scores (left image), althoughg with a lower average acceptability for the Long-NonIsland sentences, which results in a downward steeper line. 

Note that in the test suite we developed for this thesis, the Long-Island sentences for adjunct islands have a form that might make them more easily identifiable has unacceptable than the ones in the Sprouse test suite: compare \textit{Che cosa Gianni è partito per Parigi dopo aver fatto?} (new dataset) with \textit{Cosa ti irriti se dimentico in ufficio?}. The ending of the first sentence might be less frequent and result in a lower acceptability score.
% in the sprouse data, at the end there is a preprositional ..phrase like “in ufficio”.


\subsubsection{Overall observations across all four island phenomena}

The Gpt-2 scores on the original Sprouse et al. test suite (middle image) have wider standard error bars, which are considerably smaller for the new test suite developed for the present thesis (right image).\footnote{This might be due to the fact that the number of items between the two test suites increases from 8 to 50.}


While on human ratings the unacceptable sentences (Long-NonIslands) receive all on average a z-score of about -1, there is more variability in the scores given by the Gpt model, in particular for whether islands sentences (\autoref{fig:wh_whether}), which receive much higher acceptability rating on average (betwenn 0 and 0.5 the Sprouse test suite and ours).
..

% TODO: remaining tables with accuracy scores, comparison with BLiMP extraction islands scores; compare btw models, scoring measures, and datasets (BLiMP, Sprouse, Madeddu)

% todo: all the plots in the appendix (BERT, LP/PenLP, logistic LP/PenLP, .. ) and brief comments 
% (like limitations of BERT acceptability estimates)

% Italian models:
% "LorenzoDeMattei/GePpeTto"
% "dbmdz/bert-base-italian-xxl-cased": ModelTypes.BERT,
% "idb-ita/gilberto-uncased-from-camembert"

% testsuites/datasources:
% Sprouse et al.
% Madeddu

% scoring measures:
% LP/PenLP (softmax-based)
% % LP/PenLP (logistic-based)


\subsection{Discussion on plots from BERT, the logistic function, and PenLP}

% BERT-like models, LP/PenLP scores also based on logistic function scores
% (comment also on the accuracy scores from other models?)

In \autoref{fig:bert_penlp_l_sprouse} we see the plots of the scores from the BERT model, with the PenLP-L sentence acceptability estimate (using the logistic function rather than the softmax, as described in \autoref{sec:softmax}). Interestingly, it seems that this combination of BERT with the logistic function makes a clear separation between unacceptable sentences (the Long-NonIsland type) and acceptable sentence types, with minimal, if any, difference in score between acceptable sentences.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/AppendixA/Sprouse_wh_dbmdz_bert-base-italian-xxl-cased_PLL-zscores-likert-2022-07-11.png} 
	\caption{Plots from BERT with the PenLP-L sentence acceptability approximation}
	\label{fig:bert_penlp_l_sprouse}
\end{figure}

If we compare these plots with the accuracy scores in \autoref{tab:accuracy_it_data} from this model (BERT, logistic function, PenLP) , we see that they are among the best with an average accuracy of 80.7\%; however, looking at the scores for each phenomenon, the scores are high only in about half the cases, and quite low for the rest: Long-NonIsland sentences of all four island phenomena are scored with 58\%, 64\%, 74\%, 58\% accuracy, and the Short-NonIsland of subject islands with 28\% accuracy.

%TODO: check if this is also a skewing effect of the normalization (from the likert scale and the z-scores) and the presence of much negative large values for the unacceptable sentences: in this case the normalization would ..squeeze together the score for the acceptable sentences, if they have a smaller magnitude.

% With the , the use of the logistic function scores as a basis for LP/PenLP instead results in much ..worse plots (  ). Whether, complex and subject islands get almost identical plots, due to the ..skewing/normalization effect of the discretization and the zscores, because the adjunct islands have much higher magnitude negative values (low acceptability) for the unacceptable sentence (long island.) (? so it seems that with BERT and logistic function scores emerge more clearly a distinction between acceptable and unacceptable sentences, without much differenciation within these two groups).

\subsection{Discussion on plots from GilBERTo}

Overall, for GilBERTo, the plots are significantly different from those on human ratings. (..)

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/AppendixA/Madeddu_wh_idb-ita_gilberto-uncased-from-camembert_LP-zscores-likert-2022-07-11.png} 
	\caption{Plots from GilBERTo with the LP sentence acceptability approximation}
	\label{fig:gilberto_lp_madeddu}
\end{figure}


\section{Draft notes on the results}
\subsection{Observation on plots for other models}


The plots for Geppetto and PenLp are the most similar to the plots on human ratings. However, with the LP measure, the plots differ significantly.

The plots for BERT with PenLP on the Sprouse data are similar to the results on humans from \citet{sprouse2016experimental}, %..with some ..differences..


In the plots from the GilBERTo scores, we see that for complex NP and subject islands, the sentences with the island structure receive a higher acceptability than the non island ones (this is no longer the case when removing the penalty for sentence lenght, using the LP sentence acceptability measure). With the PenLP sentence acceptability approximation, the lines tend to have similar slopes, close to being parallel (indicating a lack of, or small, island effect). 

%
%Plots on the Madeddu dataset developed for the present thesis:
%
%BERT, softmax, PenLP and LP
%..
%for adjunct islands, LP seem to work best (without penalizing for sentence lenght).
%Using the logistic function scores, again we see the ..binary differentiation btween unacceptable (long non island) and acceptable (the other 3 types) sentences. However here the plots are more ..flattened, with the scores being very close to the same values.

We refer to the Appendix for the plots for the other models.

\subsection{What seems to affect the models acceptability scores}


\subsubsection{Adjunct islands}

Guardando le short-nonislands, sembra anche qui preponderante il fatto di usare nomi propri di persona (che ha uno score di accettabilità minore) e usare invece di nomi comuni animati/di mestieri/.. (che aumenta l’accettabilità). Ma questo non sembra influenzare il DD score finale % (..evidentemente i vari fattori si bilanciano).


\subsection{Discarded observations on differences between the plots}

\subsubsection{Other notes on  Complex np islands,  what seems to affect the models acceptability scores}
In \autoref{fig:wh_complex} we see that the non-island line (the line connecting the two acceptability scores for short and long distance dependency sentences without an island structure) is significantly lower in new data (todo: see constructs that increase the score of both long and short)

The (gpt) model seem not to make much difference in ..acceptability btw “regular” subordinates and complex noun phrases ..

-- also the short island point is significantly lower

All the 50 + 8 items of the two test suites (the ones from Sprouse et al and the ones developed for the present thesis), when altered to use the following construct for the complex NP "avuto l'intuizione che" (had the intuition that), are scored by the model as if there is no island restriction violation anymore. 

il verbo “intuire” diminuisce ..l’accuratezza del giudizio di accettabilità
in particolare diminuzione della accettabilità delle frasi long nonisland (con long distance wh dependency e struttura non island), che ricevono accettabilità minore di quelle con struttura island:
Esempio analisi variazioni con verbo “intuire” complex np, ..

the verb form “sapeva” (imperfect), compared to ..present perfect forms (“ha osservato/affermato/..”) seems to get better DD score in complex np islands (and also in another phenomenon ..).

\subsubsection{Whether islands}
NB: note that according to human scores, for this type of whether island sentences, the "correct" scoring is to have an increase in acceptability going from short to long non island sentences.
Maybe comparing the scores between acceptable sentences (in this case short and long non islands), expecting it to match humans acceptability judgements .. is beside the present ..research question.
In any case, we noted a reverse in the acceptability difference between this two types of sentences (short and long non islands) when changing the subject of the subordinate sentence from a personal pronoun ("io", 1st pers sg), to a proper noun (i.e. "Gianni"), to a common noun (i.e. "il parlamentare").

%-- significant variation in acceptability diff between short non island and long non island, when replacing: the personal pronouns io/lei, with a proper noun like "Gianni", or even more with a noun like "il parlamentare" (the congressman). In this latter case, significant improvement toward the expected acceptability rate (only 10 out of 50 are .."missclassified"). With the proper noun "Gianni", 18 are missclassified. With the personal pronoun 29 out of 50 are missclassified.
%Try replacing with .. another proper noun, more common like ..
%
%
%(tables and examples of these variations to put on the thesis/report?)
%
%(possible explanation: personal pronouns and proper nouns might be less common in the type/genre of corpora these models were trained on, like wikipedia).
%Observations: the direct objects in the examples are all relatively common words..\footnote{\citet{wei2021frequency} found significant frequency effects (but for agreement .. tests, which are much easier to isolate), for items that occurr rarely in the training corpus (..less then ..10-100 times). But to notice this effect they had to purposly tweak the training corpus. Replicating this for a preexisting corpus (using very rare vocabulary ..) is much more complex and out of the scope of the present study.}
%try with rarer ones.
%..
%-- both on sprouse and new data, gpt "incorrectly" increases the accceptability from short to long non island (check constructs that instead decrease the acceptability?)


\section{BLiMP English dataset}
..
\subsection{English models details}
..

\section{Token surprisal analysis}

(TODO)
%We found the most informative analysis (rather than the factorial test design) was looking at the token surprisals for the Bert-like models (do it also on the Gpt2 logits?).
%..
%(even more informative, or complementary, would be to look at the activations in the model layers)
%
%
%
%"The ratio of token-level PPPLs between unacceptable and
%acceptable sentences overall increases with performance, separating the two sentence sets" \citep{salazar2020masked}
% plot token surprisal on Blimp sentences, compare spikes of minimal pairs differing for just one word


\section{Follow up experiments and future work}

(..)
% finetune the models for acceptability? (already existin italian models for this, like from Trotta et al. 2021?)
% finetuning is very fast anyway 
% (fine tune also for english the Bostrom models?)

% (also to the plot on acceptability score by sentence lenght?)

% what about a follow up analysis on the thesis, based on the results? like manipulating data to check the effect of somee factors, ..


%Analizzare le attivazioni nei vari livelli del modello ..
%Fare un training .”a strati”, in cui in ciascuno strato c’è il target dell’apprendimento di un livello di base della lingua (morfologia, sintassi, semantica, ampiezza del vocabolario, ..), con strati successivi che aumentano la complessità della conoscenza che il modello ha del linguaggio.
%
%
%Confronto percentuali accuratezza (short/long non island, short island) con quelle in BLiMP
%(and other works with extraction islands evaluations?)

% test the worst scoring examples (incorrect, and with the highest PLL diff btw acceptable/unacceptable sentences)
% test balancing island sentences, addint rare words to the long island (eg. "mediatore")
% test changing the ..minimal pairs/sentences in a item like Wilcox rather than Sprouse. Staring from the long island, remove the dependency (instead of making it short, which changes the lexical content)

% in the intro section to island effects, explaining what they are, also say that repeated exposure increases their acceptability (..so maybe it's just their rarity that makes them unacceptable.. language is use.. if they where more common, because needed for some comunicative goals, they would become acceptable..) (maybe this is true, or more the case, for some island types than others)

% confound: try with items that gives semantic cues (the single omitted word is very likely given the verb before and the ..modifier after it, and sentences that don't give such cues, like more vague verbs and modifiers, or no modifiers; generic modifiers: time specifiers)

% complex np are already balanced
% wh could be better balanced
% subject islands are the trickiest to balance ..
% adjuncts ..

% todo: chiarire i vari tipi di subject islands ..
% compare the subject islands definition (and the other types) by Wilcox and by Sprouse

% Wilcox: "The Complex NP Constraint (CNPC) holds that a gap cannot be hosted in a sentential clause dominated by a noun phrase with a lexical head noun. This constraint accounts for the unacceptability of (8b), (8c), (8f) and (8g) below. The CNPC does not apply to other NP modifiers, such as PPs, unless the modified NP occurs in subject position (Huang, 1982). This ban, called the Subject Constraint (SC), accounts for the unacceptability of (8h) compared to (8d)."

% "Subject islands " "as movement out of the subject NP " \citep{sprouse2013experimental}
% IP: inflectional phrase (IP) 
% CP: complementizer Phrase

% do all movements from the subject NP result in island violation effects?

italian rc-dependencies, in the case of, introduced only by ..oblique relative pronouns
"one global decision that we made was to investigate rc-dependencies that form restrictive relative clauses introduced by a relative pronoun (as opposed to appositive relatives or restrictive relatives introduced by a complementizer, which we leave to future research). This had two major consequences. First, all of the rc-dependencies in Italian had to be constructed with oblique (PP) argument gaps" 
("con il/dal quale/su/da/con cui, " etc.)
"because Italian restrictive relatives can only be introduced by a relative pronoun when the head of the relative is an oblique argument (subjects and direct objects can only be introduced by the complementizer che ‘that’). This led to a systematic difference between Italian rc-dependencies, which were constructed with oblique argument gaps, and English rc-dependencies, which were constructed with direct object gaps."

% restrictive relative clauses introduced by a relative pronoun
% as opposed to appositive relatives 
% or restrictive relatives introduced by a complementizer



% recap the cross lingual diffs in En and It about island effects
% then edit/write the whole chapter
% add notes to other chapters in the analysis of results, formation of the dataset, ..


\chapter{Conclusions}
% mention in the conclusions/discussions about the confounds due to semantic full words that produce spikes .. that the softamax function, and the fact that these models are trained to learn a probability distribution of language.. goes contrary to the actual use of language that "does not maximize probability" (cite ref)

% TODO? also discuss, with examples, the choices made in BLiMP to test the same phenomena (in blimp: confound because of wrong ..word sequences?)

% there seems to be errors in the sentences generated by Wilcox et al. for wh-rc/subj: "The newspaper reported that the novel who the famous author received favorable reviews in the press . <eos>"

\chapter{Note sections to remove}
\section{misc ideas with notes}

..
(misc ideas with notes)
using multiple POS taggers and semantic role taggers etc.
one, the default, for most common (..more prototypical) sentences/structures
the other ones, used in ..garden path scenarios, when multiple indicators show that some POS tags or semantic role or other category might be wrong.
the second set of taggers should do a ..full parsing analysis.. combining and ranking different parsing hypotheses)


(on perplexity: the target of "low perplexity" in a language model, should not be in the ..words themselves, because language has informative/comunicative goals that are to introduce new, less predictable, information; it should be at other levels, like that of correctedness/acceptability; target of low perplexity of POS tags, semantic role tags, etc.) \\
\citet{von2018pos} use perplexity on POS tags as a measure of syntactic complexity;
"working on POS sequences avoids having to deal with data sparsity issues." \citep{von2018pos}

what about using perplexity on POS tags as a measure/estimation of syntactic well formedness?
": Lawrence et al. (2000) train RNNs to do
acceptability classification over sequences of POS
tags corresponding to example sentences from a
syntax textbook."

discriminate btw complex/rare vs unacceptable? a threshold (linearly separable) on perplexity? Or multiple dimensions, non linearly, should be used?
(but if there is a wrong/mispelled word.. the pos tagger might misclassify it)
(distinguishing ..sentence unacceptability by linguistic level: morphological mistakes/typos, ..word order errors, ..)
(also: perplexity uses a markov model of language? no, but multiplication the perplexity of each word to obtain the perplexity of the whole sequence.. a "less linear" formula might be needed..)
(montemagni et al paper on profileU on linguistic profiling? use for sentence acceptability estimates? use for ..style detection/automated analysis ..


\section{Misc notes with refs}


First work with targeted syntactic tests on modern language models (RNNs or transformers) is \citet{linzen2016assessing}, while first use of psycholinguistic tests for modern LM is from \citet{futrell2018rnns}.

“NATURAL LANGUAGE DOES NOT MAXIMIZE PROBABILITY” 
“Why is human-written text not the most probable text? We conjecture that this is an intrinsic property of human language. Language models that assign probabilities one word at a time without a global model of the text will have trouble capturing this effect. Grice’s Maxims of Communication (Grice, 1975) show that people optimize against stating the obvious. Thus, making every word as predictable as possible will be disfavored. This makes solving the problem simply by training larger models or improving neural architectures using standard per-word learning objectives unlikely: such models are forced to favor the lowest common denominator, rather than informative language.” 
\citep{holtzman2019curious}





"Auxiliary training objectives Adding auxiliary unsupervised training objectives is an alternative form of semi-supervised learning. Early work by Collobert and Weston [10] used a wide variety of auxiliary NLP tasks such as POS tagging, chunking, named entity recognition, and language modeling to improve semantic role labeling. More recently, Rei [50] added an auxiliary language modeling objective to their target task objective and demonstrated performance gains on sequence labeling tasks. Our experiments also use an auxiliary objective, but as we show, unsupervised pre-training already learns several linguistic aspects relevant to target tasks." (Radford Gpt)

survey on alternative training tasks etc.: paper "a primer on BERTology"

"QUALITATIVE ANALYSIS" (REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION, Tim Rocktaschel et al. 2016)

"Efficiency considerations are important when building language models that use
such large sets of n-grams. Rather than store each word as a string, it is generally
represented in memory as a 64-bit hash number, with the words themselves stored
on disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries." (Jurafsky 2021 3rd ef ch.3)

"the standard information theory textbook Cover and Thomas (1991). "

"It uses the output of a character-level
Convolutional Neural Network (CNN) as input to
the LSTM. This model has the best published perplexity for English text." \citep{wilcox2018rnn}

\section{Notes and refs on incorporating explicit semantic info}

% "Syntax in LMs: There have been several proposals over the years to incorporate explicit syntax into LMs" ", in practice it can still be beneficial to inject syntax into the model. This can be done by combining it with a supervised parser (Dyer et al., 2016) or other multi-task learning objectives (Enguehard et al., 2017)" \citep{marvin2018targeted}

% "Multi-task training with a syntactic objective (CCG supertagging) mitigated this drop in performance for some but not all of the dependencies we tested. We conjecture that the benefits of the inductive bias conferred by multi-task learning will be amplified when the amount of training data is limited"  \citep{marvin2018targeted}
% (that is, incorporating linguitic signals could be effective in lowering the amount of training data needed)

% "Second, we find a larger effect of model inductive bias than training data size on SG score, a result that accords with van Schijndel et al. (2019). Models afforded explicit structural supervision during training outperform other models: One structurally supervised model is able to achieve the same SG scores as a purely sequence-based model trained on ∼100 times the number of tokens. Furthermore, several Transformer models achieve the same SG score as a Transformer trained on ∼200 times the amount of data. " \citep{hu2020systematic}

% Alternative training tasks:
% "Task reformulation: Language model is usually pre-trained with a Masked Language Modeling (MLM) objective, which is motivated by Cloze task in (Taylor, 1953). There have been several works reformulating few-shot learning tasks as cloze questions to reuse pre-trained LM such as LM prompt (Jiang et al., 2020), PET (Radford et al., 2019; Schick \& Schutze, 2020a), and recent ¨ LM-BFF (Gao et al., 2020). It shows a pre-trained LM can achieve non-trivial performance with few annotated samples. There are also some other works transforming NLP tasks as generative QA tasks (Puri \& Catanzaro, 2019)." but low performance on CoLA (possible beacuse of "different data distribution"): "The only exception is the CoLA (the linguistic acceptability task). It is might due to that MLM pre-trainng and entailment training did not see this type of data distribution before"

% enriched training data (with supervised tasks): "Intermediate training: The work by (Phang et al., 2018) shows that supplementing pre-trained LMs with further training on data-rich supervised tasks can obtain additional performance improvements on the GLUE benchmark."

% Freezing / Frozen Layers:
% "The results align with Lee et al. (2019), who discuss the performance degradation of BERT-based models depending upon the number of frozen layers." (Cherniavskii et al. 2022)

% BAAYEN, R. HARALD. 2007. Analyzing linguistic data: A practical introduction to statistics using R. Cambridge: Cambridge University Press.
% BAAYEN, R. HARALD; DOUGLAS J. DAVIDSON; and DOUGLAS M. BATES. 2008. Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language 59.390–412.

% \nocite{wei2021frequency, hu2020systematic, lau2020furiously, bostrom2020byte, futrell-etal-2019-neural}
